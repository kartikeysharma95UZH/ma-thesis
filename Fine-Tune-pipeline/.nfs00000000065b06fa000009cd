
 27%|██▋       | 228/840 [15:08<2:41:08, 15.80s/it]
 27%|██▋       | 229/840 [15:11<2:01:25, 11.92s/it]
 27%|██▋       | 230/840 [15:14<1:33:35,  9.21s/it]
 28%|██▊       | 231/840 [15:17<1:14:03,  7.30s/it]
 28%|██▊       | 232/840 [15:20<1:00:26,  5.97s/it]
 28%|██▊       | 233/840 [15:22<50:55,  5.03s/it]  
 28%|██▊       | 234/840 [15:25<44:10,  4.37s/it]
 28%|██▊       | 235/840 [15:28<39:32,  3.92s/it]
 28%|██▊       | 236/840 [15:31<36:12,  3.60s/it]
 28%|██▊       | 237/840 [15:34<33:53,  3.37s/it]
 28%|██▊       | 238/840 [15:37<32:14,  3.21s/it]
 28%|██▊       | 239/840 [15:40<31:07,  3.11s/it]
 29%|██▊       | 240/840 [15:42<30:14,  3.02s/it]
 29%|██▊       | 241/840 [15:45<29:40,  2.97s/it]
 29%|██▉       | 242/840 [15:48<29:13,  2.93s/it]
 29%|██▉       | 243/840 [15:51<28:51,  2.90s/it]
 29%|██▉       | 244/840 [15:54<28:42,  2.89s/it]
 29%|██▉       | 245/840 [16:29<2:05:53, 12.69s/it]
 29%|██▉       | 246/840 [16:32<1:36:25,  9.74s/it]
 29%|██▉       | 247/840 [16:35<1:15:47,  7.67s/it]
 30%|██▉       | 248/840 [16:38<1:01:23,  6.22s/it]
 30%|██▉       | 249/840 [16:41<51:21,  5.21s/it]  
 30%|██▉       | 250/840 [16:44<44:19,  4.51s/it]
 30%|██▉       | 251/840 [16:46<39:15,  4.00s/it]
 30%|███       | 252/840 [16:49<35:48,  3.65s/it]
 30%|███       | 253/840 [16:52<33:22,  3.41s/it]
 30%|███       | 254/840 [16:55<31:45,  3.25s/it]
 30%|███       | 255/840 [16:58<30:32,  3.13s/it]
 30%|███       | 256/840 [17:01<29:41,  3.05s/it]
 31%|███       | 257/840 [17:03<29:00,  2.99s/it]
 31%|███       | 258/840 [17:06<28:32,  2.94s/it]
 31%|███       | 259/840 [17:09<28:08,  2.91s/it]
 31%|███       | 260/840 [17:12<27:53,  2.88s/it]
 31%|███       | 261/840 [17:15<27:52,  2.89s/it]
 31%|███       | 262/840 [17:18<27:42,  2.88s/it]
 31%|███▏      | 263/840 [17:21<27:28,  2.86s/it]
 31%|███▏      | 264/840 [17:23<27:18,  2.84s/it]
 32%|███▏      | 265/840 [17:26<27:16,  2.85s/it]
 32%|███▏      | 266/840 [17:29<27:14,  2.85s/it]
 32%|███▏      | 267/840 [17:32<27:06,  2.84s/it]
 32%|███▏      | 268/840 [17:35<27:04,  2.84s/it]
 32%|███▏      | 269/840 [17:38<27:04,  2.84s/it]
 32%|███▏      | 270/840 [17:40<26:58,  2.84s/it]
 32%|███▏      | 271/840 [17:43<26:55,  2.84s/it]
 32%|███▏      | 272/840 [17:46<26:53,  2.84s/it]
 32%|███▎      | 273/840 [17:49<26:52,  2.84s/it]
 33%|███▎      | 274/840 [17:52<26:49,  2.84s/it]
 33%|███▎      | 275/840 [17:55<26:45,  2.84s/it]
 33%|███▎      | 276/840 [17:57<26:35,  2.83s/it]
 33%|███▎      | 277/840 [18:00<26:35,  2.83s/it]
 33%|███▎      | 278/840 [18:03<26:30,  2.83s/it]
 33%|███▎      | 279/840 [18:06<26:34,  2.84s/it]
 33%|███▎      | 280/840 [18:09<26:30,  2.84s/it]
 33%|███▎      | 281/840 [18:12<26:30,  2.85s/it]
 34%|███▎      | 282/840 [18:15<26:32,  2.85s/it]
                                                 

 34%|███▎      | 282/840 [18:15<26:32,  2.85s/it]int64
{'eval_loss': 0.018836358562111855, 'eval_rouge1': 24.9199, 'eval_rouge2': 21.9341, 'eval_rougeL': 24.9317, 'eval_rougeLsum': 24.8805, 'eval_runtime': 14.8659, 'eval_samples_per_second': 13.252, 'eval_steps_per_second': 1.009, 'epoch': 4.0}
{'loss': 0.023, 'learning_rate': 6.642857142857143e-05, 'epoch': 4.99}


  0%|          | 0/15 [00:00<?, ?it/s][A

 13%|█▎        | 2/15 [00:00<00:05,  2.24it/s][A

 20%|██        | 3/15 [00:01<00:07,  1.57it/s][A

 27%|██▋       | 4/15 [00:02<00:08,  1.36it/s][A

 33%|███▎      | 5/15 [00:03<00:07,  1.26it/s][A

 40%|████      | 6/15 [00:04<00:07,  1.21it/s][A

 47%|████▋     | 7/15 [00:05<00:06,  1.17it/s][A

 53%|█████▎    | 8/15 [00:06<00:06,  1.15it/s][A

 60%|██████    | 9/15 [00:07<00:05,  1.14it/s][A

 67%|██████▋   | 10/15 [00:08<00:04,  1.13it/s][A

 73%|███████▎  | 11/15 [00:09<00:03,  1.12it/s][A

 80%|████████  | 12/15 [00:09<00:02,  1.12it/s][A

 87%|████████▋ | 13/15 [00:10<00:01,  1.12it/s][A

 93%|█████████▎| 14/15 [00:11<00:00,  1.11it/s][A

100%|██████████| 15/15 [00:12<00:00,  1.34it/s][A
                                                 

                                               
[A
 34%|███▎      | 282/840 [18:29<26:32,  2.85s/it]

100%|██████████| 15/15 [00:12<00:00,  1.34it/s][A

                                               [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

 34%|███▎      | 283/840 [20:03<5:21:28, 34.63s/it]
 34%|███▍      | 284/840 [20:06<3:52:39, 25.11s/it]
 34%|███▍      | 285/840 [20:09<2:50:20, 18.42s/it]
 34%|███▍      | 286/840 [20:12<2:06:52, 13.74s/it]
 34%|███▍      | 287/840 [20:15<1:36:27, 10.47s/it]
 34%|███▍      | 288/840 [20:17<1:15:13,  8.18s/it]
 34%|███▍      | 289/840 [20:20<1:00:23,  6.58s/it]
 35%|███▍      | 290/840 [20:23<50:01,  5.46s/it]  
 35%|███▍      | 291/840 [20:26<42:48,  4.68s/it]
 35%|███▍      | 292/840 [20:29<37:43,  4.13s/it]
 35%|███▍      | 293/840 [20:32<34:09,  3.75s/it]
 35%|███▌      | 294/840 [20:35<31:37,  3.48s/it]
 35%|███▌      | 295/840 [20:37<29:52,  3.29s/it]
 35%|███▌      | 296/840 [20:40<28:35,  3.15s/it]
 35%|███▌      | 297/840 [20:43<27:39,  3.06s/it]
 35%|███▌      | 298/840 [20:46<27:03,  3.00s/it]
 36%|███▌      | 299/840 [20:49<26:29,  2.94s/it]
 36%|███▌      | 300/840 [20:52<26:12,  2.91s/it]
 36%|███▌      | 301/840 [20:54<26:03,  2.90s/it]
 36%|███▌      | 302/840 [20:57<25:48,  2.88s/it]
 36%|███▌      | 303/840 [21:00<26:00,  2.91s/it]
 36%|███▌      | 304/840 [21:03<25:48,  2.89s/it]
 36%|███▋      | 305/840 [21:06<25:40,  2.88s/it]
 36%|███▋      | 306/840 [21:09<25:29,  2.86s/it]
 37%|███▋      | 307/840 [21:12<25:19,  2.85s/it]
 37%|███▋      | 308/840 [21:14<25:14,  2.85s/it]
 37%|███▋      | 309/840 [21:17<25:06,  2.84s/it]
 37%|███▋      | 310/840 [21:20<24:58,  2.83s/it]
 37%|███▋      | 311/840 [21:23<25:00,  2.84s/it]
 37%|███▋      | 312/840 [21:26<24:59,  2.84s/it]
 37%|███▋      | 313/840 [21:29<24:57,  2.84s/it]
 37%|███▋      | 314/840 [21:31<24:58,  2.85s/it]
 38%|███▊      | 315/840 [21:34<24:56,  2.85s/it]
 38%|███▊      | 316/840 [21:37<24:54,  2.85s/it]
 38%|███▊      | 317/840 [21:43<31:36,  3.63s/it]
 38%|███▊      | 318/840 [21:45<29:31,  3.39s/it]
 38%|███▊      | 319/840 [21:48<28:02,  3.23s/it]
 38%|███▊      | 320/840 [21:51<26:55,  3.11s/it]
 38%|███▊      | 321/840 [21:54<26:13,  3.03s/it]
 38%|███▊      | 322/840 [21:57<25:41,  2.98s/it]
 38%|███▊      | 323/840 [22:00<25:18,  2.94s/it]
 39%|███▊      | 324/840 [22:03<25:01,  2.91s/it]
 39%|███▊      | 325/840 [22:05<24:46,  2.89s/it]
 39%|███▉      | 326/840 [22:08<24:34,  2.87s/it]
 39%|███▉      | 327/840 [22:11<24:26,  2.86s/it]
 39%|███▉      | 328/840 [22:14<24:21,  2.85s/it]
 39%|███▉      | 329/840 [22:17<24:18,  2.85s/it]
 39%|███▉      | 330/840 [22:20<24:09,  2.84s/it]
 39%|███▉      | 331/840 [22:22<24:07,  2.84s/it]
 40%|███▉      | 332/840 [22:25<24:03,  2.84s/it]
 40%|███▉      | 333/840 [22:28<24:00,  2.84s/it]
 40%|███▉      | 334/840 [22:31<23:55,  2.84s/it]
 40%|███▉      | 335/840 [22:34<23:56,  2.84s/it]
 40%|████      | 336/840 [22:37<24:15,  2.89s/it]
 40%|████      | 337/840 [22:40<24:07,  2.88s/it]
 40%|████      | 338/840 [22:42<24:00,  2.87s/it]
 40%|████      | 339/840 [22:45<22:49,  2.73s/it]
                                                 

 40%|████      | 339/840 [22:45<22:49,  2.73s/it]int64
{'eval_loss': 0.016573410481214523, 'eval_rouge1': 24.9553, 'eval_rouge2': 21.9844, 'eval_rougeL': 24.9695, 'eval_rougeLsum': 24.9249, 'eval_runtime': 13.2793, 'eval_samples_per_second': 14.835, 'eval_steps_per_second': 1.13, 'epoch': 4.99}
{'loss': 0.0193, 'learning_rate': 5.964285714285714e-05, 'epoch': 6.0}


  0%|          | 0/15 [00:00<?, ?it/s][A

 13%|█▎        | 2/15 [00:00<00:05,  2.23it/s][A

 20%|██        | 3/15 [00:01<00:07,  1.57it/s][A

 27%|██▋       | 4/15 [00:02<00:08,  1.36it/s][A

 33%|███▎      | 5/15 [00:03<00:07,  1.26it/s][A

 40%|████      | 6/15 [00:04<00:07,  1.21it/s][A

 47%|████▋     | 7/15 [00:05<00:06,  1.17it/s][A

 53%|█████▎    | 8/15 [00:06<00:06,  1.15it/s][A

 60%|██████    | 9/15 [00:07<00:05,  1.14it/s][A

 67%|██████▋   | 10/15 [00:08<00:04,  1.13it/s][A

 73%|███████▎  | 11/15 [00:09<00:03,  1.12it/s][A

 80%|████████  | 12/15 [00:09<00:02,  1.12it/s][A

 87%|████████▋ | 13/15 [00:10<00:01,  1.12it/s][A

 93%|█████████▎| 14/15 [00:13<00:00,  1.11it/s][A

100%|██████████| 15/15 [00:13<00:00,  1.20s/it][A
                                                 

                                               
[A
 40%|████      | 339/840 [23:00<22:49,  2.73s/it]

100%|██████████| 15/15 [00:13<00:00,  1.20s/it][A

                                               [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

 40%|████      | 340/840 [23:18<1:37:33, 11.71s/it]
 41%|████      | 341/840 [23:20<1:15:19,  9.06s/it]
 41%|████      | 342/840 [23:23<59:41,  7.19s/it]  
 41%|████      | 343/840 [23:26<48:48,  5.89s/it]
 41%|████      | 344/840 [23:29<41:11,  4.98s/it]
 41%|████      | 345/840 [23:32<35:47,  4.34s/it]
 41%|████      | 346/840 [23:35<32:00,  3.89s/it]
 41%|████▏     | 347/840 [23:37<29:23,  3.58s/it]
 41%|████▏     | 348/840 [23:40<27:34,  3.36s/it]
 42%|████▏     | 349/840 [23:43<26:12,  3.20s/it]
 42%|████▏     | 350/840 [23:46<25:17,  3.10s/it]
 42%|████▏     | 351/840 [23:49<24:34,  3.02s/it]
 42%|████▏     | 352/840 [23:52<24:13,  2.98s/it]
 42%|████▏     | 353/840 [23:55<23:51,  2.94s/it]
 42%|████▏     | 354/840 [23:57<23:38,  2.92s/it]
 42%|████▏     | 355/840 [24:00<23:25,  2.90s/it]
 42%|████▏     | 356/840 [24:03<23:15,  2.88s/it]
 42%|████▎     | 357/840 [24:06<23:03,  2.86s/it]
 43%|████▎     | 358/840 [24:09<22:53,  2.85s/it]
 43%|████▎     | 359/840 [24:12<22:47,  2.84s/it]
 43%|████▎     | 360/840 [24:14<22:47,  2.85s/it]
 43%|████▎     | 361/840 [24:17<22:44,  2.85s/it]
 43%|████▎     | 362/840 [24:20<22:39,  2.84s/it]
 43%|████▎     | 363/840 [24:23<22:33,  2.84s/it]
 43%|████▎     | 364/840 [24:26<22:37,  2.85s/it]
 43%|████▎     | 365/840 [24:29<22:34,  2.85s/it]
 44%|████▎     | 366/840 [24:32<22:28,  2.85s/it]
 44%|████▎     | 367/840 [24:35<22:42,  2.88s/it]
 44%|████▍     | 368/840 [24:37<22:38,  2.88s/it]
 44%|████▍     | 369/840 [24:40<22:31,  2.87s/it]
 44%|████▍     | 370/840 [24:43<22:20,  2.85s/it]
 44%|████▍     | 371/840 [24:46<22:16,  2.85s/it]
 44%|████▍     | 372/840 [24:49<22:17,  2.86s/it]
 44%|████▍     | 373/840 [24:52<22:15,  2.86s/it]
 45%|████▍     | 374/840 [24:54<22:06,  2.85s/it]
 45%|████▍     | 375/840 [24:57<22:03,  2.85s/it]
 45%|████▍     | 376/840 [25:00<22:02,  2.85s/it]
 45%|████▍     | 377/840 [25:03<21:56,  2.84s/it]
 45%|████▌     | 378/840 [25:06<21:54,  2.84s/it]
 45%|████▌     | 379/840 [25:09<21:48,  2.84s/it]
 45%|████▌     | 380/840 [25:12<21:49,  2.85s/it]
 45%|████▌     | 381/840 [25:14<21:48,  2.85s/it]
 45%|████▌     | 382/840 [25:17<21:42,  2.84s/it]
 46%|████▌     | 383/840 [25:20<21:36,  2.84s/it]
 46%|████▌     | 384/840 [25:23<21:37,  2.84s/it]
 46%|████▌     | 385/840 [25:26<21:28,  2.83s/it]
 46%|████▌     | 386/840 [25:29<21:23,  2.83s/it]
 46%|████▌     | 387/840 [25:31<21:18,  2.82s/it]
 46%|████▌     | 388/840 [25:34<21:21,  2.83s/it]
 46%|████▋     | 389/840 [25:37<21:23,  2.85s/it]
 46%|████▋     | 390/840 [25:40<21:19,  2.84s/it]
 47%|████▋     | 391/840 [25:43<21:14,  2.84s/it]
 47%|████▋     | 392/840 [25:46<21:10,  2.84s/it]
 47%|████▋     | 393/840 [25:48<21:08,  2.84s/it]
 47%|████▋     | 394/840 [25:51<21:02,  2.83s/it]
 47%|████▋     | 395/840 [25:54<20:56,  2.82s/it]
                                                 

 47%|████▋     | 395/840 [25:55<20:56,  2.82s/it]int64
{'eval_loss': 0.015108154155313969, 'eval_rouge1': 24.9139, 'eval_rouge2': 21.9508, 'eval_rougeL': 24.9304, 'eval_rougeLsum': 24.8797, 'eval_runtime': 14.7961, 'eval_samples_per_second': 13.314, 'eval_steps_per_second': 1.014, 'epoch': 6.0}
{'loss': 0.0174, 'learning_rate': 5.297619047619048e-05, 'epoch': 6.99}


  0%|          | 0/15 [00:00<?, ?it/s][A

 13%|█▎        | 2/15 [00:00<00:05,  2.23it/s][A

 20%|██        | 3/15 [00:01<00:07,  1.57it/s][A

 27%|██▋       | 4/15 [00:02<00:08,  1.36it/s][A

 33%|███▎      | 5/15 [00:03<00:07,  1.26it/s][A

 40%|████      | 6/15 [00:04<00:07,  1.20it/s][A

 47%|████▋     | 7/15 [00:05<00:06,  1.17it/s][A

 53%|█████▎    | 8/15 [00:06<00:06,  1.15it/s][A

 60%|██████    | 9/15 [00:07<00:05,  1.13it/s][A

 67%|██████▋   | 10/15 [00:08<00:04,  1.13it/s][A

 73%|███████▎  | 11/15 [00:09<00:03,  1.12it/s][A

 80%|████████  | 12/15 [00:09<00:02,  1.12it/s][A

 87%|████████▋ | 13/15 [00:10<00:01,  1.12it/s][A

 93%|█████████▎| 14/15 [00:11<00:00,  1.11it/s][A

100%|██████████| 15/15 [00:11<00:00,  1.42it/s][A
                                                 

                                               
[A
 47%|████▋     | 395/840 [26:08<20:56,  2.82s/it]

100%|██████████| 15/15 [00:12<00:00,  1.42it/s][A

                                               [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

 47%|████▋     | 396/840 [26:35<1:45:20, 14.24s/it]
 47%|████▋     | 397/840 [26:38<1:19:52, 10.82s/it]
 47%|████▋     | 398/840 [26:41<1:02:09,  8.44s/it]
 48%|████▊     | 399/840 [26:43<49:39,  6.76s/it]  
 48%|████▊     | 400/840 [26:46<40:57,  5.59s/it]
 48%|████▊     | 401/840 [26:49<34:54,  4.77s/it]
 48%|████▊     | 402/840 [26:52<30:39,  4.20s/it]
 48%|████▊     | 403/840 [26:55<27:39,  3.80s/it]
 48%|████▊     | 404/840 [26:58<25:30,  3.51s/it]
 48%|████▊     | 405/840 [27:01<24:03,  3.32s/it]
 48%|████▊     | 406/840 [27:03<22:59,  3.18s/it]
 48%|████▊     | 407/840 [27:06<22:24,  3.10s/it]
 49%|████▊     | 408/840 [27:09<21:50,  3.03s/it]
 49%|████▊     | 409/840 [27:12<21:22,  2.98s/it]
 49%|████▉     | 410/840 [27:15<21:02,  2.94s/it]
 49%|████▉     | 411/840 [27:18<20:47,  2.91s/it]
 49%|████▉     | 412/840 [27:21<20:38,  2.89s/it]
 49%|████▉     | 413/840 [27:23<20:26,  2.87s/it]
 49%|████▉     | 414/840 [27:26<20:21,  2.87s/it]
 49%|████▉     | 415/840 [27:29<20:15,  2.86s/it]
 50%|████▉     | 416/840 [27:32<20:08,  2.85s/it]
 50%|████▉     | 417/840 [27:35<20:22,  2.89s/it]
 50%|████▉     | 418/840 [27:38<20:12,  2.87s/it]
 50%|████▉     | 419/840 [27:41<20:00,  2.85s/it]
 50%|█████     | 420/840 [27:43<19:58,  2.85s/it]
 50%|█████     | 421/840 [27:46<19:51,  2.84s/it]
 50%|█████     | 422/840 [27:49<19:48,  2.84s/it]
 50%|█████     | 423/840 [27:52<19:49,  2.85s/it]
 50%|█████     | 424/840 [27:55<19:45,  2.85s/it]
 51%|█████     | 425/840 [27:58<19:42,  2.85s/it]
 51%|█████     | 426/840 [28:01<19:38,  2.85s/it]
 51%|█████     | 427/840 [28:03<19:31,  2.84s/it]
 51%|█████     | 428/840 [28:06<19:25,  2.83s/it]
 51%|█████     | 429/840 [28:09<19:22,  2.83s/it]
 51%|█████     | 430/840 [28:12<19:19,  2.83s/it]
 51%|█████▏    | 431/840 [28:15<19:18,  2.83s/it]
 51%|█████▏    | 432/840 [28:17<19:15,  2.83s/it]
 52%|█████▏    | 433/840 [28:20<19:08,  2.82s/it]
 52%|█████▏    | 434/840 [28:23<19:06,  2.82s/it]
 52%|█████▏    | 435/840 [28:26<19:03,  2.82s/it]
 52%|█████▏    | 436/840 [28:29<19:03,  2.83s/it]
 52%|█████▏    | 437/840 [28:32<19:01,  2.83s/it]
 52%|█████▏    | 438/840 [28:34<18:57,  2.83s/it]
 52%|█████▏    | 439/840 [28:37<18:57,  2.84s/it]
 52%|█████▏    | 440/840 [28:41<20:09,  3.02s/it]
 52%|█████▎    | 441/840 [28:44<19:44,  2.97s/it]
 53%|█████▎    | 442/840 [28:46<19:26,  2.93s/it]
 53%|█████▎    | 443/840 [28:49<19:13,  2.90s/it]
 53%|█████▎    | 444/840 [28:52<19:07,  2.90s/it]
 53%|█████▎    | 445/840 [28:55<18:55,  2.87s/it]
 53%|█████▎    | 446/840 [28:58<18:46,  2.86s/it]
 53%|█████▎    | 447/840 [29:01<18:38,  2.85s/it]
 53%|█████▎    | 448/840 [29:03<18:34,  2.84s/it]
 53%|█████▎    | 449/840 [29:06<18:32,  2.84s/it]
 54%|█████▎    | 450/840 [29:09<18:34,  2.86s/it]
 54%|█████▎    | 451/840 [29:12<18:31,  2.86s/it]
 54%|█████▍    | 452/840 [29:14<17:31,  2.71s/it]
                                                 

 54%|█████▍    | 452/840 [29:17<17:31,  2.71s/it]Terminated
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!
  warnings.warn(
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_special_tokens.py:39: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...

Map:   0%|          | 0/1576 [00:00<?, ? examples/s]
Map:  63%|██████▎   | 1000/1576 [00:00<00:00, 2640.12 examples/s]
Map: 100%|██████████| 1576/1576 [00:00<00:00, 2664.06 examples/s]
                                                                 

Map:   0%|          | 0/197 [00:00<?, ? examples/s]
                                                   

Map:   0%|          | 0/198 [00:00<?, ? examples/s]
                                                   
/home/user/ksharma/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230717_085203-5f01d750
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-large_ep-15_ga-2_b-2_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/df_2000_rows-updated_self_model
wandb: 🚀 View run at https://wandb.ai/ma-thesis/df_2000_rows-updated_self_model/runs/5f01d750
################################################ flan-t5-large_ep-15_ga-2_b-2_lr-0.0001_wd-0.01 ################################################

  0%|          | 0/840 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0%|          | 1/840 [00:23<5:30:42, 23.65s/it]
  0%|          | 2/840 [00:33<3:36:49, 15.52s/it]
  0%|          | 3/840 [00:43<3:01:10, 12.99s/it]
  0%|          | 4/840 [00:53<2:44:43, 11.82s/it]
  1%|          | 5/840 [01:03<2:36:00, 11.21s/it]
  1%|          | 6/840 [01:13<2:29:08, 10.73s/it]
  1%|          | 7/840 [01:23<2:26:19, 10.54s/it]
  1%|          | 8/840 [01:33<2:22:57, 10.31s/it]
  1%|          | 9/840 [01:43<2:21:45, 10.24s/it]
  1%|          | 10/840 [01:53<2:21:13, 10.21s/it]
  1%|▏         | 11/840 [02:03<2:19:42, 10.11s/it]
  1%|▏         | 12/840 [02:13<2:18:36, 10.04s/it]
  2%|▏         | 13/840 [02:23<2:18:20, 10.04s/it]
  2%|▏         | 14/840 [02:33<2:18:08, 10.03s/it]
  2%|▏         | 15/840 [02:49<2:42:59, 11.85s/it]
  2%|▏         | 16/840 [02:59<2:34:10, 11.23s/it]
  2%|▏         | 17/840 [03:09<2:28:34, 10.83s/it]
  2%|▏         | 18/840 [03:18<2:23:56, 10.51s/it]
  2%|▏         | 19/840 [03:28<2:21:02, 10.31s/it]
  2%|▏         | 20/840 [03:38<2:18:37, 10.14s/it]
  2%|▎         | 21/840 [03:48<2:17:50, 10.10s/it]
  3%|▎         | 22/840 [03:58<2:16:40, 10.03s/it]
  3%|▎         | 23/840 [04:08<2:16:53, 10.05s/it]
  3%|▎         | 24/840 [04:18<2:17:00, 10.07s/it]
  3%|▎         | 25/840 [04:28<2:16:00, 10.01s/it]
  3%|▎         | 26/840 [04:38<2:15:47, 10.01s/it]
  3%|▎         | 27/840 [04:48<2:15:03,  9.97s/it]
  3%|▎         | 28/840 [04:58<2:15:02,  9.98s/it]
  3%|▎         | 29/840 [05:08<2:15:24, 10.02s/it]
  4%|▎         | 30/840 [05:18<2:14:07,  9.94s/it]
  4%|▎         | 31/840 [05:28<2:13:53,  9.93s/it]
  4%|▍         | 32/840 [05:37<2:13:22,  9.90s/it]
  4%|▍         | 33/840 [05:47<2:13:01,  9.89s/it]
  4%|▍         | 34/840 [05:57<2:12:30,  9.86s/it]
  4%|▍         | 35/840 [06:07<2:12:34,  9.88s/it]
  4%|▍         | 36/840 [06:17<2:12:26,  9.88s/it]
  4%|▍         | 37/840 [06:27<2:13:02,  9.94s/it]
  5%|▍         | 38/840 [06:37<2:12:26,  9.91s/it]
  5%|▍         | 39/840 [06:47<2:12:18,  9.91s/it]
  5%|▍         | 40/840 [06:57<2:11:55,  9.89s/it]
  5%|▍         | 41/840 [07:07<2:11:56,  9.91s/it]
  5%|▌         | 42/840 [07:17<2:12:33,  9.97s/it]
  5%|▌         | 43/840 [07:27<2:13:09, 10.02s/it]
  5%|▌         | 44/840 [07:37<2:12:07,  9.96s/it]
  5%|▌         | 45/840 [07:47<2:13:26, 10.07s/it]
  5%|▌         | 46/840 [07:57<2:12:12,  9.99s/it]
  6%|▌         | 47/840 [08:07<2:12:06, 10.00s/it]
  6%|▌         | 48/840 [08:17<2:12:19, 10.02s/it]
  6%|▌         | 49/840 [08:27<2:11:42,  9.99s/it]
  6%|▌         | 50/840 [08:37<2:10:53,  9.94s/it]
  6%|▌         | 51/840 [08:47<2:10:37,  9.93s/it]
  6%|▌         | 52/840 [08:56<2:10:01,  9.90s/it]
  6%|▋         | 53/840 [09:06<2:09:53,  9.90s/it]
  6%|▋         | 54/840 [09:16<2:09:09,  9.86s/it]
  7%|▋         | 55/840 [09:26<2:09:04,  9.87s/it]
  7%|▋         | 56/840 [09:36<2:08:42,  9.85s/it]
                                                  

  7%|▋         | 56/840 [09:40<2:08:42,  9.85s/it]{'loss': 7.0464, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.99}


  0%|          | 0/15 [00:00<?, ?it/s][A

 13%|█▎        | 2/15 [00:02<00:18,  1.42s/it][A

 20%|██        | 3/15 [00:05<00:24,  2.02s/it][A

 27%|██▋       | 4/15 [00:08<00:25,  2.32s/it][A

 33%|███▎      | 5/15 [00:11<00:24,  2.49s/it][A

 40%|████      | 6/15 [00:14<00:23,  2.59s/it][A

 47%|████▋     | 7/15 [00:17<00:21,  2.69s/it][A

 53%|█████▎    | 8/15 [00:19<00:19,  2.74s/it][A

 60%|██████    | 9/15 [00:22<00:16,  2.77s/it][A

 67%|██████▋   | 10/15 [00:25<00:14,  2.80s/it][A

 73%|███████▎  | 11/15 [00:28<00:11,  2.82s/it][A

 80%|████████  | 12/15 [00:31<00:08,  2.84s/it][A

 87%|████████▋ | 13/15 [00:34<00:05,  2.84s/it][A

 93%|█████████▎| 14/15 [00:36<00:02,  2.83s/it][A

100%|██████████| 15/15 [00:37<00:00,  2.19s/it][A

                                               
[A
                                                  

100%|██████████| 15/15 [00:37<00:00,  2.19s/it][A
  7%|▋         | 56/840 [10:21<2:08:42,  9.85s/it]

                                               [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  7%|▋         | 57/840 [18:19<35:40:30, 164.02s/it]
  7%|▋         | 58/840 [18:29<25:35:29, 117.81s/it]
  7%|▋         | 59/840 [18:40<18:32:56, 85.50s/it] 
  7%|▋         | 60/840 [18:50<13:38:05, 62.93s/it]
  7%|▋         | 61/840 [19:00<10:10:28, 47.02s/it]
  7%|▋         | 62/840 [19:10<7:46:03, 35.94s/it] 
  8%|▊         | 63/840 [19:20<6:04:24, 28.14s/it]
  8%|▊         | 64/840 [19:30<4:53:59, 22.73s/it]
  8%|▊         | 65/840 [19:40<4:04:22, 18.92s/it]
  8%|▊         | 66/840 [19:50<3:29:17, 16.22s/it]
  8%|▊         | 67/840 [20:00<3:04:24, 14.31s/it]Terminated
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!
  warnings.warn(
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_special_tokens.py:39: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...

Map:   0%|          | 0/1576 [00:00<?, ? examples/s]
Map:  63%|██████▎   | 1000/1576 [00:00<00:00, 2506.14 examples/s]
Map: 100%|██████████| 1576/1576 [00:00<00:00, 2576.58 examples/s]
                                                                 

Map:   0%|          | 0/197 [00:00<?, ? examples/s]
                                                   

Map:   0%|          | 0/198 [00:00<?, ? examples/s]
                                                   
/home/user/ksharma/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230717_091305-tlbswe6r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-large_ep-15_ga-2_b-2_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/df_2000_rows-updated_self_model
wandb: 🚀 View run at https://wandb.ai/ma-thesis/df_2000_rows-updated_self_model/runs/tlbswe6r
################################################ t5-large_ep-15_ga-2_b-2_lr-0.0001_wd-0.01 ################################################

  0%|          | 0/840 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0%|          | 1/840 [00:22<5:12:25, 22.34s/it]
  0%|          | 2/840 [00:31<3:22:41, 14.51s/it]
  0%|          | 3/840 [00:40<2:47:29, 12.01s/it]
  0%|          | 4/840 [00:49<2:32:21, 10.93s/it]Terminated
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!
  warnings.warn(
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_special_tokens.py:39: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

Map:   0%|          | 0/1576 [00:00<?, ? examples/s]
Map:  63%|██████▎   | 1000/1576 [00:00<00:00, 2578.65 examples/s]
Map: 100%|██████████| 1576/1576 [00:00<00:00, 2698.50 examples/s]
                                                                 

Map:   0%|          | 0/197 [00:00<?, ? examples/s]
                                                   

Map:   0%|          | 0/198 [00:00<?, ? examples/s]
                                                   
/home/user/ksharma/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230717_091513-p02je9jb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-xl_ep-15_ga-2_b-2_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/df_2000_rows-updated_self_model
wandb: 🚀 View run at https://wandb.ai/ma-thesis/df_2000_rows-updated_self_model/runs/p02je9jb
################################################ flan-t5-xl_ep-15_ga-2_b-2_lr-0.0001_wd-0.01 ################################################

  0%|          | 0/840 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Terminated
./run_models.sh: line 85: flag: command not found
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name_}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name_}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name_}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name_}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Downloading (…)lve/main/config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 702/702 [00:00<00:00, 426kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|          | 10.5M/2.33G [00:00<03:05, 12.5MB/s]Downloading pytorch_model.bin:   1%|          | 21.0M/2.33G [00:01<01:43, 22.4MB/s]Downloading pytorch_model.bin:   1%|▏         | 31.5M/2.33G [00:01<01:22, 27.7MB/s]Downloading pytorch_model.bin:   2%|▏         | 41.9M/2.33G [00:01<01:13, 31.0MB/s]Downloading pytorch_model.bin:   2%|▏         | 52.4M/2.33G [00:01<01:08, 33.4MB/s]Downloading pytorch_model.bin:   3%|▎         | 62.9M/2.33G [00:02<01:00, 37.5MB/s]Downloading pytorch_model.bin:   3%|▎         | 73.4M/2.33G [00:02<01:00, 37.5MB/s]Downloading pytorch_model.bin:   4%|▎         | 83.9M/2.33G [00:02<00:59, 37.9MB/s]Downloading pytorch_model.bin:   4%|▍         | 94.4M/2.33G [00:02<00:54, 41.0MB/s]Downloading pytorch_model.bin:   5%|▍         | 105M/2.33G [00:03<00:55, 40.2MB/s] Downloading pytorch_model.bin:   5%|▍         | 115M/2.33G [00:03<00:55, 39.6MB/s]Downloading pytorch_model.bin:   5%|▌         | 126M/2.33G [00:03<00:52, 42.3MB/s]Downloading pytorch_model.bin:   6%|▌         | 136M/2.33G [00:03<00:53, 41.1MB/s]Downloading pytorch_model.bin:   6%|▋         | 147M/2.33G [00:04<00:54, 40.3MB/s]Downloading pytorch_model.bin:   7%|▋         | 157M/2.33G [00:04<00:50, 42.8MB/s]Downloading pytorch_model.bin:   7%|▋         | 168M/2.33G [00:04<00:52, 41.4MB/s]Downloading pytorch_model.bin:   8%|▊         | 178M/2.33G [00:04<00:49, 43.5MB/s]Downloading pytorch_model.bin:   8%|▊         | 189M/2.33G [00:05<00:51, 41.9MB/s]Downloading pytorch_model.bin:   9%|▊         | 199M/2.33G [00:05<00:51, 41.0MB/s]Downloading pytorch_model.bin:   9%|▉         | 210M/2.33G [00:05<00:48, 43.3MB/s]Downloading pytorch_model.bin:   9%|▉         | 220M/2.33G [00:05<00:50, 41.8MB/s]Downloading pytorch_model.bin:  10%|▉         | 231M/2.33G [00:06<00:51, 40.9MB/s]Downloading pytorch_model.bin:  10%|█         | 241M/2.33G [00:06<00:48, 43.3MB/s]Downloading pytorch_model.bin:  11%|█         | 252M/2.33G [00:06<00:49, 41.7MB/s]Downloading pytorch_model.bin:  11%|█▏        | 262M/2.33G [00:06<00:47, 43.9MB/s]Downloading pytorch_model.bin:  12%|█▏        | 273M/2.33G [00:07<00:48, 42.1MB/s]Downloading pytorch_model.bin:  12%|█▏        | 283M/2.33G [00:07<00:49, 41.1MB/s]Downloading pytorch_model.bin:  13%|█▎        | 294M/2.33G [00:07<00:46, 43.4MB/s]Downloading pytorch_model.bin:  13%|█▎        | 304M/2.33G [00:07<00:48, 41.8MB/s]Downloading pytorch_model.bin:  14%|█▎        | 315M/2.33G [00:08<00:49, 40.9MB/s]Downloading pytorch_model.bin:  14%|█▍        | 325M/2.33G [00:08<00:46, 43.0MB/s]Downloading pytorch_model.bin:  14%|█▍        | 336M/2.33G [00:08<00:47, 41.8MB/s]Downloading pytorch_model.bin:  15%|█▍        | 346M/2.33G [00:08<00:45, 44.1MB/s]Downloading pytorch_model.bin:  15%|█▌        | 357M/2.33G [00:09<00:46, 42.3MB/s]Downloading pytorch_model.bin:  16%|█▌        | 367M/2.33G [00:09<00:47, 41.2MB/s]Downloading pytorch_model.bin:  16%|█▌        | 377M/2.33G [00:09<00:44, 43.5MB/s]Downloading pytorch_model.bin:  17%|█▋        | 388M/2.33G [00:09<00:46, 42.0MB/s]Downloading pytorch_model.bin:  17%|█▋        | 398M/2.33G [00:10<00:47, 41.1MB/s]Downloading pytorch_model.bin:  18%|█▊        | 409M/2.33G [00:10<00:51, 37.4MB/s]Downloading pytorch_model.bin:  18%|█▊        | 419M/2.33G [00:10<00:43, 43.6MB/s]Downloading pytorch_model.bin:  18%|█▊        | 430M/2.33G [00:10<00:45, 41.8MB/s]Downloading pytorch_model.bin:  19%|█▉        | 440M/2.33G [00:11<00:43, 43.7MB/s]Downloading pytorch_model.bin:  19%|█▉        | 451M/2.33G [00:11<00:44, 42.0MB/s]Downloading pytorch_model.bin:  20%|█▉        | 461M/2.33G [00:11<00:42, 44.4MB/s]Downloading pytorch_model.bin:  20%|██        | 472M/2.33G [00:11<00:43, 42.4MB/s]Downloading pytorch_model.bin:  21%|██        | 482M/2.33G [00:12<00:44, 41.3MB/s]Downloading pytorch_model.bin:  21%|██        | 493M/2.33G [00:12<00:42, 43.5MB/s]Downloading pytorch_model.bin:  22%|██▏       | 503M/2.33G [00:12<00:43, 42.0MB/s]Downloading pytorch_model.bin:  22%|██▏       | 514M/2.33G [00:12<00:44, 40.8MB/s]Downloading pytorch_model.bin:  23%|██▎       | 524M/2.33G [00:13<00:41, 43.4MB/s]Downloading pytorch_model.bin:  23%|██▎       | 535M/2.33G [00:13<00:43, 41.4MB/s]Downloading pytorch_model.bin:  23%|██▎       | 545M/2.33G [00:13<00:43, 40.7MB/s]Downloading pytorch_model.bin:  24%|██▍       | 556M/2.33G [00:13<00:40, 43.3MB/s]Downloading pytorch_model.bin:  24%|██▍       | 566M/2.33G [00:14<00:42, 41.4MB/s]Downloading pytorch_model.bin:  25%|██▍       | 577M/2.33G [00:14<00:43, 40.4MB/s]Downloading pytorch_model.bin:  25%|██▌       | 587M/2.33G [00:14<00:40, 42.8MB/s]Downloading pytorch_model.bin:  26%|██▌       | 598M/2.33G [00:14<00:41, 41.6MB/s]Downloading pytorch_model.bin:  26%|██▌       | 608M/2.33G [00:15<00:42, 40.7MB/s]Downloading pytorch_model.bin:  27%|██▋       | 619M/2.33G [00:15<00:39, 43.1MB/s]Downloading pytorch_model.bin:  27%|██▋       | 629M/2.33G [00:15<00:40, 41.8MB/s]Downloading pytorch_model.bin:  27%|██▋       | 640M/2.33G [00:15<00:38, 44.1MB/s]Downloading pytorch_model.bin:  28%|██▊       | 650M/2.33G [00:16<00:39, 42.1MB/s]Downloading pytorch_model.bin:  28%|██▊       | 661M/2.33G [00:16<00:40, 41.2MB/s]Downloading pytorch_model.bin:  29%|██▉       | 671M/2.33G [00:16<00:38, 43.5MB/s]Downloading pytorch_model.bin:  29%|██▉       | 682M/2.33G [00:16<00:39, 42.1MB/s]Downloading pytorch_model.bin:  30%|██▉       | 692M/2.33G [00:17<00:39, 41.0MB/s]Downloading pytorch_model.bin:  30%|███       | 703M/2.33G [00:17<00:37, 43.4MB/s]Downloading pytorch_model.bin:  31%|███       | 713M/2.33G [00:17<00:38, 41.8MB/s]Downloading pytorch_model.bin:  31%|███       | 724M/2.33G [00:17<00:39, 40.8MB/s]Downloading pytorch_model.bin:  32%|███▏      | 734M/2.33G [00:18<00:37, 43.1MB/s]Downloading pytorch_model.bin:  32%|███▏      | 744M/2.33G [00:18<00:38, 41.7MB/s]Downloading pytorch_model.bin:  32%|███▏      | 755M/2.33G [00:18<00:35, 43.9MB/s]Downloading pytorch_model.bin:  33%|███▎      | 765M/2.33G [00:18<00:37, 42.2MB/s]Downloading pytorch_model.bin:  33%|███▎      | 776M/2.33G [00:19<00:38, 40.4MB/s]Downloading pytorch_model.bin:  34%|███▍      | 786M/2.33G [00:19<00:39, 39.5MB/s]Downloading pytorch_model.bin:  34%|███▍      | 797M/2.33G [00:19<00:39, 39.3MB/s]Downloading pytorch_model.bin:  35%|███▍      | 807M/2.33G [00:19<00:36, 42.0MB/s]Downloading pytorch_model.bin:  35%|███▌      | 818M/2.33G [00:20<00:36, 41.0MB/s]Downloading pytorch_model.bin:  36%|███▌      | 828M/2.33G [00:20<00:37, 40.2MB/s]Downloading pytorch_model.bin:  36%|███▌      | 839M/2.33G [00:20<00:35, 42.5MB/s]Downloading pytorch_model.bin:  36%|███▋      | 849M/2.33G [00:20<00:35, 41.3MB/s]Downloading pytorch_model.bin:  37%|███▋      | 860M/2.33G [00:21<00:34, 42.4MB/s]Downloading pytorch_model.bin:  37%|███▋      | 870M/2.33G [00:21<00:34, 41.9MB/s]Downloading pytorch_model.bin:  38%|███▊      | 881M/2.33G [00:21<00:35, 40.8MB/s]Downloading pytorch_model.bin:  38%|███▊      | 891M/2.33G [00:21<00:35, 40.1MB/s]Downloading pytorch_model.bin:  39%|███▊      | 902M/2.33G [00:22<00:33, 42.6MB/s]Downloading pytorch_model.bin:  39%|███▉      | 912M/2.33G [00:22<00:34, 41.4MB/s]Downloading pytorch_model.bin:  40%|███▉      | 923M/2.33G [00:22<00:34, 40.5MB/s]Downloading pytorch_model.bin:  40%|████      | 933M/2.33G [00:22<00:32, 43.0MB/s]Downloading pytorch_model.bin:  41%|████      | 944M/2.33G [00:23<00:33, 41.5MB/s]Downloading pytorch_model.bin:  41%|████      | 954M/2.33G [00:23<00:33, 40.8MB/s]Downloading pytorch_model.bin:  41%|████▏     | 965M/2.33G [00:23<00:33, 41.0MB/s]Downloading pytorch_model.bin:  42%|████▏     | 975M/2.33G [00:23<00:32, 42.0MB/s]Downloading pytorch_model.bin:  42%|████▏     | 986M/2.33G [00:24<00:30, 44.6MB/s]Downloading pytorch_model.bin:  43%|████▎     | 996M/2.33G [00:24<00:31, 42.4MB/s]Downloading pytorch_model.bin:  43%|████▎     | 1.01G/2.33G [00:24<00:32, 41.3MB/s]Downloading pytorch_model.bin:  44%|████▎     | 1.02G/2.33G [00:24<00:30, 43.6MB/s]Downloading pytorch_model.bin:  44%|████▍     | 1.03G/2.33G [00:25<00:31, 41.9MB/s]Downloading pytorch_model.bin:  45%|████▍     | 1.04G/2.33G [00:25<00:29, 44.3MB/s]Downloading pytorch_model.bin:  45%|████▌     | 1.05G/2.33G [00:25<00:30, 42.3MB/s]Downloading pytorch_model.bin:  45%|████▌     | 1.06G/2.33G [00:25<00:30, 41.3MB/s]Downloading pytorch_model.bin:  46%|████▌     | 1.07G/2.33G [00:26<00:28, 43.6MB/s]Downloading pytorch_model.bin:  46%|████▋     | 1.08G/2.33G [00:26<00:29, 41.9MB/s]Downloading pytorch_model.bin:  47%|████▋     | 1.09G/2.33G [00:26<00:30, 40.9MB/s]Downloading pytorch_model.bin:  47%|████▋     | 1.10G/2.33G [00:26<00:28, 43.1MB/s]Downloading pytorch_model.bin:  48%|████▊     | 1.11G/2.33G [00:27<00:29, 41.6MB/s]Downloading pytorch_model.bin:  48%|████▊     | 1.12G/2.33G [00:27<00:29, 40.9MB/s]Downloading pytorch_model.bin:  49%|████▊     | 1.13G/2.33G [00:27<00:27, 43.1MB/s]Downloading pytorch_model.bin:  49%|████▉     | 1.14G/2.33G [00:27<00:28, 41.8MB/s]Downloading pytorch_model.bin:  50%|████▉     | 1.15G/2.33G [00:28<00:28, 40.8MB/s]Downloading pytorch_model.bin:  50%|████▉     | 1.16G/2.33G [00:28<00:27, 43.0MB/s]Downloading pytorch_model.bin:  50%|█████     | 1.17G/2.33G [00:28<00:27, 41.6MB/s]Downloading pytorch_model.bin:  51%|█████     | 1.18G/2.33G [00:28<00:28, 40.6MB/s]Downloading pytorch_model.bin:  51%|█████▏    | 1.20G/2.33G [00:29<00:26, 43.1MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 1.21G/2.33G [00:29<00:26, 41.7MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 1.22G/2.33G [00:29<00:25, 43.9MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 1.23G/2.33G [00:29<00:26, 42.3MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 1.24G/2.33G [00:30<00:26, 41.0MB/s]Downloading pytorch_model.bin:  54%|█████▎    | 1.25G/2.33G [00:30<00:25, 43.3MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 1.26G/2.33G [00:30<00:25, 41.8MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 1.27G/2.33G [00:30<00:25, 40.8MB/s]Downloading pytorch_model.bin:  55%|█████▍    | 1.28G/2.33G [00:31<00:24, 43.2MB/s]Downloading pytorch_model.bin:  55%|█████▌    | 1.29G/2.33G [00:31<00:24, 41.7MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 1.30G/2.33G [00:31<00:25, 40.8MB/s]Downloading pytorch_model.bin:  56%|█████▋    | 1.31G/2.33G [00:31<00:23, 43.2MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 1.32G/2.33G [00:32<00:24, 41.8MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 1.33G/2.33G [00:32<00:22, 43.9MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.34G/2.33G [00:32<00:23, 42.1MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.35G/2.33G [00:32<00:23, 40.8MB/s]Downloading pytorch_model.bin:  59%|█████▊    | 1.36G/2.33G [00:33<00:22, 43.3MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 1.37G/2.33G [00:33<00:22, 41.9MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 1.38G/2.33G [00:33<00:23, 40.8MB/s]Downloading pytorch_model.bin:  60%|█████▉    | 1.39G/2.33G [00:33<00:21, 43.2MB/s]Downloading pytorch_model.bin:  60%|██████    | 1.41G/2.33G [00:34<00:22, 40.9MB/s]Downloading pytorch_model.bin:  61%|██████    | 1.42G/2.33G [00:34<00:22, 41.0MB/s]Downloading pytorch_model.bin:  61%|██████    | 1.43G/2.33G [00:34<00:21, 43.0MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.44G/2.33G [00:34<00:21, 41.6MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.45G/2.33G [00:35<00:21, 40.7MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.46G/2.33G [00:35<00:20, 43.1MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.47G/2.33G [00:35<00:20, 41.3MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.48G/2.33G [00:35<00:21, 40.5MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 1.49G/2.33G [00:36<00:19, 43.0MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 1.50G/2.33G [00:36<00:19, 41.6MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 1.51G/2.33G [00:36<00:18, 43.9MB/s]Downloading pytorch_model.bin:  65%|██████▌   | 1.52G/2.33G [00:36<00:19, 42.1MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 1.53G/2.33G [00:37<00:19, 41.1MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 1.54G/2.33G [00:37<00:18, 43.5MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.55G/2.33G [00:37<00:18, 41.9MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.56G/2.33G [00:37<00:18, 40.6MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.57G/2.33G [00:38<00:17, 43.0MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.58G/2.33G [00:38<00:17, 41.8MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.59G/2.33G [00:38<00:18, 40.8MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 1.60G/2.33G [00:38<00:16, 43.1MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 1.61G/2.33G [00:39<00:17, 41.7MB/s]Downloading pytorch_model.bin:  70%|██████▉   | 1.63G/2.33G [00:39<00:17, 40.7MB/s]Downloading pytorch_model.bin:  70%|███████   | 1.64G/2.33G [00:39<00:16, 43.0MB/s]Downloading pytorch_model.bin:  71%|███████   | 1.65G/2.33G [00:39<00:16, 41.8MB/s]Downloading pytorch_model.bin:  71%|███████   | 1.66G/2.33G [00:40<00:15, 43.7MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.67G/2.33G [00:40<00:15, 42.2MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.68G/2.33G [00:40<00:15, 41.0MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.69G/2.33G [00:40<00:14, 43.0MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.70G/2.33G [00:41<00:15, 41.8MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.71G/2.33G [00:41<00:15, 41.0MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 1.72G/2.33G [00:41<00:14, 43.4MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 1.73G/2.33G [00:41<00:14, 41.3MB/s]Downloading pytorch_model.bin:  75%|███████▍  | 1.74G/2.33G [00:42<00:14, 40.6MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 1.75G/2.33G [00:42<00:14, 39.5MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 1.76G/2.33G [00:42<00:13, 42.1MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 1.77G/2.33G [00:42<00:13, 41.1MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.78G/2.33G [00:43<00:13, 40.4MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.79G/2.33G [00:43<00:12, 42.9MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.80G/2.33G [00:43<00:12, 41.6MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 1.81G/2.33G [00:43<00:12, 40.5MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 1.82G/2.33G [00:44<00:11, 42.9MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 1.84G/2.33G [00:44<00:11, 41.5MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 1.85G/2.33G [00:44<00:11, 40.6MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 1.86G/2.33G [00:44<00:11, 42.8MB/s]Downloading pytorch_model.bin:  80%|████████  | 1.87G/2.33G [00:45<00:11, 41.4MB/s]Downloading pytorch_model.bin:  81%|████████  | 1.88G/2.33G [00:45<00:11, 40.6MB/s]Downloading pytorch_model.bin:  81%|████████  | 1.89G/2.33G [00:45<00:10, 43.1MB/s]Downloading pytorch_model.bin:  81%|████████▏ | 1.90G/2.33G [00:45<00:10, 41.7MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 1.91G/2.33G [00:46<00:10, 40.7MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 1.92G/2.33G [00:46<00:09, 43.1MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.93G/2.33G [00:46<00:09, 41.6MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.94G/2.33G [00:46<00:09, 40.6MB/s]Downloading pytorch_model.bin:  84%|████████▎ | 1.95G/2.33G [00:47<00:08, 43.1MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 1.96G/2.33G [00:47<00:08, 41.7MB/s]Downloading pytorch_model.bin:  85%|████████▍ | 1.97G/2.33G [00:47<00:08, 43.4MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 1.98G/2.33G [00:47<00:08, 42.3MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 1.99G/2.33G [00:48<00:08, 41.3MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 2.00G/2.33G [00:48<00:07, 43.2MB/s]Downloading pytorch_model.bin:  86%|████████▋ | 2.01G/2.33G [00:48<00:07, 42.1MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 2.02G/2.33G [00:48<00:06, 43.9MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 2.03G/2.33G [00:49<00:06, 42.5MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 2.04G/2.33G [00:49<00:06, 41.3MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 2.06G/2.33G [00:49<00:06, 43.4MB/s]Downloading pytorch_model.bin:  89%|████████▊ | 2.07G/2.33G [00:49<00:06, 42.1MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 2.08G/2.33G [00:50<00:06, 41.1MB/s]Downloading pytorch_model.bin:  90%|████████▉ | 2.09G/2.33G [00:50<00:05, 43.4MB/s]Downloading pytorch_model.bin:  90%|█████████ | 2.10G/2.33G [00:50<00:05, 41.9MB/s]Downloading pytorch_model.bin:  90%|█████████ | 2.11G/2.33G [00:50<00:05, 43.9MB/s]Downloading pytorch_model.bin:  91%|█████████ | 2.12G/2.33G [00:51<00:05, 42.3MB/s]Downloading pytorch_model.bin:  91%|█████████▏| 2.13G/2.33G [00:51<00:04, 41.2MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 2.14G/2.33G [00:51<00:04, 43.5MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 2.15G/2.33G [00:51<00:04, 41.9MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 2.16G/2.33G [00:52<00:04, 40.8MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 2.17G/2.33G [00:52<00:03, 43.2MB/s]Downloading pytorch_model.bin:  94%|█████████▎| 2.18G/2.33G [00:52<00:03, 41.7MB/s]Downloading pytorch_model.bin:  94%|█████████▍| 2.19G/2.33G [00:52<00:03, 40.7MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 2.20G/2.33G [00:53<00:02, 43.1MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 2.21G/2.33G [00:53<00:02, 41.5MB/s]Downloading pytorch_model.bin:  95%|█████████▌| 2.22G/2.33G [00:53<00:02, 43.6MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 2.23G/2.33G [00:53<00:02, 42.1MB/s]Downloading pytorch_model.bin:  96%|█████████▋| 2.24G/2.33G [00:54<00:02, 41.1MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 2.25G/2.33G [00:54<00:01, 43.5MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 2.26G/2.33G [00:54<00:01, 41.9MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 2.28G/2.33G [00:54<00:01, 44.1MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 2.29G/2.33G [00:55<00:01, 41.8MB/s]Downloading pytorch_model.bin:  99%|█████████▊| 2.30G/2.33G [00:55<00:00, 41.5MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 2.31G/2.33G [00:55<00:00, 43.2MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 2.32G/2.33G [00:55<00:00, 42.1MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 2.33G/2.33G [00:56<00:00, 41.1MB/s]Downloading pytorch_model.bin: 100%|██████████| 2.33G/2.33G [00:56<00:00, 41.5MB/s]
Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 91.8kB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 376/376 [00:00<00:00, 358kB/s]
Downloading (…)ve/main/spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]Downloading (…)ve/main/spiece.model: 100%|██████████| 4.31M/4.31M [00:00<00:00, 22.1MB/s]Downloading (…)ve/main/spiece.model: 100%|██████████| 4.31M/4.31M [00:00<00:00, 21.7MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 56.8kB/s]
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name_}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 642/642 [00:00<00:00, 215kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|          | 10.5M/4.92G [00:00<00:51, 95.7MB/s]Downloading pytorch_model.bin:   0%|          | 21.0M/4.92G [00:00<00:49, 99.2MB/s]Downloading pytorch_model.bin:   1%|          | 31.5M/4.92G [00:00<00:48, 101MB/s] Downloading pytorch_model.bin:   1%|          | 52.4M/4.92G [00:00<00:46, 104MB/s]Downloading pytorch_model.bin:   1%|▏         | 73.4M/4.92G [00:00<00:45, 107MB/s]Downloading pytorch_model.bin:   2%|▏         | 94.4M/4.92G [00:00<00:44, 109MB/s]Downloading pytorch_model.bin:   2%|▏         | 115M/4.92G [00:01<00:43, 111MB/s] Downloading pytorch_model.bin:   3%|▎         | 136M/4.92G [00:01<00:42, 113MB/s]Downloading pytorch_model.bin:   3%|▎         | 157M/4.92G [00:01<00:41, 115MB/s]Downloading pytorch_model.bin:   4%|▎         | 178M/4.92G [00:01<00:40, 116MB/s]Downloading pytorch_model.bin:   4%|▍         | 199M/4.92G [00:01<00:40, 117MB/s]Downloading pytorch_model.bin:   4%|▍         | 220M/4.92G [00:01<00:40, 117MB/s]Downloading pytorch_model.bin:   5%|▍         | 241M/4.92G [00:02<00:39, 117MB/s]Downloading pytorch_model.bin:   5%|▌         | 262M/4.92G [00:02<00:39, 118MB/s]Downloading pytorch_model.bin:   6%|▌         | 283M/4.92G [00:02<00:39, 118MB/s]Downloading pytorch_model.bin:   6%|▌         | 304M/4.92G [00:02<00:39, 118MB/s]Downloading pytorch_model.bin:   7%|▋         | 325M/4.92G [00:02<00:38, 118MB/s]Downloading pytorch_model.bin:   7%|▋         | 346M/4.92G [00:03<00:38, 118MB/s]Downloading pytorch_model.bin:   7%|▋         | 367M/4.92G [00:03<00:38, 118MB/s]Downloading pytorch_model.bin:   8%|▊         | 388M/4.92G [00:03<00:39, 116MB/s]Downloading pytorch_model.bin:   8%|▊         | 409M/4.92G [00:03<00:38, 117MB/s]Downloading pytorch_model.bin:   9%|▊         | 430M/4.92G [00:03<00:38, 118MB/s]Downloading pytorch_model.bin:   9%|▉         | 451M/4.92G [00:03<00:37, 118MB/s]Downloading pytorch_model.bin:  10%|▉         | 472M/4.92G [00:04<00:37, 118MB/s]Downloading pytorch_model.bin:  10%|█         | 493M/4.92G [00:04<00:37, 118MB/s]Downloading pytorch_model.bin:  10%|█         | 514M/4.92G [00:04<00:37, 118MB/s]Downloading pytorch_model.bin:  11%|█         | 535M/4.92G [00:04<00:37, 117MB/s]Downloading pytorch_model.bin:  11%|█▏        | 556M/4.92G [00:04<00:37, 116MB/s]Downloading pytorch_model.bin:  12%|█▏        | 577M/4.92G [00:04<00:37, 116MB/s]Downloading pytorch_model.bin:  12%|█▏        | 598M/4.92G [00:05<00:37, 117MB/s]Downloading pytorch_model.bin:  13%|█▎        | 619M/4.92G [00:05<00:36, 117MB/s]Downloading pytorch_model.bin:  13%|█▎        | 640M/4.92G [00:05<00:36, 117MB/s]Downloading pytorch_model.bin:  13%|█▎        | 661M/4.92G [00:05<00:36, 117MB/s]Downloading pytorch_model.bin:  14%|█▍        | 682M/4.92G [00:05<00:36, 118MB/s]Downloading pytorch_model.bin:  14%|█▍        | 703M/4.92G [00:06<00:35, 118MB/s]Downloading pytorch_model.bin:  15%|█▍        | 724M/4.92G [00:06<00:35, 117MB/s]Downloading pytorch_model.bin:  15%|█▌        | 744M/4.92G [00:06<00:35, 117MB/s]Downloading pytorch_model.bin:  16%|█▌        | 765M/4.92G [00:06<00:35, 118MB/s]Downloading pytorch_model.bin:  16%|█▌        | 786M/4.92G [00:06<00:35, 118MB/s]Downloading pytorch_model.bin:  16%|█▋        | 807M/4.92G [00:06<00:34, 118MB/s]Downloading pytorch_model.bin:  17%|█▋        | 828M/4.92G [00:07<00:34, 118MB/s]Downloading pytorch_model.bin:  17%|█▋        | 849M/4.92G [00:07<00:34, 118MB/s]Downloading pytorch_model.bin:  18%|█▊        | 870M/4.92G [00:07<00:34, 119MB/s]Downloading pytorch_model.bin:  18%|█▊        | 891M/4.92G [00:07<00:34, 118MB/s]Downloading pytorch_model.bin:  19%|█▊        | 912M/4.92G [00:07<00:34, 117MB/s]Downloading pytorch_model.bin:  19%|█▉        | 933M/4.92G [00:08<00:33, 118MB/s]Downloading pytorch_model.bin:  19%|█▉        | 954M/4.92G [00:08<00:33, 118MB/s]Downloading pytorch_model.bin:  20%|█▉        | 975M/4.92G [00:08<00:33, 118MB/s]Downloading pytorch_model.bin:  20%|██        | 996M/4.92G [00:08<00:33, 118MB/s]Downloading pytorch_model.bin:  21%|██        | 1.02G/4.92G [00:08<00:33, 117MB/s]Downloading pytorch_model.bin:  21%|██        | 1.04G/4.92G [00:08<00:33, 117MB/s]Downloading pytorch_model.bin:  22%|██▏       | 1.06G/4.92G [00:09<00:32, 117MB/s]Downloading pytorch_model.bin:  22%|██▏       | 1.08G/4.92G [00:09<00:32, 118MB/s]Downloading pytorch_model.bin:  22%|██▏       | 1.10G/4.92G [00:09<00:32, 118MB/s]Downloading pytorch_model.bin:  23%|██▎       | 1.12G/4.92G [00:09<00:32, 118MB/s]Downloading pytorch_model.bin:  23%|██▎       | 1.14G/4.92G [00:09<00:32, 118MB/s]Downloading pytorch_model.bin:  24%|██▎       | 1.16G/4.92G [00:09<00:31, 118MB/s]Downloading pytorch_model.bin:  24%|██▍       | 1.18G/4.92G [00:10<00:31, 118MB/s]Downloading pytorch_model.bin:  25%|██▍       | 1.21G/4.92G [00:10<00:41, 89.6MB/s]Downloading pytorch_model.bin:  25%|██▍       | 1.22G/4.92G [00:10<00:40, 91.4MB/s]Downloading pytorch_model.bin:  25%|██▌       | 1.24G/4.92G [00:10<00:38, 96.1MB/s]Downloading pytorch_model.bin:  26%|██▌       | 1.26G/4.92G [00:11<00:36, 100MB/s] Downloading pytorch_model.bin:  26%|██▌       | 1.28G/4.92G [00:11<00:35, 104MB/s]Downloading pytorch_model.bin:  26%|██▋       | 1.30G/4.92G [00:11<00:33, 106MB/s]Downloading pytorch_model.bin:  27%|██▋       | 1.32G/4.92G [00:11<00:32, 109MB/s]Downloading pytorch_model.bin:  27%|██▋       | 1.34G/4.92G [00:11<00:32, 112MB/s]Downloading pytorch_model.bin:  28%|██▊       | 1.36G/4.92G [00:11<00:31, 114MB/s]Downloading pytorch_model.bin:  28%|██▊       | 1.38G/4.92G [00:12<00:31, 114MB/s]Downloading pytorch_model.bin:  29%|██▊       | 1.41G/4.92G [00:12<00:30, 116MB/s]Downloading pytorch_model.bin:  29%|██▉       | 1.43G/4.92G [00:12<00:29, 116MB/s]Downloading pytorch_model.bin:  29%|██▉       | 1.45G/4.92G [00:12<00:29, 117MB/s]Downloading pytorch_model.bin:  30%|██▉       | 1.47G/4.92G [00:12<00:29, 116MB/s]Downloading pytorch_model.bin:  30%|███       | 1.49G/4.92G [00:12<00:29, 117MB/s]Downloading pytorch_model.bin:  31%|███       | 1.51G/4.92G [00:13<00:29, 117MB/s]Downloading pytorch_model.bin:  31%|███       | 1.53G/4.92G [00:13<00:28, 118MB/s]Downloading pytorch_model.bin:  32%|███▏      | 1.55G/4.92G [00:13<00:28, 118MB/s]Downloading pytorch_model.bin:  32%|███▏      | 1.57G/4.92G [00:13<00:28, 118MB/s]Downloading pytorch_model.bin:  32%|███▏      | 1.59G/4.92G [00:13<00:28, 118MB/s]Downloading pytorch_model.bin:  33%|███▎      | 1.61G/4.92G [00:14<00:27, 118MB/s]Downloading pytorch_model.bin:  33%|███▎      | 1.64G/4.92G [00:14<00:27, 118MB/s]Downloading pytorch_model.bin:  34%|███▎      | 1.66G/4.92G [00:14<00:27, 118MB/s]Downloading pytorch_model.bin:  34%|███▍      | 1.68G/4.92G [00:14<00:27, 118MB/s]Downloading pytorch_model.bin:  35%|███▍      | 1.70G/4.92G [00:14<00:27, 117MB/s]Downloading pytorch_model.bin:  35%|███▍      | 1.72G/4.92G [00:14<00:27, 116MB/s]Downloading pytorch_model.bin:  35%|███▌      | 1.74G/4.92G [00:15<00:27, 116MB/s]Downloading pytorch_model.bin:  36%|███▌      | 1.76G/4.92G [00:15<00:26, 117MB/s]Downloading pytorch_model.bin:  36%|███▌      | 1.78G/4.92G [00:15<00:26, 117MB/s]Downloading pytorch_model.bin:  37%|███▋      | 1.80G/4.92G [00:15<00:26, 118MB/s]Downloading pytorch_model.bin:  37%|███▋      | 1.82G/4.92G [00:15<00:26, 118MB/s]Downloading pytorch_model.bin:  38%|███▊      | 1.85G/4.92G [00:16<00:26, 118MB/s]Downloading pytorch_model.bin:  38%|███▊      | 1.87G/4.92G [00:16<00:25, 117MB/s]Downloading pytorch_model.bin:  38%|███▊      | 1.89G/4.92G [00:16<00:25, 118MB/s]Downloading pytorch_model.bin:  39%|███▉      | 1.91G/4.92G [00:16<00:25, 118MB/s]Downloading pytorch_model.bin:  39%|███▉      | 1.93G/4.92G [00:16<00:25, 118MB/s]Downloading pytorch_model.bin:  40%|███▉      | 1.95G/4.92G [00:16<00:25, 118MB/s]Downloading pytorch_model.bin:  40%|████      | 1.97G/4.92G [00:17<00:25, 116MB/s]Downloading pytorch_model.bin:  41%|████      | 1.99G/4.92G [00:17<00:25, 116MB/s]Downloading pytorch_model.bin:  41%|████      | 2.01G/4.92G [00:17<00:24, 116MB/s]Downloading pytorch_model.bin:  41%|████▏     | 2.03G/4.92G [00:17<00:24, 116MB/s]Downloading pytorch_model.bin:  42%|████▏     | 2.06G/4.92G [00:17<00:24, 117MB/s]Downloading pytorch_model.bin:  42%|████▏     | 2.08G/4.92G [00:17<00:24, 118MB/s]Downloading pytorch_model.bin:  43%|████▎     | 2.10G/4.92G [00:18<00:24, 117MB/s]Downloading pytorch_model.bin:  43%|████▎     | 2.12G/4.92G [00:18<00:23, 118MB/s]Downloading pytorch_model.bin:  43%|████▎     | 2.14G/4.92G [00:18<00:23, 118MB/s]Downloading pytorch_model.bin:  44%|████▍     | 2.16G/4.92G [00:18<00:23, 118MB/s]Downloading pytorch_model.bin:  44%|████▍     | 2.18G/4.92G [00:18<00:23, 118MB/s]Downloading pytorch_model.bin:  45%|████▍     | 2.20G/4.92G [00:19<00:22, 118MB/s]Downloading pytorch_model.bin:  45%|████▌     | 2.22G/4.92G [00:19<00:22, 118MB/s]Downloading pytorch_model.bin:  46%|████▌     | 2.24G/4.92G [00:19<00:22, 118MB/s]Downloading pytorch_model.bin:  46%|████▌     | 2.26G/4.92G [00:19<00:22, 118MB/s]Downloading pytorch_model.bin:  46%|████▋     | 2.29G/4.92G [00:19<00:22, 118MB/s]Downloading pytorch_model.bin:  47%|████▋     | 2.31G/4.92G [00:19<00:22, 117MB/s]Downloading pytorch_model.bin:  47%|████▋     | 2.33G/4.92G [00:20<00:21, 118MB/s]Downloading pytorch_model.bin:  48%|████▊     | 2.35G/4.92G [00:20<00:21, 118MB/s]Downloading pytorch_model.bin:  48%|████▊     | 2.37G/4.92G [00:20<00:28, 89.0MB/s]Downloading pytorch_model.bin:  49%|████▊     | 2.39G/4.92G [00:20<00:27, 93.6MB/s]Downloading pytorch_model.bin:  49%|████▉     | 2.41G/4.92G [00:21<00:25, 97.7MB/s]Downloading pytorch_model.bin:  49%|████▉     | 2.43G/4.92G [00:21<00:24, 101MB/s] Downloading pytorch_model.bin:  50%|████▉     | 2.45G/4.92G [00:21<00:23, 104MB/s]Downloading pytorch_model.bin:  50%|█████     | 2.47G/4.92G [00:21<00:22, 108MB/s]Downloading pytorch_model.bin:  51%|█████     | 2.50G/4.92G [00:21<00:21, 110MB/s]Downloading pytorch_model.bin:  51%|█████     | 2.52G/4.92G [00:21<00:21, 112MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 2.54G/4.92G [00:22<00:21, 110MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 2.56G/4.92G [00:22<00:21, 109MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 2.58G/4.92G [00:22<00:21, 109MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 2.60G/4.92G [00:22<00:21, 110MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 2.62G/4.92G [00:22<00:20, 110MB/s]Downloading pytorch_model.bin:  54%|█████▎    | 2.64G/4.92G [00:23<00:20, 110MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 2.66G/4.92G [00:23<00:21, 107MB/s]Downloading pytorch_model.bin:  55%|█████▍    | 2.68G/4.92G [00:23<00:22, 97.6MB/s]Downloading pytorch_model.bin:  55%|█████▌    | 2.71G/4.92G [00:23<00:22, 100MB/s] Downloading pytorch_model.bin:  55%|█████▌    | 2.73G/4.92G [00:23<00:21, 103MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 2.75G/4.92G [00:24<00:20, 105MB/s]Downloading pytorch_model.bin:  56%|█████▋    | 2.77G/4.92G [00:24<00:19, 108MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 2.79G/4.92G [00:24<00:19, 110MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 2.81G/4.92G [00:24<00:18, 112MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 2.83G/4.92G [00:24<00:18, 114MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 2.85G/4.92G [00:25<00:17, 115MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 2.87G/4.92G [00:25<00:17, 116MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 2.89G/4.92G [00:25<00:17, 117MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 2.92G/4.92G [00:25<00:17, 117MB/s]Downloading pytorch_model.bin:  60%|█████▉    | 2.94G/4.92G [00:25<00:16, 118MB/s]Downloading pytorch_model.bin:  60%|██████    | 2.96G/4.92G [00:25<00:16, 118MB/s]Downloading pytorch_model.bin:  61%|██████    | 2.98G/4.92G [00:26<00:16, 118MB/s]Downloading pytorch_model.bin:  61%|██████    | 3.00G/4.92G [00:26<00:16, 118MB/s]Downloading pytorch_model.bin:  61%|██████▏   | 3.02G/4.92G [00:26<00:16, 118MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 3.04G/4.92G [00:26<00:15, 118MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 3.06G/4.92G [00:26<00:15, 118MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 3.08G/4.92G [00:27<00:15, 118MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 3.10G/4.92G [00:27<00:15, 118MB/s]Downloading pytorch_model.bin:  64%|██████▎   | 3.12G/4.92G [00:27<00:15, 118MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 3.15G/4.92G [00:27<00:15, 118MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 3.17G/4.92G [00:27<00:14, 118MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 3.19G/4.92G [00:27<00:14, 118MB/s]Downloading pytorch_model.bin:  65%|██████▌   | 3.21G/4.92G [00:28<00:14, 118MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 3.23G/4.92G [00:28<00:14, 118MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 3.25G/4.92G [00:28<00:14, 118MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 3.27G/4.92G [00:28<00:13, 118MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 3.29G/4.92G [00:28<00:13, 118MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 3.31G/4.92G [00:28<00:13, 118MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 3.33G/4.92G [00:29<00:13, 118MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 3.36G/4.92G [00:29<00:13, 118MB/s]Downloading pytorch_model.bin:  69%|██████▊   | 3.38G/4.92G [00:29<00:13, 118MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 3.40G/4.92G [00:29<00:12, 118MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 3.42G/4.92G [00:29<00:12, 118MB/s]Downloading pytorch_model.bin:  70%|██████▉   | 3.44G/4.92G [00:30<00:12, 118MB/s]Downloading pytorch_model.bin:  70%|███████   | 3.46G/4.92G [00:30<00:12, 118MB/s]Downloading pytorch_model.bin:  71%|███████   | 3.48G/4.92G [00:30<00:12, 118MB/s]Downloading pytorch_model.bin:  71%|███████   | 3.50G/4.92G [00:30<00:11, 118MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 3.52G/4.92G [00:30<00:15, 89.6MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 3.54G/4.92G [00:31<00:14, 94.9MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 3.57G/4.92G [00:31<00:13, 99.6MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 3.59G/4.92G [00:31<00:12, 104MB/s] Downloading pytorch_model.bin:  73%|███████▎  | 3.61G/4.92G [00:31<00:12, 107MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 3.63G/4.92G [00:31<00:11, 110MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 3.65G/4.92G [00:32<00:11, 113MB/s]Downloading pytorch_model.bin:  75%|███████▍  | 3.67G/4.92G [00:32<00:10, 114MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 3.69G/4.92G [00:32<00:10, 116MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 3.71G/4.92G [00:32<00:10, 116MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 3.73G/4.92G [00:32<00:10, 117MB/s]Downloading pytorch_model.bin:  76%|███████▋  | 3.75G/4.92G [00:32<00:09, 117MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 3.77G/4.92G [00:33<00:09, 118MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 3.80G/4.92G [00:33<00:09, 118MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 3.82G/4.92G [00:33<00:09, 118MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 3.84G/4.92G [00:33<00:09, 118MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 3.86G/4.92G [00:33<00:08, 118MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 3.88G/4.92G [00:33<00:08, 118MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 3.90G/4.92G [00:34<00:08, 115MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 3.92G/4.92G [00:34<00:09, 105MB/s]Downloading pytorch_model.bin:  80%|████████  | 3.94G/4.92G [00:34<00:10, 93.9MB/s]Downloading pytorch_model.bin:  80%|████████  | 3.95G/4.92G [00:34<00:11, 86.8MB/s]Downloading pytorch_model.bin:  81%|████████  | 3.97G/4.92G [00:35<00:10, 88.4MB/s]Downloading pytorch_model.bin:  81%|████████  | 3.98G/4.92G [00:35<00:10, 85.6MB/s]Downloading pytorch_model.bin:  81%|████████▏ | 4.01G/4.92G [00:35<00:09, 92.9MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 4.03G/4.92G [00:35<00:09, 98.5MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 4.05G/4.92G [00:35<00:08, 103MB/s] Downloading pytorch_model.bin:  83%|████████▎ | 4.07G/4.92G [00:35<00:07, 107MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 4.09G/4.92G [00:36<00:07, 110MB/s]Downloading pytorch_model.bin:  84%|████████▎ | 4.11G/4.92G [00:36<00:07, 112MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 4.13G/4.92G [00:36<00:06, 114MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 4.15G/4.92G [00:36<00:06, 115MB/s]Downloading pytorch_model.bin:  85%|████████▍ | 4.17G/4.92G [00:36<00:06, 116MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 4.19G/4.92G [00:37<00:06, 117MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 4.22G/4.92G [00:37<00:05, 117MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 4.24G/4.92G [00:37<00:05, 118MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 4.26G/4.92G [00:37<00:05, 118MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 4.28G/4.92G [00:37<00:05, 118MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 4.30G/4.92G [00:37<00:05, 118MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 4.32G/4.92G [00:38<00:05, 118MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 4.34G/4.92G [00:38<00:04, 118MB/s]Downloading pytorch_model.bin:  89%|████████▊ | 4.36G/4.92G [00:38<00:04, 118MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 4.38G/4.92G [00:38<00:04, 118MB/s]Downloading pytorch_model.bin:  90%|████████▉ | 4.40G/4.92G [00:38<00:04, 118MB/s]Downloading pytorch_model.bin:  90%|████████▉ | 4.42G/4.92G [00:38<00:04, 118MB/s]Downloading pytorch_model.bin:  90%|█████████ | 4.45G/4.92G [00:39<00:03, 118MB/s]Downloading pytorch_model.bin:  91%|█████████ | 4.47G/4.92G [00:39<00:03, 118MB/s]Downloading pytorch_model.bin:  91%|█████████ | 4.49G/4.92G [00:39<00:03, 118MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 4.51G/4.92G [00:39<00:03, 118MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 4.53G/4.92G [00:39<00:03, 118MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 4.55G/4.92G [00:40<00:03, 118MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 4.57G/4.92G [00:40<00:02, 118MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 4.59G/4.92G [00:40<00:02, 118MB/s]Downloading pytorch_model.bin:  94%|█████████▍| 4.61G/4.92G [00:40<00:02, 118MB/s]Downloading pytorch_model.bin:  94%|█████████▍| 4.63G/4.92G [00:40<00:02, 118MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 4.66G/4.92G [00:41<00:02, 89.5MB/s]Downloading pytorch_model.bin:  95%|█████████▌| 4.68G/4.92G [00:41<00:02, 94.6MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 4.70G/4.92G [00:41<00:02, 98.8MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 4.72G/4.92G [00:41<00:01, 103MB/s] Downloading pytorch_model.bin:  96%|█████████▋| 4.74G/4.92G [00:41<00:01, 106MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 4.76G/4.92G [00:42<00:01, 109MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 4.78G/4.92G [00:42<00:01, 112MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 4.80G/4.92G [00:42<00:01, 114MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 4.82G/4.92G [00:42<00:00, 115MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 4.84G/4.92G [00:42<00:00, 116MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 4.87G/4.92G [00:42<00:00, 117MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 4.89G/4.92G [00:43<00:00, 117MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 4.91G/4.92G [00:43<00:00, 118MB/s]Downloading pytorch_model.bin: 100%|██████████| 4.92G/4.92G [00:43<00:00, 113MB/s]
Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 30.5kB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 376/376 [00:00<00:00, 385kB/s]
Downloading (…)ve/main/spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]Downloading (…)ve/main/spiece.model: 100%|██████████| 4.31M/4.31M [00:00<00:00, 7.42MB/s]Downloading (…)ve/main/spiece.model: 100%|██████████| 4.31M/4.31M [00:00<00:00, 7.38MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 66.2kB/s]
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name_}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 48, in __init__
    self.output_dir = f'/home/user/ksharma/ks_thesis/saved_models/final_models/{self.run_name}_{self.timestamp}'
AttributeError: 'ModelTrainer_notebook' object has no attribute 'run_name_'
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_085020-63jpmikm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/63jpmikm
################################################ t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:12<30:31, 12.29s/it]  1%|▏         | 2/150 [00:13<14:16,  5.78s/it]  2%|▏         | 3/150 [00:14<09:04,  3.71s/it]  3%|▎         | 4/150 [00:16<06:39,  2.74s/it]  3%|▎         | 5/150 [00:17<05:19,  2.20s/it]  4%|▍         | 6/150 [00:18<04:30,  1.88s/it]  5%|▍         | 7/150 [00:19<03:59,  1.68s/it]  5%|▌         | 8/150 [00:21<03:38,  1.54s/it]  6%|▌         | 9/150 [00:22<03:24,  1.45s/it]  7%|▋         | 10/150 [00:23<03:14,  1.39s/it]  7%|▋         | 11/150 [00:24<03:06,  1.35s/it]Terminated
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_085125-btylktu1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/btylktu1
################################################ flan-t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:15<38:01, 15.31s/it]  1%|▏         | 2/150 [00:19<21:49,  8.85s/it]Terminated
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 28, in __init__
    self.model = modelssssss
NameError: name 'modelssssss' is not defined
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 28, in __init__
    self.model = modelssssss
NameError: name 'modelssssss' is not defined
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 236, in main
    my_instance = ModelTrainer_notebook(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 28, in __init__
    self.model = modelssssss
NameError: name 'modelssssss' is not defined
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_085857-8dd5tzl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/8dd5tzl7
################################################ flan-t5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:12<30:33, 12.30s/it]  1%|▏         | 2/150 [00:13<14:29,  5.87s/it]  2%|▏         | 3/150 [00:15<09:21,  3.82s/it]  3%|▎         | 4/150 [00:16<06:57,  2.86s/it]  3%|▎         | 5/150 [00:17<05:37,  2.33s/it]  4%|▍         | 6/150 [00:19<04:46,  1.99s/it]  5%|▍         | 7/150 [00:20<04:15,  1.79s/it]  5%|▌         | 8/150 [00:21<03:54,  1.65s/it]  6%|▌         | 9/150 [00:23<03:40,  1.56s/it]  7%|▋         | 10/150 [00:24<03:29,  1.50s/it]  7%|▋         | 11/150 [00:26<03:23,  1.47s/it]  8%|▊         | 12/150 [00:27<03:18,  1.44s/it]  9%|▊         | 13/150 [00:28<03:13,  1.41s/it]  9%|▉         | 14/150 [00:30<03:10,  1.40s/it] 10%|█         | 15/150 [00:31<03:08,  1.40s/it]                                                 10%|█         | 15/150 [00:31<03:08,  1.40s/it]{'loss': 13.6703, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.34it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:01<00:00,  2.34it/s][A 10%|█         | 15/150 [00:33<03:08,  1.40s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [00:39<07:19,  3.28s/it] 11%|█▏        | 17/150 [00:40<06:00,  2.71s/it] 12%|█▏        | 18/150 [00:41<05:04,  2.31s/it] 13%|█▎        | 19/150 [00:43<04:25,  2.02s/it] 13%|█▎        | 20/150 [00:44<03:57,  1.82s/it] 14%|█▍        | 21/150 [00:46<03:38,  1.69s/it] 15%|█▍        | 22/150 [00:47<03:24,  1.60s/it] 15%|█▌        | 23/150 [00:48<03:14,  1.53s/it] 16%|█▌        | 24/150 [00:50<03:06,  1.48s/it] 17%|█▋        | 25/150 [00:51<03:00,  1.44s/it] 17%|█▋        | 26/150 [00:52<02:55,  1.42s/it] 18%|█▊        | 27/150 [00:54<02:52,  1.41s/it] 19%|█▊        | 28/150 [00:55<02:50,  1.40s/it] 19%|█▉        | 29/150 [00:56<02:48,  1.39s/it] 20%|██        | 30/150 [00:58<02:46,  1.39s/it]                                                 20%|██        | 30/150 [00:58<02:46,  1.39s/it]int64
{'eval_loss': 3.3059287071228027, 'eval_rouge1': 58.7665, 'eval_rouge2': 47.2987, 'eval_rougeL': 58.4069, 'eval_rougeLsum': 58.4574, 'eval_runtime': 1.8763, 'eval_samples_per_second': 21.318, 'eval_steps_per_second': 1.066, 'epoch': 1.0}
{'loss': 3.3854, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 20%|██        | 30/150 [01:00<02:46,  1.39s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [01:06<06:51,  3.46s/it] 21%|██▏       | 32/150 [01:08<05:33,  2.83s/it] 22%|██▏       | 33/150 [01:09<04:39,  2.39s/it] 23%|██▎       | 34/150 [01:10<04:02,  2.09s/it] 23%|██▎       | 35/150 [01:12<03:36,  1.88s/it] 24%|██▍       | 36/150 [01:13<03:17,  1.73s/it] 25%|██▍       | 37/150 [01:14<03:03,  1.63s/it] 25%|██▌       | 38/150 [01:16<02:53,  1.55s/it] 26%|██▌       | 39/150 [01:17<02:45,  1.49s/it] 27%|██▋       | 40/150 [01:19<02:40,  1.46s/it] 27%|██▋       | 41/150 [01:20<02:36,  1.44s/it] 28%|██▊       | 42/150 [01:21<02:33,  1.42s/it] 29%|██▊       | 43/150 [01:23<02:30,  1.40s/it] 29%|██▉       | 44/150 [01:24<02:28,  1.40s/it] 30%|███       | 45/150 [01:25<02:25,  1.39s/it]                                                 30%|███       | 45/150 [01:25<02:25,  1.39s/it]int64
{'eval_loss': 1.6283767223358154, 'eval_rouge1': 67.2051, 'eval_rouge2': 57.5642, 'eval_rougeL': 66.9095, 'eval_rougeLsum': 66.8329, 'eval_runtime': 1.9825, 'eval_samples_per_second': 20.177, 'eval_steps_per_second': 1.009, 'epoch': 2.0}
{'loss': 2.0858, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 30%|███       | 45/150 [01:27<02:25,  1.39s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [01:33<05:50,  3.37s/it] 31%|███▏      | 47/150 [01:35<04:45,  2.77s/it] 32%|███▏      | 48/150 [01:36<04:00,  2.36s/it] 33%|███▎      | 49/150 [01:38<03:32,  2.11s/it] 33%|███▎      | 50/150 [01:39<03:08,  1.89s/it] 34%|███▍      | 51/150 [01:40<02:51,  1.73s/it] 35%|███▍      | 52/150 [01:42<02:39,  1.63s/it] 35%|███▌      | 53/150 [01:43<02:29,  1.54s/it] 36%|███▌      | 54/150 [01:45<02:23,  1.49s/it] 37%|███▋      | 55/150 [01:46<02:18,  1.46s/it] 37%|███▋      | 56/150 [01:47<02:14,  1.43s/it] 38%|███▊      | 57/150 [01:49<02:11,  1.41s/it] 39%|███▊      | 58/150 [01:50<02:13,  1.45s/it] 39%|███▉      | 59/150 [01:52<02:10,  1.43s/it] 40%|████      | 60/150 [01:53<02:07,  1.42s/it]                                                 40%|████      | 60/150 [01:53<02:07,  1.42s/it]int64
{'eval_loss': 0.9933831095695496, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.6429, 'eval_rougeL': 66.6667, 'eval_rougeLsum': 66.6667, 'eval_runtime': 1.9166, 'eval_samples_per_second': 20.87, 'eval_steps_per_second': 1.044, 'epoch': 3.0}
{'loss': 1.2228, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 40%|████      | 60/150 [01:55<02:07,  1.42s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [02:01<05:03,  3.41s/it] 41%|████▏     | 62/150 [02:02<04:06,  2.80s/it] 42%|████▏     | 63/150 [02:04<03:27,  2.38s/it] 43%|████▎     | 64/150 [02:05<02:59,  2.08s/it] 43%|████▎     | 65/150 [02:07<02:38,  1.87s/it] 44%|████▍     | 66/150 [02:08<02:24,  1.72s/it] 45%|████▍     | 67/150 [02:09<02:13,  1.61s/it] 45%|████▌     | 68/150 [02:11<02:06,  1.54s/it] 46%|████▌     | 69/150 [02:12<02:01,  1.50s/it] 47%|████▋     | 70/150 [02:13<01:57,  1.47s/it] 47%|████▋     | 71/150 [02:15<01:53,  1.44s/it] 48%|████▊     | 72/150 [02:16<01:50,  1.42s/it] 49%|████▊     | 73/150 [02:18<01:48,  1.41s/it] 49%|████▉     | 74/150 [02:19<01:46,  1.40s/it] 50%|█████     | 75/150 [02:20<01:44,  1.40s/it]                                                 50%|█████     | 75/150 [02:20<01:44,  1.40s/it]int64
{'eval_loss': 0.2728580832481384, 'eval_rouge1': 66.1111, 'eval_rouge2': 56.9286, 'eval_rougeL': 66.1508, 'eval_rougeLsum': 66.1706, 'eval_runtime': 1.9203, 'eval_samples_per_second': 20.83, 'eval_steps_per_second': 1.042, 'epoch': 4.0}
{'loss': 0.595, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 50%|█████     | 75/150 [02:22<01:44,  1.40s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [02:29<04:16,  3.47s/it] 51%|█████▏    | 77/150 [02:30<03:27,  2.84s/it] 52%|█████▏    | 78/150 [02:31<02:52,  2.40s/it] 53%|█████▎    | 79/150 [02:33<02:28,  2.10s/it] 53%|█████▎    | 80/150 [02:34<02:12,  1.89s/it] 54%|█████▍    | 81/150 [02:36<01:59,  1.73s/it] 55%|█████▍    | 82/150 [02:37<01:50,  1.62s/it] 55%|█████▌    | 83/150 [02:38<01:43,  1.55s/it] 56%|█████▌    | 84/150 [02:40<01:38,  1.49s/it] 57%|█████▋    | 85/150 [02:41<01:34,  1.45s/it] 57%|█████▋    | 86/150 [02:42<01:31,  1.43s/it] 58%|█████▊    | 87/150 [02:44<01:29,  1.42s/it] 59%|█████▊    | 88/150 [02:45<01:27,  1.41s/it] 59%|█████▉    | 89/150 [02:47<01:25,  1.40s/it] 60%|██████    | 90/150 [02:48<01:26,  1.44s/it]                                                 60%|██████    | 90/150 [02:48<01:26,  1.44s/it]int64
{'eval_loss': 0.10267700999975204, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.6429, 'eval_rougeL': 66.6667, 'eval_rougeLsum': 66.6667, 'eval_runtime': 1.9168, 'eval_samples_per_second': 20.868, 'eval_steps_per_second': 1.043, 'epoch': 5.0}
{'loss': 0.2653, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.24it/s][A                                                
                                             [A 60%|██████    | 90/150 [02:50<01:26,  1.44s/it]
100%|██████████| 2/2 [00:01<00:00,  2.24it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [02:56<03:15,  3.31s/it] 61%|██████▏   | 92/150 [02:57<02:38,  2.73s/it] 62%|██████▏   | 93/150 [02:59<02:15,  2.38s/it] 63%|██████▎   | 94/150 [03:00<01:56,  2.08s/it] 63%|██████▎   | 95/150 [03:01<01:42,  1.86s/it] 64%|██████▍   | 96/150 [03:03<01:32,  1.72s/it] 65%|██████▍   | 97/150 [03:04<01:25,  1.61s/it] 65%|██████▌   | 98/150 [03:06<01:20,  1.55s/it] 66%|██████▌   | 99/150 [03:07<01:16,  1.50s/it] 67%|██████▋   | 100/150 [03:08<01:12,  1.46s/it] 67%|██████▋   | 101/150 [03:10<01:10,  1.44s/it] 68%|██████▊   | 102/150 [03:11<01:10,  1.47s/it] 69%|██████▊   | 103/150 [03:13<01:07,  1.44s/it] 69%|██████▉   | 104/150 [03:14<01:05,  1.42s/it] 70%|███████   | 105/150 [03:15<01:03,  1.40s/it]                                                  70%|███████   | 105/150 [03:15<01:03,  1.40s/it]int64
{'eval_loss': 0.03157424181699753, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.6429, 'eval_rougeL': 66.6667, 'eval_rougeLsum': 66.6667, 'eval_runtime': 1.9114, 'eval_samples_per_second': 20.927, 'eval_steps_per_second': 1.046, 'epoch': 6.0}
{'loss': 0.1366, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.34it/s][A                                                 
                                             [A 70%|███████   | 105/150 [03:17<01:03,  1.40s/it]
100%|██████████| 2/2 [00:00<00:00,  2.34it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 106/150 [03:23<02:25,  3.31s/it] 71%|███████▏  | 107/150 [03:24<01:57,  2.72s/it] 72%|███████▏  | 108/150 [03:26<01:37,  2.32s/it] 73%|███████▎  | 109/150 [03:27<01:23,  2.03s/it] 73%|███████▎  | 110/150 [03:29<01:13,  1.83s/it] 74%|███████▍  | 111/150 [03:30<01:05,  1.68s/it] 75%|███████▍  | 112/150 [03:31<01:00,  1.59s/it] 75%|███████▌  | 113/150 [03:33<00:56,  1.52s/it] 76%|███████▌  | 114/150 [03:34<00:54,  1.52s/it] 77%|███████▋  | 115/150 [03:36<00:51,  1.48s/it] 77%|███████▋  | 116/150 [03:37<00:49,  1.45s/it] 78%|███████▊  | 117/150 [03:38<00:47,  1.43s/it] 79%|███████▊  | 118/150 [03:40<00:45,  1.41s/it] 79%|███████▉  | 119/150 [03:41<00:43,  1.40s/it] 80%|████████  | 120/150 [03:42<00:41,  1.39s/it]                                                  80%|████████  | 120/150 [03:42<00:41,  1.39s/it]int64
{'eval_loss': 0.018739864230155945, 'eval_rouge1': 66.1905, 'eval_rouge2': 56.619, 'eval_rougeL': 66.1905, 'eval_rougeLsum': 66.3095, 'eval_runtime': 1.8284, 'eval_samples_per_second': 21.877, 'eval_steps_per_second': 1.094, 'epoch': 7.0}
{'loss': 0.0944, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.24it/s][A                                                 
                                             [A 80%|████████  | 120/150 [03:44<00:41,  1.39s/it]
100%|██████████| 2/2 [00:01<00:00,  2.24it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 121/150 [03:50<01:34,  3.25s/it] 81%|████████▏ | 122/150 [03:51<01:15,  2.68s/it] 82%|████████▏ | 123/150 [03:53<01:01,  2.28s/it] 83%|████████▎ | 124/150 [03:54<00:52,  2.01s/it] 83%|████████▎ | 125/150 [03:55<00:45,  1.82s/it] 84%|████████▍ | 126/150 [03:57<00:40,  1.68s/it] 85%|████████▍ | 127/150 [03:58<00:36,  1.59s/it] 85%|████████▌ | 128/150 [04:00<00:33,  1.52s/it] 86%|████████▌ | 129/150 [04:01<00:30,  1.48s/it] 87%|████████▋ | 130/150 [04:02<00:28,  1.44s/it] 87%|████████▋ | 131/150 [04:04<00:26,  1.41s/it] 88%|████████▊ | 132/150 [04:05<00:25,  1.40s/it] 89%|████████▊ | 133/150 [04:06<00:23,  1.39s/it] 89%|████████▉ | 134/150 [04:08<00:22,  1.39s/it] 90%|█████████ | 135/150 [04:09<00:20,  1.38s/it]                                                  90%|█████████ | 135/150 [04:09<00:20,  1.38s/it]int64
{'eval_loss': 0.01887449622154236, 'eval_rouge1': 66.1905, 'eval_rouge2': 56.619, 'eval_rougeL': 66.1905, 'eval_rougeLsum': 66.3095, 'eval_runtime': 1.9108, 'eval_samples_per_second': 20.934, 'eval_steps_per_second': 1.047, 'epoch': 8.0}
{'loss': 0.0764, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.22it/s][A                                                 
                                             [A 90%|█████████ | 135/150 [04:11<00:20,  1.38s/it]
100%|██████████| 2/2 [00:01<00:00,  2.22it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 136/150 [04:18<00:49,  3.52s/it] 91%|█████████▏| 137/150 [04:19<00:37,  2.88s/it] 92%|█████████▏| 138/150 [04:20<00:29,  2.42s/it] 93%|█████████▎| 139/150 [04:22<00:23,  2.11s/it] 93%|█████████▎| 140/150 [04:23<00:18,  1.88s/it] 94%|█████████▍| 141/150 [04:24<00:15,  1.72s/it] 95%|█████████▍| 142/150 [04:26<00:12,  1.62s/it] 95%|█████████▌| 143/150 [04:27<00:10,  1.54s/it] 96%|█████████▌| 144/150 [04:29<00:08,  1.50s/it] 97%|█████████▋| 145/150 [04:30<00:07,  1.46s/it] 97%|█████████▋| 146/150 [04:31<00:05,  1.44s/it] 98%|█████████▊| 147/150 [04:33<00:04,  1.42s/it] 99%|█████████▊| 148/150 [04:34<00:02,  1.40s/it] 99%|█████████▉| 149/150 [04:35<00:01,  1.40s/it]100%|██████████| 150/150 [04:37<00:00,  1.39s/it]                                                 100%|██████████| 150/150 [04:37<00:00,  1.39s/it]int64
{'eval_loss': 0.01753879152238369, 'eval_rouge1': 66.1905, 'eval_rouge2': 56.619, 'eval_rougeL': 66.1905, 'eval_rougeLsum': 66.3095, 'eval_runtime': 1.9161, 'eval_samples_per_second': 20.876, 'eval_steps_per_second': 1.044, 'epoch': 9.0}
{'loss': 0.0696, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.27it/s][A                                                 
                                             [A100%|██████████| 150/150 [04:39<00:00,  1.39s/it]
100%|██████████| 2/2 [00:01<00:00,  2.27it/s][A
                                             [A                                                 100%|██████████| 150/150 [04:44<00:00,  1.39s/it]100%|██████████| 150/150 [04:44<00:00,  1.90s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.017273273319005966, 'eval_rouge1': 66.1905, 'eval_rouge2': 56.619, 'eval_rougeL': 66.1905, 'eval_rougeLsum': 66.3095, 'eval_runtime': 1.874, 'eval_samples_per_second': 21.345, 'eval_steps_per_second': 1.067, 'epoch': 10.0}
{'train_runtime': 291.9347, 'train_samples_per_second': 12.332, 'train_steps_per_second': 0.514, 'train_loss': 2.1601596442858377, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▄▃▂▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁██▇██▇▇▇▇
wandb:                    eval/rouge2 ▁█████▇▇▇▇
wandb:                    eval/rougeL ▁██▇██▇▇▇▇
wandb:                 eval/rougeLsum ▁██▇██████
wandb:                   eval/runtime ▃█▅▅▅▅▁▅▅▃
wandb:        eval/samples_per_second ▆▁▄▄▄▄█▄▄▆
wandb:          eval/steps_per_second ▆▁▄▄▄▄█▄▄▆
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▃▂▂▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01727
wandb:                    eval/rouge1 66.1905
wandb:                    eval/rouge2 56.619
wandb:                    eval/rougeL 66.1905
wandb:                 eval/rougeLsum 66.3095
wandb:                   eval/runtime 1.874
wandb:        eval/samples_per_second 21.345
wandb:          eval/steps_per_second 1.067
wandb:                    train/epoch 10.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0696
wandb:               train/total_flos 81849902284800.0
wandb:               train/train_loss 2.16016
wandb:            train/train_runtime 291.9347
wandb: train/train_samples_per_second 12.332
wandb:   train/train_steps_per_second 0.514
wandb: 
wandb: 🚀 View run flan-t5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/8dd5tzl7
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_085857-8dd5tzl7/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_090435-4q2akugc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/4q2akugc
################################################ t5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:12<30:20, 12.22s/it]  1%|▏         | 2/150 [00:13<14:12,  5.76s/it]  2%|▏         | 3/150 [00:14<09:03,  3.70s/it]  3%|▎         | 4/150 [00:15<06:37,  2.72s/it]  3%|▎         | 5/150 [00:17<05:16,  2.18s/it]  4%|▍         | 6/150 [00:18<04:27,  1.86s/it]  5%|▍         | 7/150 [00:19<03:57,  1.66s/it]  5%|▌         | 8/150 [00:20<03:36,  1.52s/it]  6%|▌         | 9/150 [00:22<03:22,  1.43s/it]  7%|▋         | 10/150 [00:23<03:11,  1.37s/it]  7%|▋         | 11/150 [00:24<03:03,  1.32s/it]  8%|▊         | 12/150 [00:25<02:58,  1.29s/it]  9%|▊         | 13/150 [00:26<02:54,  1.27s/it]  9%|▉         | 14/150 [00:28<02:50,  1.25s/it] 10%|█         | 15/150 [00:29<02:47,  1.24s/it]                                                 10%|█         | 15/150 [00:29<02:47,  1.24s/it]{'loss': 4.0394, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.45it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:00<00:00,  2.45it/s][A 10%|█         | 15/150 [00:31<02:47,  1.24s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [00:36<06:39,  2.98s/it] 11%|█▏        | 17/150 [00:37<05:26,  2.45s/it] 12%|█▏        | 18/150 [00:38<04:35,  2.09s/it] 13%|█▎        | 19/150 [00:40<03:59,  1.83s/it] 13%|█▎        | 20/150 [00:41<03:34,  1.65s/it] 14%|█▍        | 21/150 [00:42<03:16,  1.53s/it] 15%|█▍        | 22/150 [00:43<03:04,  1.44s/it] 15%|█▌        | 23/150 [00:45<02:53,  1.37s/it] 16%|█▌        | 24/150 [00:46<02:47,  1.33s/it] 17%|█▋        | 25/150 [00:47<02:42,  1.30s/it] 17%|█▋        | 26/150 [00:48<02:38,  1.28s/it] 18%|█▊        | 27/150 [00:49<02:35,  1.26s/it] 19%|█▊        | 28/150 [00:51<02:33,  1.26s/it] 19%|█▉        | 29/150 [00:52<02:30,  1.25s/it] 20%|██        | 30/150 [00:53<02:29,  1.24s/it]                                                 20%|██        | 30/150 [00:53<02:29,  1.24s/it]int64
{'eval_loss': 0.5890740752220154, 'eval_rouge1': 2.3077, 'eval_rouge2': 1.8182, 'eval_rougeL': 1.5385, 'eval_rougeLsum': 1.5385, 'eval_runtime': 1.7877, 'eval_samples_per_second': 22.376, 'eval_steps_per_second': 1.119, 'epoch': 1.0}
{'loss': 0.5093, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.48it/s][A                                                
                                             [A 20%|██        | 30/150 [00:55<02:29,  1.24s/it]
100%|██████████| 2/2 [00:00<00:00,  2.48it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [01:01<06:06,  3.08s/it] 21%|██▏       | 32/150 [01:02<04:57,  2.52s/it] 22%|██▏       | 33/150 [01:03<04:10,  2.14s/it] 23%|██▎       | 34/150 [01:04<03:36,  1.86s/it] 23%|██▎       | 35/150 [01:05<03:12,  1.67s/it] 24%|██▍       | 36/150 [01:07<02:55,  1.54s/it] 25%|██▍       | 37/150 [01:08<02:43,  1.45s/it] 25%|██▌       | 38/150 [01:09<02:34,  1.38s/it] 26%|██▌       | 39/150 [01:10<02:28,  1.34s/it] 27%|██▋       | 40/150 [01:12<02:23,  1.31s/it] 27%|██▋       | 41/150 [01:13<02:19,  1.28s/it] 28%|██▊       | 42/150 [01:14<02:16,  1.26s/it] 29%|██▊       | 43/150 [01:15<02:13,  1.25s/it] 29%|██▉       | 44/150 [01:16<02:12,  1.25s/it] 30%|███       | 45/150 [01:18<02:09,  1.24s/it]                                                 30%|███       | 45/150 [01:18<02:09,  1.24s/it]int64
{'eval_loss': 0.05825648829340935, 'eval_rouge1': 62.7183, 'eval_rouge2': 53.631, 'eval_rougeL': 62.4802, 'eval_rougeLsum': 62.4008, 'eval_runtime': 1.7769, 'eval_samples_per_second': 22.511, 'eval_steps_per_second': 1.126, 'epoch': 2.0}
{'loss': 0.1018, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.59it/s][A                                                
                                             [A 30%|███       | 45/150 [01:19<02:09,  1.24s/it]
100%|██████████| 2/2 [00:00<00:00,  2.59it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [01:25<05:09,  2.97s/it] 31%|███▏      | 47/150 [01:26<04:12,  2.45s/it] 32%|███▏      | 48/150 [01:27<03:39,  2.15s/it] 33%|███▎      | 49/150 [01:29<03:09,  1.88s/it] 33%|███▎      | 50/150 [01:30<02:47,  1.68s/it] 34%|███▍      | 51/150 [01:31<02:32,  1.54s/it] 35%|███▍      | 52/150 [01:32<02:22,  1.45s/it] 35%|███▌      | 53/150 [01:34<02:13,  1.38s/it] 36%|███▌      | 54/150 [01:35<02:08,  1.33s/it] 37%|███▋      | 55/150 [01:36<02:03,  1.30s/it] 37%|███▋      | 56/150 [01:37<02:00,  1.28s/it] 38%|███▊      | 57/150 [01:38<01:58,  1.27s/it] 39%|███▊      | 58/150 [01:40<01:56,  1.27s/it] 39%|███▉      | 59/150 [01:41<01:54,  1.26s/it] 40%|████      | 60/150 [01:42<01:52,  1.25s/it]                                                 40%|████      | 60/150 [01:42<01:52,  1.25s/it]int64
{'eval_loss': 0.02214045636355877, 'eval_rouge1': 64.1468, 'eval_rouge2': 54.2857, 'eval_rougeL': 64.1468, 'eval_rougeLsum': 64.1667, 'eval_runtime': 1.6919, 'eval_samples_per_second': 23.643, 'eval_steps_per_second': 1.182, 'epoch': 3.0}
{'loss': 0.0445, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A                                                
                                             [A 40%|████      | 60/150 [01:44<01:52,  1.25s/it]
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [01:49<04:31,  3.05s/it] 41%|████▏     | 62/150 [01:51<03:40,  2.50s/it] 42%|████▏     | 63/150 [01:52<03:04,  2.12s/it] 43%|████▎     | 64/150 [01:53<02:38,  1.85s/it] 43%|████▎     | 65/150 [01:54<02:22,  1.67s/it] 44%|████▍     | 66/150 [01:56<02:09,  1.54s/it] 45%|████▍     | 67/150 [01:57<02:00,  1.45s/it] 45%|████▌     | 68/150 [01:58<01:53,  1.38s/it] 46%|████▌     | 69/150 [01:59<01:48,  1.34s/it] 47%|████▋     | 70/150 [02:01<01:49,  1.36s/it] 47%|████▋     | 71/150 [02:02<01:44,  1.32s/it] 48%|████▊     | 72/150 [02:03<01:41,  1.30s/it] 49%|████▊     | 73/150 [02:04<01:38,  1.28s/it] 49%|████▉     | 74/150 [02:06<01:36,  1.27s/it] 50%|█████     | 75/150 [02:07<01:34,  1.25s/it]                                                 50%|█████     | 75/150 [02:07<01:34,  1.25s/it]int64
{'eval_loss': 0.01649179682135582, 'eval_rouge1': 65.1389, 'eval_rouge2': 55.1667, 'eval_rougeL': 65.1984, 'eval_rougeLsum': 65.119, 'eval_runtime': 1.7142, 'eval_samples_per_second': 23.335, 'eval_steps_per_second': 1.167, 'epoch': 4.0}
{'loss': 0.0386, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.53it/s][A                                                
                                             [A 50%|█████     | 75/150 [02:09<01:34,  1.25s/it]
100%|██████████| 2/2 [00:00<00:00,  2.53it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [02:14<03:43,  3.02s/it] 51%|█████▏    | 77/150 [02:15<03:01,  2.48s/it] 52%|█████▏    | 78/150 [02:16<02:31,  2.11s/it] 53%|█████▎    | 79/150 [02:18<02:10,  1.84s/it] 53%|█████▎    | 80/150 [02:19<01:56,  1.66s/it] 54%|█████▍    | 81/150 [02:20<01:46,  1.54s/it] 55%|█████▍    | 82/150 [02:21<01:38,  1.45s/it] 55%|█████▌    | 83/150 [02:23<01:32,  1.38s/it] 56%|█████▌    | 84/150 [02:24<01:27,  1.33s/it] 57%|█████▋    | 85/150 [02:25<01:25,  1.31s/it] 57%|█████▋    | 86/150 [02:26<01:22,  1.29s/it] 58%|█████▊    | 87/150 [02:28<01:20,  1.27s/it] 59%|█████▊    | 88/150 [02:29<01:18,  1.27s/it] 59%|█████▉    | 89/150 [02:30<01:16,  1.26s/it] 60%|██████    | 90/150 [02:31<01:14,  1.25s/it]                                                 60%|██████    | 90/150 [02:31<01:14,  1.25s/it]int64
{'eval_loss': 0.013797631487250328, 'eval_rouge1': 65.754, 'eval_rouge2': 55.881, 'eval_rougeL': 65.6548, 'eval_rougeLsum': 65.6944, 'eval_runtime': 1.7055, 'eval_samples_per_second': 23.454, 'eval_steps_per_second': 1.173, 'epoch': 5.0}
{'loss': 0.0325, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.51it/s][A                                                
                                             [A 60%|██████    | 90/150 [02:33<01:14,  1.25s/it]
100%|██████████| 2/2 [00:00<00:00,  2.51it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [02:38<02:57,  3.01s/it] 61%|██████▏   | 92/150 [02:40<02:23,  2.47s/it] 62%|██████▏   | 93/150 [02:41<01:59,  2.10s/it] 63%|██████▎   | 94/150 [02:42<01:42,  1.84s/it] 63%|██████▎   | 95/150 [02:43<01:31,  1.65s/it] 64%|██████▍   | 96/150 [02:45<01:22,  1.53s/it] 65%|██████▍   | 97/150 [02:46<01:16,  1.44s/it] 65%|██████▌   | 98/150 [02:47<01:10,  1.37s/it] 66%|██████▌   | 99/150 [02:48<01:07,  1.32s/it] 67%|██████▋   | 100/150 [02:49<01:04,  1.29s/it] 67%|██████▋   | 101/150 [02:51<01:02,  1.27s/it] 68%|██████▊   | 102/150 [02:52<01:00,  1.25s/it] 69%|██████▊   | 103/150 [02:53<00:58,  1.24s/it] 69%|██████▉   | 104/150 [02:54<00:56,  1.24s/it] 70%|███████   | 105/150 [02:56<00:55,  1.23s/it]                                                  70%|███████   | 105/150 [02:56<00:55,  1.23s/it]int64
{'eval_loss': 0.013250470161437988, 'eval_rouge1': 65.754, 'eval_rouge2': 55.881, 'eval_rougeL': 65.6548, 'eval_rougeLsum': 65.6944, 'eval_runtime': 1.7187, 'eval_samples_per_second': 23.273, 'eval_steps_per_second': 1.164, 'epoch': 6.0}
{'loss': 0.0301, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.61it/s][A                                                 
                                             [A 70%|███████   | 105/150 [02:57<00:55,  1.23s/it]
100%|██████████| 2/2 [00:00<00:00,  2.61it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 106/150 [03:03<02:10,  2.96s/it] 71%|███████▏  | 107/150 [03:04<01:45,  2.44s/it] 72%|███████▏  | 108/150 [03:05<01:27,  2.08s/it] 73%|███████▎  | 109/150 [03:06<01:14,  1.82s/it] 73%|███████▎  | 110/150 [03:07<01:05,  1.64s/it] 74%|███████▍  | 111/150 [03:09<00:59,  1.52s/it] 75%|███████▍  | 112/150 [03:10<00:54,  1.43s/it] 75%|███████▌  | 113/150 [03:11<00:50,  1.37s/it] 76%|███████▌  | 114/150 [03:12<00:47,  1.33s/it] 77%|███████▋  | 115/150 [03:14<00:45,  1.30s/it] 77%|███████▋  | 116/150 [03:15<00:43,  1.28s/it] 78%|███████▊  | 117/150 [03:16<00:41,  1.26s/it] 79%|███████▊  | 118/150 [03:17<00:39,  1.24s/it] 79%|███████▉  | 119/150 [03:18<00:38,  1.23s/it] 80%|████████  | 120/150 [03:20<00:36,  1.22s/it]                                                  80%|████████  | 120/150 [03:20<00:36,  1.22s/it]int64
{'eval_loss': 0.013242361135780811, 'eval_rouge1': 65.754, 'eval_rouge2': 55.881, 'eval_rougeL': 65.6548, 'eval_rougeLsum': 65.6944, 'eval_runtime': 1.6477, 'eval_samples_per_second': 24.276, 'eval_steps_per_second': 1.214, 'epoch': 7.0}
{'loss': 0.0268, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A                                                 
                                             [A 80%|████████  | 120/150 [03:21<00:36,  1.22s/it]
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 121/150 [03:27<01:27,  3.02s/it] 81%|████████▏ | 122/150 [03:28<01:09,  2.49s/it] 82%|████████▏ | 123/150 [03:29<00:56,  2.11s/it] 83%|████████▎ | 124/150 [03:31<00:47,  1.84s/it] 83%|████████▎ | 125/150 [03:32<00:41,  1.65s/it] 84%|████████▍ | 126/150 [03:33<00:36,  1.52s/it] 85%|████████▍ | 127/150 [03:34<00:33,  1.47s/it] 85%|████████▌ | 128/150 [03:36<00:30,  1.39s/it] 86%|████████▌ | 129/150 [03:37<00:28,  1.34s/it] 87%|████████▋ | 130/150 [03:38<00:26,  1.30s/it] 87%|████████▋ | 131/150 [03:39<00:24,  1.27s/it] 88%|████████▊ | 132/150 [03:40<00:22,  1.25s/it] 89%|████████▊ | 133/150 [03:42<00:21,  1.25s/it] 89%|████████▉ | 134/150 [03:43<00:19,  1.24s/it] 90%|█████████ | 135/150 [03:44<00:18,  1.23s/it]                                                  90%|█████████ | 135/150 [03:44<00:18,  1.23s/it]int64
{'eval_loss': 0.013216061517596245, 'eval_rouge1': 65.754, 'eval_rouge2': 55.881, 'eval_rougeL': 65.6548, 'eval_rougeLsum': 65.6944, 'eval_runtime': 1.7322, 'eval_samples_per_second': 23.091, 'eval_steps_per_second': 1.155, 'epoch': 8.0}
{'loss': 0.0206, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.67it/s][A                                                 
                                             [A 90%|█████████ | 135/150 [03:46<00:18,  1.23s/it]
100%|██████████| 2/2 [00:00<00:00,  2.67it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 136/150 [03:51<00:42,  3.01s/it] 91%|█████████▏| 137/150 [03:52<00:32,  2.48s/it] 92%|█████████▏| 138/150 [03:54<00:25,  2.10s/it] 93%|█████████▎| 139/150 [03:55<00:20,  1.84s/it] 93%|█████████▎| 140/150 [03:56<00:16,  1.66s/it] 94%|█████████▍| 141/150 [03:57<00:13,  1.53s/it] 95%|█████████▍| 142/150 [03:59<00:11,  1.45s/it] 95%|█████████▌| 143/150 [04:00<00:09,  1.38s/it] 96%|█████████▌| 144/150 [04:01<00:07,  1.33s/it] 97%|█████████▋| 145/150 [04:02<00:06,  1.30s/it] 97%|█████████▋| 146/150 [04:04<00:05,  1.28s/it] 98%|█████████▊| 147/150 [04:05<00:03,  1.27s/it] 99%|█████████▊| 148/150 [04:06<00:02,  1.26s/it] 99%|█████████▉| 149/150 [04:07<00:01,  1.25s/it]100%|██████████| 150/150 [04:08<00:00,  1.24s/it]                                                 100%|██████████| 150/150 [04:08<00:00,  1.24s/it]int64
{'eval_loss': 0.012950986623764038, 'eval_rouge1': 65.754, 'eval_rouge2': 55.881, 'eval_rougeL': 65.6548, 'eval_rougeLsum': 65.6944, 'eval_runtime': 1.6473, 'eval_samples_per_second': 24.283, 'eval_steps_per_second': 1.214, 'epoch': 9.0}
{'loss': 0.0234, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A                                                 
                                             [A100%|██████████| 150/150 [04:10<00:00,  1.24s/it]
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A
                                             [A                                                 100%|██████████| 150/150 [04:16<00:00,  1.24s/it]100%|██████████| 150/150 [04:16<00:00,  1.71s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.012891465798020363, 'eval_rouge1': 65.754, 'eval_rouge2': 55.881, 'eval_rougeL': 65.6548, 'eval_rougeLsum': 65.6944, 'eval_runtime': 1.7145, 'eval_samples_per_second': 23.331, 'eval_steps_per_second': 1.167, 'epoch': 10.0}
{'train_runtime': 264.8554, 'train_samples_per_second': 13.592, 'train_steps_per_second': 0.566, 'train_loss': 0.48669879118601483, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁█████████
wandb:                    eval/rouge2 ▁█████████
wandb:                    eval/rougeL ▁█████████
wandb:                 eval/rougeLsum ▁█████████
wandb:                   eval/runtime █▇▃▄▄▅▁▅▁▄
wandb:        eval/samples_per_second ▁▁▆▅▅▄█▄█▅
wandb:          eval/steps_per_second ▁▂▆▅▅▄█▄█▅
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01289
wandb:                    eval/rouge1 65.754
wandb:                    eval/rouge2 55.881
wandb:                    eval/rougeL 65.6548
wandb:                 eval/rougeLsum 65.6944
wandb:                   eval/runtime 1.7145
wandb:        eval/samples_per_second 23.331
wandb:          eval/steps_per_second 1.167
wandb:                    train/epoch 10.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0234
wandb:               train/total_flos 72789497856000.0
wandb:               train/train_loss 0.4867
wandb:            train/train_runtime 264.8554
wandb: train/train_samples_per_second 13.592
wandb:   train/train_steps_per_second 0.566
wandb: 
wandb: 🚀 View run t5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/4q2akugc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_090435-4q2akugc/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_090955-ri1pui8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/ri1pui8r
################################################ flan-t5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:15<38:12, 15.39s/it]  1%|▏         | 2/150 [00:19<21:44,  8.81s/it]  2%|▏         | 3/150 [00:23<16:28,  6.72s/it]  3%|▎         | 4/150 [00:28<13:55,  5.72s/it]  3%|▎         | 5/150 [00:32<12:33,  5.20s/it]  4%|▍         | 6/150 [00:36<11:39,  4.86s/it]  5%|▍         | 7/150 [00:40<11:04,  4.65s/it]  5%|▌         | 8/150 [00:44<10:41,  4.51s/it]  6%|▌         | 9/150 [00:49<10:25,  4.43s/it]  7%|▋         | 10/150 [00:53<10:12,  4.37s/it]  7%|▋         | 11/150 [00:57<10:01,  4.33s/it]  8%|▊         | 12/150 [01:01<09:50,  4.28s/it]  9%|▊         | 13/150 [01:06<09:43,  4.26s/it]  9%|▉         | 14/150 [01:10<09:37,  4.25s/it] 10%|█         | 15/150 [01:14<09:30,  4.23s/it]                                                 10%|█         | 15/150 [01:14<09:30,  4.23s/it]{'loss': 12.5106, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.22s/it][A                                                
                                             [A 10%|█         | 15/150 [01:19<09:30,  4.23s/it]
100%|██████████| 2/2 [00:02<00:00,  1.22s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [01:35<20:41,  9.27s/it] 11%|█▏        | 17/150 [01:39<17:12,  7.76s/it] 12%|█▏        | 18/150 [01:43<14:42,  6.68s/it] 13%|█▎        | 19/150 [01:48<12:58,  5.94s/it] 13%|█▎        | 20/150 [01:52<11:46,  5.44s/it] 14%|█▍        | 21/150 [01:56<10:53,  5.07s/it] 15%|█▍        | 22/150 [02:00<10:16,  4.82s/it] 15%|█▌        | 23/150 [02:04<09:48,  4.64s/it] 16%|█▌        | 24/150 [02:09<09:29,  4.52s/it] 17%|█▋        | 25/150 [02:13<09:15,  4.45s/it] 17%|█▋        | 26/150 [02:17<09:02,  4.37s/it] 18%|█▊        | 27/150 [02:21<08:52,  4.33s/it] 19%|█▊        | 28/150 [02:26<08:43,  4.29s/it] 19%|█▉        | 29/150 [02:30<08:37,  4.28s/it] 20%|██        | 30/150 [02:34<08:31,  4.26s/it]                                                 20%|██        | 30/150 [02:34<08:31,  4.26s/it]int64
{'eval_loss': 4.790597438812256, 'eval_rouge1': 54.0732, 'eval_rouge2': 41.7601, 'eval_rougeL': 54.0798, 'eval_rougeLsum': 53.9676, 'eval_runtime': 5.0008, 'eval_samples_per_second': 7.999, 'eval_steps_per_second': 0.4, 'epoch': 1.0}
{'loss': 3.7197, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A                                                
                                             [A 20%|██        | 30/150 [02:39<08:31,  4.26s/it]
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [02:56<19:09,  9.66s/it] 21%|██▏       | 32/150 [03:01<15:45,  8.01s/it] 22%|██▏       | 33/150 [03:05<13:26,  6.89s/it] 23%|██▎       | 34/150 [03:09<11:48,  6.10s/it] 23%|██▎       | 35/150 [03:13<10:38,  5.55s/it] 24%|██▍       | 36/150 [03:18<09:47,  5.15s/it] 25%|██▍       | 37/150 [03:22<09:12,  4.89s/it] 25%|██▌       | 38/150 [03:26<08:47,  4.71s/it] 26%|██▌       | 39/150 [03:30<08:28,  4.58s/it] 27%|██▋       | 40/150 [03:35<08:13,  4.49s/it] 27%|██▋       | 41/150 [03:39<08:01,  4.42s/it] 28%|██▊       | 42/150 [03:43<07:51,  4.37s/it] 29%|██▊       | 43/150 [03:47<07:42,  4.32s/it] 29%|██▉       | 44/150 [03:52<07:37,  4.31s/it] 30%|███       | 45/150 [03:56<07:30,  4.29s/it]                                                 30%|███       | 45/150 [03:56<07:30,  4.29s/it]int64
{'eval_loss': 1.4620593786239624, 'eval_rouge1': 54.0747, 'eval_rouge2': 41.2143, 'eval_rougeL': 54.0079, 'eval_rougeLsum': 53.9827, 'eval_runtime': 5.0971, 'eval_samples_per_second': 7.848, 'eval_steps_per_second': 0.392, 'epoch': 2.0}
{'loss': 1.4434, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                
                                             [A 30%|███       | 45/150 [04:01<07:30,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [04:18<16:50,  9.71s/it] 31%|███▏      | 47/150 [04:23<13:50,  8.07s/it] 32%|███▏      | 48/150 [04:27<11:50,  6.97s/it] 33%|███▎      | 49/150 [04:31<10:20,  6.14s/it] 33%|███▎      | 50/150 [04:35<09:16,  5.56s/it] 34%|███▍      | 51/150 [04:39<08:29,  5.14s/it] 35%|███▍      | 52/150 [04:44<07:58,  4.88s/it] 35%|███▌      | 53/150 [04:48<07:33,  4.67s/it] 36%|███▌      | 54/150 [04:52<07:14,  4.52s/it] 37%|███▋      | 55/150 [04:56<07:02,  4.45s/it] 37%|███▋      | 56/150 [05:01<06:52,  4.39s/it] 38%|███▊      | 57/150 [05:05<06:43,  4.34s/it] 39%|███▊      | 58/150 [05:09<06:37,  4.32s/it] 39%|███▉      | 59/150 [05:13<06:30,  4.29s/it] 40%|████      | 60/150 [05:18<06:24,  4.27s/it]                                                 40%|████      | 60/150 [05:18<06:24,  4.27s/it]int64
{'eval_loss': 0.07771994173526764, 'eval_rouge1': 62.9744, 'eval_rouge2': 53.1468, 'eval_rougeL': 62.9708, 'eval_rougeLsum': 62.9131, 'eval_runtime': 5.0898, 'eval_samples_per_second': 7.859, 'eval_steps_per_second': 0.393, 'epoch': 3.0}
{'loss': 0.1925, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                
                                             [A 40%|████      | 60/150 [05:23<06:24,  4.27s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [05:40<14:30,  9.78s/it] 41%|████▏     | 62/150 [05:44<11:51,  8.09s/it] 42%|████▏     | 63/150 [05:49<10:02,  6.93s/it] 43%|████▎     | 64/150 [05:53<08:45,  6.12s/it] 43%|████▎     | 65/150 [05:57<07:52,  5.56s/it] 44%|████▍     | 66/150 [06:01<07:12,  5.15s/it] 45%|████▍     | 67/150 [06:06<06:45,  4.88s/it] 45%|████▌     | 68/150 [06:10<06:22,  4.67s/it] 46%|████▌     | 69/150 [06:14<06:08,  4.55s/it] 47%|████▋     | 70/150 [06:18<05:56,  4.45s/it] 47%|████▋     | 71/150 [06:22<05:45,  4.37s/it] 48%|████▊     | 72/150 [06:27<05:36,  4.31s/it] 49%|████▊     | 73/150 [06:31<05:31,  4.30s/it] 49%|████▉     | 74/150 [06:35<05:24,  4.27s/it] 50%|█████     | 75/150 [06:39<05:20,  4.28s/it]                                                 50%|█████     | 75/150 [06:39<05:20,  4.28s/it]int64
{'eval_loss': 0.025199905037879944, 'eval_rouge1': 63.5913, 'eval_rouge2': 53.4048, 'eval_rougeL': 63.6905, 'eval_rougeLsum': 63.6706, 'eval_runtime': 5.0772, 'eval_samples_per_second': 7.878, 'eval_steps_per_second': 0.394, 'epoch': 4.0}
{'loss': 0.0634, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A                                                
                                             [A 50%|█████     | 75/150 [06:44<05:20,  4.28s/it]
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [07:02<12:04,  9.79s/it] 51%|█████▏    | 77/150 [07:06<09:52,  8.12s/it] 52%|█████▏    | 78/150 [07:10<08:20,  6.95s/it] 53%|█████▎    | 79/150 [07:15<07:16,  6.15s/it] 53%|█████▎    | 80/150 [07:19<06:30,  5.57s/it] 54%|█████▍    | 81/150 [07:23<05:57,  5.18s/it] 55%|█████▍    | 82/150 [07:27<05:32,  4.89s/it] 55%|█████▌    | 83/150 [07:32<05:14,  4.70s/it] 56%|█████▌    | 84/150 [07:36<05:00,  4.56s/it] 57%|█████▋    | 85/150 [07:40<04:50,  4.47s/it] 57%|█████▋    | 86/150 [07:44<04:42,  4.41s/it] 58%|█████▊    | 87/150 [07:49<04:33,  4.34s/it] 59%|█████▊    | 88/150 [07:53<04:26,  4.30s/it] 59%|█████▉    | 89/150 [07:57<04:22,  4.31s/it] 60%|██████    | 90/150 [08:01<04:16,  4.28s/it]                                                 60%|██████    | 90/150 [08:01<04:16,  4.28s/it]int64
{'eval_loss': 0.01253447961062193, 'eval_rouge1': 64.7619, 'eval_rouge2': 54.619, 'eval_rougeL': 64.8016, 'eval_rougeLsum': 64.5833, 'eval_runtime': 5.1877, 'eval_samples_per_second': 7.71, 'eval_steps_per_second': 0.386, 'epoch': 5.0}
{'loss': 0.0425, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.27s/it][A                                                
                                             [A 60%|██████    | 90/150 [08:06<04:16,  4.28s/it]
100%|██████████| 2/2 [00:02<00:00,  1.27s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [08:24<09:32,  9.70s/it] 61%|██████▏   | 92/150 [08:28<07:48,  8.07s/it] 62%|██████▏   | 93/150 [08:32<06:34,  6.92s/it] 63%|██████▎   | 94/150 [08:36<05:42,  6.12s/it] 63%|██████▎   | 95/150 [08:41<05:05,  5.56s/it] 64%|██████▍   | 96/150 [08:45<04:38,  5.15s/it] 65%|██████▍   | 97/150 [08:49<04:17,  4.86s/it] 65%|██████▌   | 98/150 [08:53<04:02,  4.67s/it] 66%|██████▌   | 99/150 [08:58<03:51,  4.54s/it] 67%|██████▋   | 100/150 [09:02<03:42,  4.46s/it] 67%|██████▋   | 101/150 [09:06<03:35,  4.39s/it] 68%|██████▊   | 102/150 [09:10<03:28,  4.35s/it] 69%|██████▊   | 103/150 [09:15<03:23,  4.33s/it] 69%|██████▉   | 104/150 [09:19<03:17,  4.30s/it] 70%|███████   | 105/150 [09:23<03:13,  4.29s/it]                                                  70%|███████   | 105/150 [09:23<03:13,  4.29s/it]int64
{'eval_loss': 0.004502883646637201, 'eval_rouge1': 65.9127, 'eval_rouge2': 57.2857, 'eval_rougeL': 66.0317, 'eval_rougeLsum': 65.9127, 'eval_runtime': 5.1627, 'eval_samples_per_second': 7.748, 'eval_steps_per_second': 0.387, 'epoch': 6.0}
{'loss': 0.0285, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                 
                                             [A 70%|███████   | 105/150 [09:28<03:13,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 106/150 [09:46<07:09,  9.77s/it] 71%|███████▏  | 107/150 [09:50<05:48,  8.10s/it] 72%|███████▏  | 108/150 [09:54<04:51,  6.95s/it] 73%|███████▎  | 109/150 [09:58<04:11,  6.13s/it] 73%|███████▎  | 110/150 [10:02<03:41,  5.55s/it] 74%|███████▍  | 111/150 [10:07<03:20,  5.15s/it] 75%|███████▍  | 112/150 [10:11<03:05,  4.89s/it] 75%|███████▌  | 113/150 [10:15<02:53,  4.70s/it] 76%|███████▌  | 114/150 [10:19<02:43,  4.55s/it] 77%|███████▋  | 115/150 [10:24<02:37,  4.49s/it] 77%|███████▋  | 116/150 [10:28<02:29,  4.41s/it] 78%|███████▊  | 117/150 [10:32<02:23,  4.36s/it] 79%|███████▊  | 118/150 [10:36<02:17,  4.29s/it] 79%|███████▉  | 119/150 [10:41<02:12,  4.27s/it] 80%|████████  | 120/150 [10:45<02:07,  4.26s/it]                                                  80%|████████  | 120/150 [10:45<02:07,  4.26s/it]int64
{'eval_loss': 0.0040435646660625935, 'eval_rouge1': 65.9127, 'eval_rouge2': 57.2857, 'eval_rougeL': 66.0317, 'eval_rougeLsum': 65.9127, 'eval_runtime': 5.1495, 'eval_samples_per_second': 7.768, 'eval_steps_per_second': 0.388, 'epoch': 7.0}
{'loss': 0.0241, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.37s/it][A                                                 
                                             [A 80%|████████  | 120/150 [10:50<02:07,  4.26s/it]
100%|██████████| 2/2 [00:02<00:00,  1.37s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 121/150 [11:08<04:44,  9.81s/it] 81%|████████▏ | 122/150 [11:12<03:48,  8.15s/it] 82%|████████▏ | 123/150 [11:16<03:07,  6.95s/it] 83%|████████▎ | 124/150 [11:20<02:39,  6.12s/it] 83%|████████▎ | 125/150 [11:24<02:18,  5.55s/it] 84%|████████▍ | 126/150 [11:29<02:03,  5.15s/it] 85%|████████▍ | 127/150 [11:33<01:51,  4.85s/it] 85%|████████▌ | 128/150 [11:37<01:42,  4.66s/it] 86%|████████▌ | 129/150 [11:41<01:35,  4.53s/it] 87%|████████▋ | 130/150 [11:45<01:28,  4.44s/it] 87%|████████▋ | 131/150 [11:50<01:24,  4.43s/it] 88%|████████▊ | 132/150 [11:54<01:18,  4.35s/it] 89%|████████▊ | 133/150 [11:58<01:13,  4.31s/it] 89%|████████▉ | 134/150 [12:03<01:08,  4.29s/it] 90%|█████████ | 135/150 [12:07<01:04,  4.27s/it]                                                  90%|█████████ | 135/150 [12:07<01:04,  4.27s/it]int64
{'eval_loss': 0.003246911335736513, 'eval_rouge1': 65.9127, 'eval_rouge2': 57.2857, 'eval_rougeL': 66.0317, 'eval_rougeLsum': 65.9127, 'eval_runtime': 5.3207, 'eval_samples_per_second': 7.518, 'eval_steps_per_second': 0.376, 'epoch': 8.0}
{'loss': 0.0226, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A                                                 
                                             [A 90%|█████████ | 135/150 [12:12<01:04,  4.27s/it]
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 136/150 [12:28<02:10,  9.30s/it] 91%|█████████▏| 137/150 [12:32<01:41,  7.79s/it] 92%|█████████▏| 138/150 [12:36<01:20,  6.72s/it] 93%|█████████▎| 139/150 [12:41<01:05,  5.98s/it] 93%|█████████▎| 140/150 [12:45<00:55,  5.51s/it] 94%|█████████▍| 141/150 [12:49<00:46,  5.15s/it] 95%|█████████▍| 142/150 [12:53<00:38,  4.87s/it] 95%|█████████▌| 143/150 [12:58<00:32,  4.68s/it] 96%|█████████▌| 144/150 [13:02<00:27,  4.54s/it] 97%|█████████▋| 145/150 [13:06<00:22,  4.46s/it] 97%|█████████▋| 146/150 [13:10<00:17,  4.38s/it] 98%|█████████▊| 147/150 [13:15<00:13,  4.34s/it] 99%|█████████▊| 148/150 [13:19<00:08,  4.33s/it] 99%|█████████▉| 149/150 [13:23<00:04,  4.30s/it]100%|██████████| 150/150 [13:27<00:00,  4.29s/it]                                                 100%|██████████| 150/150 [13:27<00:00,  4.29s/it]int64
{'eval_loss': 0.0037566865794360638, 'eval_rouge1': 65.9127, 'eval_rouge2': 57.2857, 'eval_rougeL': 66.0317, 'eval_rougeLsum': 65.9127, 'eval_runtime': 5.1701, 'eval_samples_per_second': 7.737, 'eval_steps_per_second': 0.387, 'epoch': 9.0}
{'loss': 0.0251, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.28s/it][A                                                 
                                             [A100%|██████████| 150/150 [13:33<00:00,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.28s/it][A
                                             [A                                                 100%|██████████| 150/150 [13:50<00:00,  4.29s/it]100%|██████████| 150/150 [13:51<00:00,  5.54s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.003928284626454115, 'eval_rouge1': 65.9127, 'eval_rouge2': 57.2857, 'eval_rougeL': 66.0317, 'eval_rougeLsum': 65.9127, 'eval_runtime': 5.2371, 'eval_samples_per_second': 7.638, 'eval_steps_per_second': 0.382, 'epoch': 10.0}
{'train_runtime': 838.4772, 'train_samples_per_second': 4.293, 'train_steps_per_second': 0.179, 'train_loss': 1.8072494488954545, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▃▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▁▆▇▇█████
wandb:                    eval/rouge2 ▁▁▆▆▇█████
wandb:                    eval/rougeL ▁▁▆▇▇█████
wandb:                 eval/rougeLsum ▁▁▆▇▇█████
wandb:                   eval/runtime ▁▃▃▃▅▅▄█▅▆
wandb:        eval/samples_per_second █▆▆▆▄▄▅▁▄▃
wandb:          eval/steps_per_second █▆▆▆▄▄▅▁▄▃
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▃▂▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.00393
wandb:                    eval/rouge1 65.9127
wandb:                    eval/rouge2 57.2857
wandb:                    eval/rougeL 66.0317
wandb:                 eval/rougeLsum 65.9127
wandb:                   eval/runtime 5.2371
wandb:        eval/samples_per_second 7.638
wandb:          eval/steps_per_second 0.382
wandb:                    train/epoch 10.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0251
wandb:               train/total_flos 275491794124800.0
wandb:               train/train_loss 1.80725
wandb:            train/train_runtime 838.4772
wandb: train/train_samples_per_second 4.293
wandb:   train/train_steps_per_second 0.179
wandb: 
wandb: 🚀 View run flan-t5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/ri1pui8r
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_090955-ri1pui8r/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_092507-jw84fllt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/jw84fllt
################################################ t5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:15<37:33, 15.12s/it]  1%|▏         | 2/150 [00:19<21:17,  8.63s/it]  2%|▏         | 3/150 [00:23<16:00,  6.53s/it]  3%|▎         | 4/150 [00:27<13:27,  5.53s/it]  3%|▎         | 5/150 [00:31<12:04,  5.00s/it]  4%|▍         | 6/150 [00:35<11:12,  4.67s/it]  5%|▍         | 7/150 [00:39<10:37,  4.46s/it]  5%|▌         | 8/150 [00:43<10:13,  4.32s/it]  6%|▌         | 9/150 [00:47<09:54,  4.21s/it]  7%|▋         | 10/150 [00:51<09:43,  4.16s/it]  7%|▋         | 11/150 [00:55<09:32,  4.12s/it]  8%|▊         | 12/150 [00:59<09:23,  4.09s/it]  9%|▊         | 13/150 [01:03<09:16,  4.06s/it]  9%|▉         | 14/150 [01:07<09:20,  4.12s/it] 10%|█         | 15/150 [01:11<09:14,  4.11s/it]                                                 10%|█         | 15/150 [01:11<09:14,  4.11s/it]{'loss': 2.8998, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A                                                
                                             [A 10%|█         | 15/150 [01:16<09:14,  4.11s/it]
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [01:32<20:01,  8.97s/it] 11%|█▏        | 17/150 [01:36<16:43,  7.55s/it] 12%|█▏        | 18/150 [01:40<14:15,  6.48s/it] 13%|█▎        | 19/150 [01:44<12:32,  5.74s/it] 13%|█▎        | 20/150 [01:48<11:18,  5.22s/it] 14%|█▍        | 21/150 [01:52<10:27,  4.86s/it] 15%|█▍        | 22/150 [01:56<09:50,  4.61s/it] 15%|█▌        | 23/150 [02:00<09:20,  4.42s/it] 16%|█▌        | 24/150 [02:04<09:01,  4.30s/it] 17%|█▋        | 25/150 [02:08<08:46,  4.21s/it] 17%|█▋        | 26/150 [02:12<08:37,  4.17s/it] 18%|█▊        | 27/150 [02:16<08:28,  4.13s/it] 19%|█▊        | 28/150 [02:20<08:18,  4.08s/it] 19%|█▉        | 29/150 [02:24<08:10,  4.06s/it] 20%|██        | 30/150 [02:28<08:05,  4.05s/it]                                                 20%|██        | 30/150 [02:28<08:05,  4.05s/it]int64
{'eval_loss': 0.2524056136608124, 'eval_rouge1': 31.6667, 'eval_rouge2': 24.0238, 'eval_rougeL': 30.6944, 'eval_rougeLsum': 31.2302, 'eval_runtime': 4.8712, 'eval_samples_per_second': 8.211, 'eval_steps_per_second': 0.411, 'epoch': 1.0}
{'loss': 0.2318, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.10s/it][A                                                
                                             [A 20%|██        | 30/150 [02:33<08:05,  4.05s/it]
100%|██████████| 2/2 [00:02<00:00,  1.10s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [02:49<18:17,  9.22s/it] 21%|██▏       | 32/150 [02:53<15:01,  7.64s/it] 22%|██▏       | 33/150 [02:57<12:45,  6.55s/it] 23%|██▎       | 34/150 [03:01<11:10,  5.78s/it] 23%|██▎       | 35/150 [03:05<10:04,  5.26s/it] 24%|██▍       | 36/150 [03:09<09:17,  4.89s/it] 25%|██▍       | 37/150 [03:13<08:43,  4.64s/it] 25%|██▌       | 38/150 [03:17<08:18,  4.45s/it] 26%|██▌       | 39/150 [03:21<07:59,  4.32s/it] 27%|██▋       | 40/150 [03:25<07:45,  4.23s/it] 27%|██▋       | 41/150 [03:29<07:35,  4.18s/it] 28%|██▊       | 42/150 [03:33<07:24,  4.12s/it] 29%|██▊       | 43/150 [03:37<07:18,  4.09s/it] 29%|██▉       | 44/150 [03:41<07:11,  4.07s/it] 30%|███       | 45/150 [03:45<07:06,  4.06s/it]                                                 30%|███       | 45/150 [03:45<07:06,  4.06s/it]int64
{'eval_loss': 0.03518320247530937, 'eval_rouge1': 64.9371, 'eval_rouge2': 54.9827, 'eval_rougeL': 64.9516, 'eval_rougeLsum': 64.9714, 'eval_runtime': 4.6604, 'eval_samples_per_second': 8.583, 'eval_steps_per_second': 0.429, 'epoch': 2.0}
{'loss': 0.059, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 30%|███       | 45/150 [03:50<07:06,  4.06s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [04:07<15:51,  9.15s/it] 31%|███▏      | 47/150 [04:11<13:04,  7.62s/it] 32%|███▏      | 48/150 [04:15<11:09,  6.56s/it] 33%|███▎      | 49/150 [04:19<09:45,  5.80s/it] 33%|███▎      | 50/150 [04:23<08:47,  5.28s/it] 34%|███▍      | 51/150 [04:27<08:09,  4.95s/it] 35%|███▍      | 52/150 [04:31<07:38,  4.68s/it] 35%|███▌      | 53/150 [04:35<07:15,  4.49s/it] 36%|███▌      | 54/150 [04:39<06:57,  4.34s/it] 37%|███▋      | 55/150 [04:43<06:43,  4.25s/it] 37%|███▋      | 56/150 [04:47<06:33,  4.19s/it] 38%|███▊      | 57/150 [04:51<06:24,  4.13s/it] 39%|███▊      | 58/150 [04:55<06:17,  4.10s/it] 39%|███▉      | 59/150 [04:59<06:11,  4.08s/it] 40%|████      | 60/150 [05:03<06:11,  4.12s/it]                                                 40%|████      | 60/150 [05:03<06:11,  4.12s/it]int64
{'eval_loss': 0.01691245660185814, 'eval_rouge1': 64.6131, 'eval_rouge2': 55.0357, 'eval_rougeL': 64.6726, 'eval_rougeLsum': 64.5734, 'eval_runtime': 4.765, 'eval_samples_per_second': 8.395, 'eval_steps_per_second': 0.42, 'epoch': 3.0}
{'loss': 0.0332, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A                                                
                                             [A 40%|████      | 60/150 [05:08<06:11,  4.12s/it]
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [05:26<14:12,  9.58s/it] 41%|████▏     | 62/150 [05:30<11:36,  7.91s/it] 42%|████▏     | 63/150 [05:34<09:47,  6.75s/it] 43%|████▎     | 64/150 [05:38<08:31,  5.94s/it] 43%|████▎     | 65/150 [05:42<07:36,  5.37s/it] 44%|████▍     | 66/150 [05:46<06:56,  4.96s/it] 45%|████▍     | 67/150 [05:50<06:29,  4.70s/it] 45%|████▌     | 68/150 [05:54<06:07,  4.49s/it] 46%|████▌     | 69/150 [05:58<05:52,  4.35s/it] 47%|████▋     | 70/150 [06:02<05:41,  4.27s/it] 47%|████▋     | 71/150 [06:06<05:32,  4.21s/it] 48%|████▊     | 72/150 [06:10<05:23,  4.15s/it] 49%|████▊     | 73/150 [06:14<05:16,  4.11s/it] 49%|████▉     | 74/150 [06:18<05:10,  4.09s/it] 50%|█████     | 75/150 [06:22<05:04,  4.06s/it]                                                 50%|█████     | 75/150 [06:22<05:04,  4.06s/it]int64
{'eval_loss': 0.004678033292293549, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.6429, 'eval_rougeL': 66.6071, 'eval_rougeLsum': 66.6071, 'eval_runtime': 4.795, 'eval_samples_per_second': 8.342, 'eval_steps_per_second': 0.417, 'epoch': 4.0}
{'loss': 0.0235, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 50%|█████     | 75/150 [06:27<05:04,  4.06s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [06:42<10:50,  8.79s/it] 51%|█████▏    | 77/150 [06:46<08:56,  7.35s/it] 52%|█████▏    | 78/150 [06:50<07:38,  6.37s/it] 53%|█████▎    | 79/150 [06:54<06:44,  5.70s/it] 53%|█████▎    | 80/150 [06:58<06:03,  5.19s/it] 54%|█████▍    | 81/150 [07:02<05:33,  4.84s/it] 55%|█████▍    | 82/150 [07:06<05:13,  4.61s/it] 55%|█████▌    | 83/150 [07:10<04:57,  4.44s/it] 56%|█████▌    | 84/150 [07:14<04:44,  4.31s/it] 57%|█████▋    | 85/150 [07:18<04:34,  4.22s/it] 57%|█████▋    | 86/150 [07:22<04:26,  4.16s/it] 58%|█████▊    | 87/150 [07:26<04:18,  4.10s/it] 59%|█████▊    | 88/150 [07:30<04:12,  4.07s/it] 59%|█████▉    | 89/150 [07:34<04:06,  4.03s/it] 60%|██████    | 90/150 [07:38<04:02,  4.04s/it]                                                 60%|██████    | 90/150 [07:38<04:02,  4.04s/it]int64
{'eval_loss': 0.011149299331009388, 'eval_rouge1': 65.3373, 'eval_rouge2': 55.5952, 'eval_rougeL': 65.248, 'eval_rougeLsum': 65.3373, 'eval_runtime': 4.7712, 'eval_samples_per_second': 8.384, 'eval_steps_per_second': 0.419, 'epoch': 5.0}
{'loss': 0.0181, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A                                                
                                             [A 60%|██████    | 90/150 [07:43<04:02,  4.04s/it]
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [07:59<09:00,  9.16s/it] 61%|██████▏   | 92/150 [08:04<07:22,  7.63s/it] 62%|██████▏   | 93/150 [08:08<06:13,  6.55s/it] 63%|██████▎   | 94/150 [08:12<05:23,  5.78s/it] 63%|██████▎   | 95/150 [08:16<04:49,  5.26s/it] 64%|██████▍   | 96/150 [08:20<04:23,  4.89s/it] 65%|██████▍   | 97/150 [08:24<04:05,  4.62s/it] 65%|██████▌   | 98/150 [08:28<03:51,  4.45s/it] 66%|██████▌   | 99/150 [08:32<03:40,  4.32s/it] 67%|██████▋   | 100/150 [08:36<03:32,  4.25s/it] 67%|██████▋   | 101/150 [08:40<03:25,  4.19s/it] 68%|██████▊   | 102/150 [08:44<03:19,  4.15s/it] 69%|██████▊   | 103/150 [08:48<03:16,  4.17s/it] 69%|██████▉   | 104/150 [08:52<03:09,  4.12s/it] 70%|███████   | 105/150 [08:56<03:04,  4.09s/it]                                                  70%|███████   | 105/150 [08:56<03:04,  4.09s/it]int64
{'eval_loss': 0.01226536463946104, 'eval_rouge1': 65.0893, 'eval_rouge2': 55.6548, 'eval_rougeL': 65.0992, 'eval_rougeLsum': 65.1488, 'eval_runtime': 4.7702, 'eval_samples_per_second': 8.385, 'eval_steps_per_second': 0.419, 'epoch': 6.0}
{'loss': 0.0138, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.19s/it][A                                                 
                                             [A 70%|███████   | 105/150 [09:01<03:04,  4.09s/it]
100%|██████████| 2/2 [00:02<00:00,  1.19s/it][A
                                             [A                                                  70%|███████   | 105/150 [09:17<03:04,  4.09s/it] 70%|███████   | 105/150 [09:18<03:59,  5.32s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.008760958909988403, 'eval_rouge1': 65.8929, 'eval_rouge2': 56.2857, 'eval_rougeL': 65.8333, 'eval_rougeLsum': 65.8333, 'eval_runtime': 4.7947, 'eval_samples_per_second': 8.343, 'eval_steps_per_second': 0.417, 'epoch': 7.0}
{'train_runtime': 565.7408, 'train_samples_per_second': 6.363, 'train_steps_per_second': 0.265, 'train_loss': 0.46846748164721896, 'epoch': 7.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁▁▁
wandb:                    eval/rouge1 ▁██████
wandb:                    eval/rouge2 ▁▇▇████
wandb:                    eval/rougeL ▁██████
wandb:                 eval/rougeLsum ▁██████
wandb:                   eval/runtime █▁▄▅▅▅▅
wandb:        eval/samples_per_second ▁█▄▃▄▄▃
wandb:          eval/steps_per_second ▁█▄▃▄▄▃
wandb:                    train/epoch ▁▁▂▂▃▃▅▅▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▅▅▆▆▇▇███
wandb:            train/learning_rate █▇▆▄▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.00876
wandb:                    eval/rouge1 65.8929
wandb:                    eval/rouge2 56.2857
wandb:                    eval/rougeL 65.8333
wandb:                 eval/rougeLsum 65.8333
wandb:                   eval/runtime 4.7947
wandb:        eval/samples_per_second 8.343
wandb:          eval/steps_per_second 0.417
wandb:                    train/epoch 7.0
wandb:              train/global_step 105
wandb:            train/learning_rate 3e-05
wandb:                     train/loss 0.0138
wandb:               train/total_flos 181153566720000.0
wandb:               train/train_loss 0.46847
wandb:            train/train_runtime 565.7408
wandb: train/train_samples_per_second 6.363
wandb:   train/train_steps_per_second 0.265
wandb: 
wandb: 🚀 View run t5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/jw84fllt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_092507-jw84fllt/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_093537-bfp50tds
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mt5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/bfp50tds
################################################ mt5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:14<34:59, 14.09s/it]  1%|▏         | 2/150 [00:17<18:47,  7.62s/it]  2%|▏         | 3/150 [00:20<13:37,  5.56s/it]  3%|▎         | 4/150 [00:23<11:09,  4.59s/it]  3%|▎         | 5/150 [00:26<09:49,  4.06s/it]  4%|▍         | 6/150 [00:29<08:59,  3.74s/it]  5%|▍         | 7/150 [00:32<08:26,  3.54s/it]  5%|▌         | 8/150 [00:35<08:03,  3.40s/it]  6%|▌         | 9/150 [00:38<07:46,  3.31s/it]  7%|▋         | 10/150 [00:42<07:35,  3.25s/it]  7%|▋         | 11/150 [00:45<07:24,  3.19s/it]  8%|▊         | 12/150 [00:48<07:17,  3.17s/it]  9%|▊         | 13/150 [00:51<07:11,  3.15s/it]  9%|▉         | 14/150 [00:54<07:06,  3.14s/it] 10%|█         | 15/150 [00:57<07:02,  3.13s/it]                                                 10%|█         | 15/150 [00:57<07:02,  3.13s/it]{'loss': 29.0805, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A 10%|█         | 15/150 [01:01<07:02,  3.13s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [01:15<16:50,  7.54s/it] 11%|█▏        | 17/150 [01:18<13:45,  6.21s/it] 12%|█▏        | 18/150 [01:21<11:36,  5.27s/it] 13%|█▎        | 19/150 [01:24<10:04,  4.62s/it] 13%|█▎        | 20/150 [01:27<08:59,  4.15s/it] 14%|█▍        | 21/150 [01:30<08:13,  3.82s/it] 15%|█▍        | 22/150 [01:33<07:42,  3.61s/it] 15%|█▌        | 23/150 [01:37<07:19,  3.46s/it] 16%|█▌        | 24/150 [01:40<07:02,  3.35s/it] 17%|█▋        | 25/150 [01:43<06:49,  3.28s/it] 17%|█▋        | 26/150 [01:46<06:39,  3.22s/it] 18%|█▊        | 27/150 [01:49<06:31,  3.19s/it] 19%|█▊        | 28/150 [01:52<06:25,  3.16s/it] 19%|█▉        | 29/150 [01:55<06:19,  3.14s/it] 20%|██        | 30/150 [01:58<06:14,  3.12s/it]                                                 20%|██        | 30/150 [01:58<06:14,  3.12s/it]int64
{'eval_loss': 17.733203887939453, 'eval_rouge1': 12.2171, 'eval_rouge2': 3.9089, 'eval_rougeL': 12.1546, 'eval_rougeLsum': 12.0516, 'eval_runtime': 3.6596, 'eval_samples_per_second': 10.93, 'eval_steps_per_second': 0.547, 'epoch': 1.0}
{'loss': 25.5868, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 20%|██        | 30/150 [02:02<06:14,  3.12s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [02:17<15:24,  7.77s/it] 21%|██▏       | 32/150 [02:20<12:30,  6.36s/it] 22%|██▏       | 33/150 [02:23<10:30,  5.39s/it] 23%|██▎       | 34/150 [02:26<09:04,  4.70s/it] 23%|██▎       | 35/150 [02:29<08:05,  4.22s/it] 24%|██▍       | 36/150 [02:32<07:23,  3.89s/it] 25%|██▍       | 37/150 [02:35<06:52,  3.65s/it] 25%|██▌       | 38/150 [02:39<06:30,  3.49s/it] 26%|██▌       | 39/150 [02:42<06:13,  3.37s/it] 27%|██▋       | 40/150 [02:45<06:00,  3.28s/it] 27%|██▋       | 41/150 [02:48<05:51,  3.23s/it] 28%|██▊       | 42/150 [02:51<05:43,  3.18s/it] 29%|██▊       | 43/150 [02:54<05:37,  3.16s/it] 29%|██▉       | 44/150 [02:57<05:32,  3.14s/it] 30%|███       | 45/150 [03:00<05:28,  3.13s/it]                                                 30%|███       | 45/150 [03:00<05:28,  3.13s/it]int64
{'eval_loss': 16.153690338134766, 'eval_rouge1': 14.1665, 'eval_rouge2': 4.0282, 'eval_rougeL': 13.8176, 'eval_rougeLsum': 13.9501, 'eval_runtime': 3.647, 'eval_samples_per_second': 10.968, 'eval_steps_per_second': 0.548, 'epoch': 2.0}
{'loss': 22.7173, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                
                                             [A 30%|███       | 45/150 [03:04<05:28,  3.13s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [03:19<13:31,  7.80s/it] 31%|███▏      | 47/150 [03:22<10:57,  6.39s/it] 32%|███▏      | 48/150 [03:25<09:10,  5.39s/it] 33%|███▎      | 49/150 [03:28<08:01,  4.76s/it] 33%|███▎      | 50/150 [03:31<07:05,  4.26s/it] 34%|███▍      | 51/150 [03:34<06:26,  3.90s/it] 35%|███▍      | 52/150 [03:38<05:59,  3.67s/it] 35%|███▌      | 53/150 [03:41<05:39,  3.50s/it] 36%|███▌      | 54/150 [03:44<05:24,  3.38s/it] 37%|███▋      | 55/150 [03:47<05:13,  3.30s/it] 37%|███▋      | 56/150 [03:50<05:04,  3.24s/it] 38%|███▊      | 57/150 [03:53<04:56,  3.19s/it] 39%|███▊      | 58/150 [03:56<04:56,  3.22s/it] 39%|███▉      | 59/150 [04:00<04:50,  3.19s/it] 40%|████      | 60/150 [04:03<04:44,  3.16s/it]                                                 40%|████      | 60/150 [04:03<04:44,  3.16s/it]int64
{'eval_loss': 14.575057983398438, 'eval_rouge1': 15.9917, 'eval_rouge2': 3.141, 'eval_rougeL': 15.8734, 'eval_rougeLsum': 16.1058, 'eval_runtime': 3.6507, 'eval_samples_per_second': 10.957, 'eval_steps_per_second': 0.548, 'epoch': 3.0}
{'loss': 20.2191, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 40%|████      | 60/150 [04:06<04:44,  3.16s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [04:21<11:28,  7.73s/it] 41%|████▏     | 62/150 [04:24<09:17,  6.33s/it] 42%|████▏     | 63/150 [04:27<07:46,  5.37s/it] 43%|████▎     | 64/150 [04:30<06:43,  4.69s/it] 43%|████▎     | 65/150 [04:33<05:58,  4.22s/it] 44%|████▍     | 66/150 [04:36<05:25,  3.87s/it] 45%|████▍     | 67/150 [04:40<05:02,  3.65s/it] 45%|████▌     | 68/150 [04:43<04:45,  3.48s/it] 46%|████▌     | 69/150 [04:46<04:32,  3.37s/it] 47%|████▋     | 70/150 [04:49<04:27,  3.35s/it] 47%|████▋     | 71/150 [04:52<04:18,  3.27s/it] 48%|████▊     | 72/150 [04:55<04:10,  3.21s/it] 49%|████▊     | 73/150 [04:58<04:05,  3.18s/it] 49%|████▉     | 74/150 [05:01<03:59,  3.16s/it] 50%|█████     | 75/150 [05:05<03:54,  3.13s/it]                                                 50%|█████     | 75/150 [05:05<03:54,  3.13s/it]int64
{'eval_loss': 14.4717435836792, 'eval_rouge1': 4.6592, 'eval_rouge2': 0.8704, 'eval_rougeL': 4.6359, 'eval_rougeLsum': 4.5695, 'eval_runtime': 3.6555, 'eval_samples_per_second': 10.942, 'eval_steps_per_second': 0.547, 'epoch': 4.0}
{'loss': 19.5194, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 50%|█████     | 75/150 [05:08<03:54,  3.13s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [05:22<09:12,  7.47s/it] 51%|█████▏    | 77/150 [05:25<07:29,  6.16s/it] 52%|█████▏    | 78/150 [05:28<06:17,  5.24s/it] 53%|█████▎    | 79/150 [05:31<05:26,  4.60s/it] 53%|█████▎    | 80/150 [05:35<04:50,  4.15s/it] 54%|█████▍    | 81/150 [05:38<04:24,  3.83s/it] 55%|█████▍    | 82/150 [05:41<04:09,  3.67s/it] 55%|█████▌    | 83/150 [05:44<03:54,  3.50s/it] 56%|█████▌    | 84/150 [05:47<03:42,  3.37s/it] 57%|█████▋    | 85/150 [05:50<03:34,  3.29s/it] 57%|█████▋    | 86/150 [05:53<03:27,  3.24s/it] 58%|█████▊    | 87/150 [05:56<03:21,  3.20s/it] 59%|█████▊    | 88/150 [06:00<03:16,  3.17s/it] 59%|█████▉    | 89/150 [06:03<03:12,  3.16s/it] 60%|██████    | 90/150 [06:06<03:08,  3.14s/it]                                                 60%|██████    | 90/150 [06:06<03:08,  3.14s/it]int64
{'eval_loss': 15.332356452941895, 'eval_rouge1': 1.1111, 'eval_rouge2': 0.0, 'eval_rougeL': 1.1111, 'eval_rougeLsum': 1.1254, 'eval_runtime': 3.6487, 'eval_samples_per_second': 10.963, 'eval_steps_per_second': 0.548, 'epoch': 5.0}
{'loss': 18.1742, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                
                                             [A 60%|██████    | 90/150 [06:09<03:08,  3.14s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [06:26<08:03,  8.19s/it] 61%|██████▏   | 92/150 [06:29<06:26,  6.67s/it] 62%|██████▏   | 93/150 [06:32<05:19,  5.60s/it] 63%|██████▎   | 94/150 [06:35<04:31,  4.85s/it] 63%|██████▎   | 95/150 [06:38<03:57,  4.32s/it] 64%|██████▍   | 96/150 [06:41<03:32,  3.94s/it] 65%|██████▍   | 97/150 [06:44<03:15,  3.69s/it] 65%|██████▌   | 98/150 [06:47<03:02,  3.52s/it] 66%|██████▌   | 99/150 [06:51<02:52,  3.39s/it] 67%|██████▋   | 100/150 [06:54<02:45,  3.30s/it] 67%|██████▋   | 101/150 [06:57<02:39,  3.24s/it] 68%|██████▊   | 102/150 [07:00<02:33,  3.20s/it] 69%|██████▊   | 103/150 [07:03<02:28,  3.16s/it] 69%|██████▉   | 104/150 [07:06<02:23,  3.13s/it] 70%|███████   | 105/150 [07:09<02:20,  3.13s/it]                                                  70%|███████   | 105/150 [07:09<02:20,  3.13s/it]int64
{'eval_loss': 14.119397163391113, 'eval_rouge1': 1.0185, 'eval_rouge2': 0.5, 'eval_rougeL': 0.8333, 'eval_rougeLsum': 1.0185, 'eval_runtime': 3.6428, 'eval_samples_per_second': 10.981, 'eval_steps_per_second': 0.549, 'epoch': 6.0}
{'loss': 17.4042, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                 
                                             [A 70%|███████   | 105/150 [07:13<02:20,  3.13s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 106/150 [07:28<05:49,  7.95s/it] 71%|███████▏  | 107/150 [07:31<04:38,  6.49s/it] 72%|███████▏  | 108/150 [07:34<03:49,  5.47s/it] 73%|███████▎  | 109/150 [07:38<03:15,  4.77s/it] 73%|███████▎  | 110/150 [07:41<02:50,  4.27s/it] 74%|███████▍  | 111/150 [07:44<02:32,  3.92s/it] 75%|███████▍  | 112/150 [07:47<02:19,  3.66s/it] 75%|███████▌  | 113/150 [07:50<02:08,  3.48s/it] 76%|███████▌  | 114/150 [07:53<02:01,  3.37s/it] 77%|███████▋  | 115/150 [07:56<01:55,  3.29s/it] 77%|███████▋  | 116/150 [07:59<01:49,  3.22s/it] 78%|███████▊  | 117/150 [08:02<01:44,  3.18s/it] 79%|███████▊  | 118/150 [08:05<01:40,  3.15s/it] 79%|███████▉  | 119/150 [08:08<01:37,  3.14s/it] 80%|████████  | 120/150 [08:12<01:35,  3.18s/it]                                                  80%|████████  | 120/150 [08:12<01:35,  3.18s/it]int64
{'eval_loss': 12.446837425231934, 'eval_rouge1': 2.2573, 'eval_rouge2': 0.4545, 'eval_rougeL': 2.255, 'eval_rougeLsum': 2.2766, 'eval_runtime': 3.6555, 'eval_samples_per_second': 10.943, 'eval_steps_per_second': 0.547, 'epoch': 7.0}
{'loss': 16.4573, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                 
                                             [A 80%|████████  | 120/150 [08:15<01:35,  3.18s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 121/150 [08:30<03:47,  7.86s/it] 81%|████████▏ | 122/150 [08:34<02:59,  6.43s/it] 82%|████████▏ | 123/150 [08:37<02:27,  5.48s/it] 83%|████████▎ | 124/150 [08:40<02:03,  4.76s/it] 83%|████████▎ | 125/150 [08:43<01:46,  4.26s/it] 84%|████████▍ | 126/150 [08:46<01:34,  3.92s/it] 85%|████████▍ | 127/150 [08:49<01:24,  3.68s/it] 85%|████████▌ | 128/150 [08:52<01:16,  3.49s/it] 86%|████████▌ | 129/150 [08:55<01:10,  3.38s/it] 87%|████████▋ | 130/150 [08:58<01:05,  3.28s/it] 87%|████████▋ | 131/150 [09:02<01:01,  3.23s/it] 88%|████████▊ | 132/150 [09:05<00:58,  3.27s/it] 89%|████████▊ | 133/150 [09:08<00:54,  3.23s/it] 89%|████████▉ | 134/150 [09:11<00:51,  3.19s/it] 90%|█████████ | 135/150 [09:14<00:47,  3.16s/it]                                                  90%|█████████ | 135/150 [09:14<00:47,  3.16s/it]int64
{'eval_loss': 11.793710708618164, 'eval_rouge1': 2.0073, 'eval_rouge2': 0.4545, 'eval_rougeL': 2.0385, 'eval_rougeLsum': 2.0192, 'eval_runtime': 3.6404, 'eval_samples_per_second': 10.988, 'eval_steps_per_second': 0.549, 'epoch': 8.0}
{'loss': 15.7737, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                 
                                             [A 90%|█████████ | 135/150 [09:18<00:47,  3.16s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 136/150 [09:33<01:50,  7.91s/it] 91%|█████████▏| 137/150 [09:36<01:24,  6.47s/it] 92%|█████████▏| 138/150 [09:39<01:05,  5.46s/it] 93%|█████████▎| 139/150 [09:43<00:52,  4.75s/it] 93%|█████████▎| 140/150 [09:46<00:42,  4.24s/it] 94%|█████████▍| 141/150 [09:49<00:35,  3.90s/it] 95%|█████████▍| 142/150 [09:52<00:29,  3.66s/it] 95%|█████████▌| 143/150 [09:55<00:24,  3.50s/it] 96%|█████████▌| 144/150 [09:58<00:20,  3.37s/it] 97%|█████████▋| 145/150 [10:01<00:16,  3.29s/it] 97%|█████████▋| 146/150 [10:04<00:12,  3.22s/it] 98%|█████████▊| 147/150 [10:07<00:09,  3.19s/it] 99%|█████████▊| 148/150 [10:10<00:06,  3.17s/it] 99%|█████████▉| 149/150 [10:14<00:03,  3.15s/it]100%|██████████| 150/150 [10:17<00:00,  3.14s/it]                                                 100%|██████████| 150/150 [10:17<00:00,  3.14s/it]int64
{'eval_loss': 11.423368453979492, 'eval_rouge1': 2.8413, 'eval_rouge2': 0.4167, 'eval_rougeL': 2.9623, 'eval_rougeLsum': 2.9411, 'eval_runtime': 3.653, 'eval_samples_per_second': 10.95, 'eval_steps_per_second': 0.547, 'epoch': 9.0}
{'loss': 15.3396, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                 
                                             [A100%|██████████| 150/150 [10:20<00:00,  3.14s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A                                                 100%|██████████| 150/150 [10:32<00:00,  3.14s/it]100%|██████████| 150/150 [10:32<00:00,  4.22s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 11.322454452514648, 'eval_rouge1': 2.8915, 'eval_rouge2': 0.4167, 'eval_rougeL': 3.0073, 'eval_rougeLsum': 2.9709, 'eval_runtime': 3.6531, 'eval_samples_per_second': 10.95, 'eval_steps_per_second': 0.547, 'epoch': 10.0}
{'train_runtime': 641.3715, 'train_samples_per_second': 5.613, 'train_steps_per_second': 0.234, 'train_loss': 20.027222696940104, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▆▅▄▅▄▂▂▁▁
wandb:                    eval/rouge1 ▆▇█▃▁▁▂▁▂▂
wandb:                    eval/rouge2 ██▆▃▁▂▂▂▂▂
wandb:                    eval/rougeL ▆▇█▃▁▁▂▂▂▂
wandb:                 eval/rougeLsum ▆▇█▃▁▁▂▁▂▂
wandb:                   eval/runtime █▃▅▇▄▂▇▁▆▆
wandb:        eval/samples_per_second ▁▆▄▂▅▇▃█▃▃
wandb:          eval/steps_per_second ▁▄▄▁▄█▁█▁▁
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▆▅▃▃▂▂▂▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 11.32245
wandb:                    eval/rouge1 2.8915
wandb:                    eval/rouge2 0.4167
wandb:                    eval/rougeL 3.0073
wandb:                 eval/rougeLsum 2.9709
wandb:                   eval/runtime 3.6531
wandb:        eval/samples_per_second 10.95
wandb:          eval/steps_per_second 0.547
wandb:                    train/epoch 10.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 15.3396
wandb:               train/total_flos 101169517363200.0
wandb:               train/train_loss 20.02722
wandb:            train/train_runtime 641.3715
wandb: train/train_samples_per_second 5.613
wandb:   train/train_steps_per_second 0.234
wandb: 
wandb: 🚀 View run mt5-base_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/bfp50tds
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_093537-bfp50tds/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_094729-82nhb7ah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mt5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/82nhb7ah
################################################ mt5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:17<43:51, 17.66s/it]Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 252, in main
    my_instance.train()
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 160, in train
    trainer.train()
  File "/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/trainer.py", line 1809, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/trainer.py", line 2665, in training_step
    self.accelerator.backward(loss)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/accelerate/accelerator.py", line 1821, in backward
    loss.backward(**kwargs)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 143, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 95, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 23.70 GiB total capacity; 22.34 GiB already allocated; 7.69 MiB free; 22.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 🚀 View run mt5-large_ep-10_ga-1_b-4_lr-0.0001_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/82nhb7ah
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_094729-82nhb7ah/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_094832-x28ifkb5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/x28ifkb5
################################################ flan-t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:12<30:52, 12.43s/it]  1%|▏         | 2/150 [00:13<14:38,  5.94s/it]  2%|▏         | 3/150 [00:15<09:28,  3.86s/it]  3%|▎         | 4/150 [00:16<07:01,  2.88s/it]  3%|▎         | 5/150 [00:18<05:41,  2.36s/it]  4%|▍         | 6/150 [00:19<04:51,  2.02s/it]  5%|▍         | 7/150 [00:20<04:20,  1.82s/it]  5%|▌         | 8/150 [00:22<03:58,  1.68s/it]  6%|▌         | 9/150 [00:23<03:42,  1.58s/it]  7%|▋         | 10/150 [00:24<03:32,  1.52s/it]  7%|▋         | 11/150 [00:26<03:25,  1.48s/it]  8%|▊         | 12/150 [00:27<03:19,  1.45s/it]  9%|▊         | 13/150 [00:29<03:15,  1.43s/it]  9%|▉         | 14/150 [00:30<03:11,  1.41s/it] 10%|█         | 15/150 [00:31<03:08,  1.40s/it]                                                 10%|█         | 15/150 [00:31<03:08,  1.40s/it]{'loss': 7.484, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.24it/s][A                                                
                                             [A 10%|█         | 15/150 [00:33<03:08,  1.40s/it]
100%|██████████| 2/2 [00:01<00:00,  2.24it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [00:39<07:13,  3.23s/it] 11%|█▏        | 17/150 [00:40<05:56,  2.68s/it] 12%|█▏        | 18/150 [00:42<05:02,  2.29s/it] 13%|█▎        | 19/150 [00:43<04:25,  2.02s/it] 13%|█▎        | 20/150 [00:44<03:58,  1.83s/it] 14%|█▍        | 21/150 [00:46<03:38,  1.70s/it] 15%|█▍        | 22/150 [00:47<03:25,  1.60s/it] 15%|█▌        | 23/150 [00:49<03:15,  1.54s/it] 16%|█▌        | 24/150 [00:50<03:08,  1.49s/it] 17%|█▋        | 25/150 [00:51<03:02,  1.46s/it] 17%|█▋        | 26/150 [00:53<02:58,  1.44s/it] 18%|█▊        | 27/150 [00:54<02:54,  1.42s/it] 19%|█▊        | 28/150 [00:55<02:52,  1.42s/it] 19%|█▉        | 29/150 [00:57<02:49,  1.40s/it] 20%|██        | 30/150 [00:58<02:47,  1.40s/it]                                                 20%|██        | 30/150 [00:58<02:47,  1.40s/it]int64
{'eval_loss': 1.488560438156128, 'eval_rouge1': 66.4881, 'eval_rouge2': 56.5, 'eval_rougeL': 66.5041, 'eval_rougeLsum': 66.5842, 'eval_runtime': 1.9349, 'eval_samples_per_second': 20.673, 'eval_steps_per_second': 1.034, 'epoch': 1.0}
{'loss': 1.2256, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 20%|██        | 30/150 [01:00<02:47,  1.40s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [01:06<06:40,  3.36s/it] 21%|██▏       | 32/150 [01:08<05:26,  2.77s/it] 22%|██▏       | 33/150 [01:09<04:35,  2.35s/it] 23%|██▎       | 34/150 [01:10<04:00,  2.07s/it] 23%|██▎       | 35/150 [01:12<03:34,  1.86s/it] 24%|██▍       | 36/150 [01:13<03:15,  1.72s/it] 25%|██▍       | 37/150 [01:14<03:02,  1.62s/it] 25%|██▌       | 38/150 [01:16<02:53,  1.54s/it] 26%|██▌       | 39/150 [01:17<02:46,  1.50s/it] 27%|██▋       | 40/150 [01:19<02:41,  1.47s/it] 27%|██▋       | 41/150 [01:20<02:37,  1.44s/it] 28%|██▊       | 42/150 [01:21<02:33,  1.42s/it] 29%|██▊       | 43/150 [01:23<02:30,  1.41s/it] 29%|██▉       | 44/150 [01:24<02:28,  1.40s/it] 30%|███       | 45/150 [01:26<02:26,  1.39s/it]                                                 30%|███       | 45/150 [01:26<02:26,  1.39s/it]int64
{'eval_loss': 0.14698703587055206, 'eval_rouge1': 66.1508, 'eval_rouge2': 56.881, 'eval_rougeL': 66.0714, 'eval_rougeLsum': 66.1706, 'eval_runtime': 1.9127, 'eval_samples_per_second': 20.913, 'eval_steps_per_second': 1.046, 'epoch': 2.0}
{'loss': 0.1591, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.25it/s][A                                                
                                             [A 30%|███       | 45/150 [01:27<02:26,  1.39s/it]
100%|██████████| 2/2 [00:01<00:00,  2.25it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [01:34<05:55,  3.42s/it] 31%|███▏      | 47/150 [01:35<04:48,  2.80s/it] 32%|███▏      | 48/150 [01:36<04:02,  2.38s/it] 33%|███▎      | 49/150 [01:38<03:33,  2.12s/it] 33%|███▎      | 50/150 [01:39<03:10,  1.90s/it] 34%|███▍      | 51/150 [01:41<02:52,  1.74s/it] 35%|███▍      | 52/150 [01:42<02:40,  1.63s/it] 35%|███▌      | 53/150 [01:43<02:31,  1.56s/it] 36%|███▌      | 54/150 [01:45<02:25,  1.51s/it] 37%|███▋      | 55/150 [01:46<02:19,  1.47s/it] 37%|███▋      | 56/150 [01:48<02:15,  1.44s/it] 38%|███▊      | 57/150 [01:49<02:13,  1.43s/it] 39%|███▊      | 58/150 [01:51<02:15,  1.47s/it] 39%|███▉      | 59/150 [01:52<02:11,  1.45s/it] 40%|████      | 60/150 [01:53<02:08,  1.43s/it]                                                 40%|████      | 60/150 [01:53<02:08,  1.43s/it]int64
{'eval_loss': 0.01860988140106201, 'eval_rouge1': 65.6944, 'eval_rouge2': 55.9524, 'eval_rougeL': 65.7738, 'eval_rougeLsum': 65.6746, 'eval_runtime': 1.908, 'eval_samples_per_second': 20.964, 'eval_steps_per_second': 1.048, 'epoch': 3.0}
{'loss': 0.0468, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.24it/s][A                                                
                                             [A 40%|████      | 60/150 [01:55<02:08,  1.43s/it]
100%|██████████| 2/2 [00:01<00:00,  2.24it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [02:01<04:54,  3.31s/it] 41%|████▏     | 62/150 [02:02<04:00,  2.73s/it] 42%|████▏     | 63/150 [02:04<03:22,  2.33s/it] 43%|████▎     | 64/150 [02:05<02:55,  2.04s/it] 43%|████▎     | 65/150 [02:07<02:36,  1.84s/it] 44%|████▍     | 66/150 [02:08<02:23,  1.71s/it] 45%|████▍     | 67/150 [02:09<02:13,  1.61s/it] 45%|████▌     | 68/150 [02:11<02:06,  1.54s/it] 46%|████▌     | 69/150 [02:12<02:00,  1.49s/it] 47%|████▋     | 70/150 [02:14<01:57,  1.47s/it] 47%|████▋     | 71/150 [02:15<01:53,  1.44s/it] 48%|████▊     | 72/150 [02:16<01:50,  1.42s/it] 49%|████▊     | 73/150 [02:18<01:48,  1.41s/it] 49%|████▉     | 74/150 [02:19<01:46,  1.40s/it] 50%|█████     | 75/150 [02:20<01:44,  1.39s/it]                                                 50%|█████     | 75/150 [02:20<01:44,  1.39s/it]int64
{'eval_loss': 0.025855248793959618, 'eval_rouge1': 65.6944, 'eval_rouge2': 55.9524, 'eval_rougeL': 65.7738, 'eval_rougeLsum': 65.6746, 'eval_runtime': 1.9067, 'eval_samples_per_second': 20.979, 'eval_steps_per_second': 1.049, 'epoch': 4.0}
{'loss': 0.0262, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 50%|█████     | 75/150 [02:22<01:44,  1.39s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [02:28<04:10,  3.38s/it] 51%|█████▏    | 77/150 [02:30<03:22,  2.78s/it] 52%|█████▏    | 78/150 [02:31<02:49,  2.36s/it] 53%|█████▎    | 79/150 [02:33<02:26,  2.07s/it] 53%|█████▎    | 80/150 [02:34<02:11,  1.87s/it] 54%|█████▍    | 81/150 [02:35<01:59,  1.72s/it] 55%|█████▍    | 82/150 [02:37<01:50,  1.62s/it] 55%|█████▌    | 83/150 [02:38<01:43,  1.55s/it] 56%|█████▌    | 84/150 [02:40<01:39,  1.50s/it] 57%|█████▋    | 85/150 [02:41<01:35,  1.47s/it] 57%|█████▋    | 86/150 [02:42<01:31,  1.44s/it] 58%|█████▊    | 87/150 [02:44<01:30,  1.43s/it] 59%|█████▊    | 88/150 [02:45<01:27,  1.42s/it] 59%|█████▉    | 89/150 [02:46<01:25,  1.40s/it] 60%|██████    | 90/150 [02:48<01:24,  1.40s/it]                                                 60%|██████    | 90/150 [02:48<01:24,  1.40s/it]int64
{'eval_loss': 0.02485519088804722, 'eval_rouge1': 65.6944, 'eval_rouge2': 55.9524, 'eval_rougeL': 65.7738, 'eval_rougeLsum': 65.6746, 'eval_runtime': 1.9204, 'eval_samples_per_second': 20.829, 'eval_steps_per_second': 1.041, 'epoch': 5.0}
{'loss': 0.0187, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.23it/s][A                                                
                                             [A 60%|██████    | 90/150 [02:50<01:24,  1.40s/it]
100%|██████████| 2/2 [00:01<00:00,  2.23it/s][A
                                             [A                                                 60%|██████    | 90/150 [02:56<01:24,  1.40s/it] 60%|██████    | 90/150 [02:56<01:57,  1.97s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.023533295840024948, 'eval_rouge1': 66.0714, 'eval_rouge2': 56.881, 'eval_rougeL': 66.1706, 'eval_rougeLsum': 66.0714, 'eval_runtime': 1.9143, 'eval_samples_per_second': 20.895, 'eval_steps_per_second': 1.045, 'epoch': 6.0}
{'train_runtime': 184.9285, 'train_samples_per_second': 19.467, 'train_steps_per_second': 0.811, 'train_loss': 1.4933876630332734, 'epoch': 6.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁▁
wandb:                    eval/rouge1 █▅▁▁▁▄
wandb:                    eval/rouge2 ▅█▁▁▁█
wandb:                    eval/rougeL █▄▁▁▁▅
wandb:                 eval/rougeLsum █▅▁▁▁▄
wandb:                   eval/runtime █▂▁▁▄▃
wandb:        eval/samples_per_second ▁▆██▅▆
wandb:          eval/steps_per_second ▁▇██▄▆
wandb:                    train/epoch ▁▁▂▂▄▄▅▅▇▇███
wandb:              train/global_step ▁▁▂▂▄▄▅▅▇▇███
wandb:            train/learning_rate █▇▅▄▂▁
wandb:                     train/loss █▂▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.02353
wandb:                    eval/rouge1 66.0714
wandb:                    eval/rouge2 56.881
wandb:                    eval/rougeL 66.1706
wandb:                 eval/rougeLsum 66.0714
wandb:                   eval/runtime 1.9143
wandb:        eval/samples_per_second 20.895
wandb:          eval/steps_per_second 1.045
wandb:                    train/epoch 6.0
wandb:              train/global_step 90
wandb:            train/learning_rate 0.00012
wandb:                     train/loss 0.0187
wandb:               train/total_flos 49109941370880.0
wandb:               train/train_loss 1.49339
wandb:            train/train_runtime 184.9285
wandb: train/train_samples_per_second 19.467
wandb:   train/train_steps_per_second 0.811
wandb: 
wandb: 🚀 View run flan-t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/x28ifkb5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_094832-x28ifkb5/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_095222-s6rghfcm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/s6rghfcm
################################################ t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:12<30:42, 12.36s/it]  1%|▏         | 2/150 [00:13<14:22,  5.83s/it]  2%|▏         | 3/150 [00:14<09:08,  3.73s/it]  3%|▎         | 4/150 [00:16<06:42,  2.76s/it]  3%|▎         | 5/150 [00:17<05:20,  2.21s/it]  4%|▍         | 6/150 [00:18<04:30,  1.88s/it]  5%|▍         | 7/150 [00:19<03:59,  1.68s/it]  5%|▌         | 8/150 [00:21<03:37,  1.53s/it]  6%|▌         | 9/150 [00:22<03:23,  1.44s/it]  7%|▋         | 10/150 [00:23<03:13,  1.38s/it]  7%|▋         | 11/150 [00:24<03:06,  1.34s/it]  8%|▊         | 12/150 [00:26<03:01,  1.32s/it]  9%|▊         | 13/150 [00:27<02:57,  1.29s/it]  9%|▉         | 14/150 [00:28<02:53,  1.28s/it] 10%|█         | 15/150 [00:29<02:51,  1.27s/it]                                                 10%|█         | 15/150 [00:29<02:51,  1.27s/it]{'loss': 2.1745, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.44it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:00<00:00,  2.44it/s][A 10%|█         | 15/150 [00:31<02:51,  1.27s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [00:36<06:37,  2.97s/it] 11%|█▏        | 17/150 [00:37<05:26,  2.45s/it] 12%|█▏        | 18/150 [00:39<04:35,  2.09s/it] 13%|█▎        | 19/150 [00:40<04:00,  1.84s/it] 13%|█▎        | 20/150 [00:41<03:36,  1.67s/it] 14%|█▍        | 21/150 [00:42<03:18,  1.54s/it] 15%|█▍        | 22/150 [00:44<03:05,  1.45s/it] 15%|█▌        | 23/150 [00:45<02:55,  1.38s/it] 16%|█▌        | 24/150 [00:46<02:48,  1.34s/it] 17%|█▋        | 25/150 [00:47<02:43,  1.31s/it] 17%|█▋        | 26/150 [00:49<02:40,  1.29s/it] 18%|█▊        | 27/150 [00:50<02:36,  1.27s/it] 19%|█▊        | 28/150 [00:51<02:33,  1.26s/it] 19%|█▉        | 29/150 [00:52<02:32,  1.26s/it] 20%|██        | 30/150 [00:54<02:29,  1.24s/it]                                                 20%|██        | 30/150 [00:54<02:29,  1.24s/it]int64
{'eval_loss': 0.06180746480822563, 'eval_rouge1': 53.4425, 'eval_rouge2': 45.3929, 'eval_rougeL': 53.2738, 'eval_rougeLsum': 53.1746, 'eval_runtime': 1.8005, 'eval_samples_per_second': 22.216, 'eval_steps_per_second': 1.111, 'epoch': 1.0}
{'loss': 0.0759, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.61it/s][A                                                
                                             [A 20%|██        | 30/150 [00:55<02:29,  1.24s/it]
100%|██████████| 2/2 [00:00<00:00,  2.61it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [01:01<05:54,  2.98s/it] 21%|██▏       | 32/150 [01:02<04:49,  2.45s/it] 22%|██▏       | 33/150 [01:03<04:03,  2.08s/it] 23%|██▎       | 34/150 [01:04<03:31,  1.83s/it] 23%|██▎       | 35/150 [01:06<03:09,  1.64s/it] 24%|██▍       | 36/150 [01:07<02:52,  1.52s/it] 25%|██▍       | 37/150 [01:08<02:41,  1.43s/it] 25%|██▌       | 38/150 [01:09<02:34,  1.38s/it] 26%|██▌       | 39/150 [01:10<02:27,  1.33s/it] 27%|██▋       | 40/150 [01:12<02:22,  1.30s/it] 27%|██▋       | 41/150 [01:13<02:19,  1.28s/it] 28%|██▊       | 42/150 [01:14<02:16,  1.27s/it] 29%|██▊       | 43/150 [01:15<02:14,  1.26s/it] 29%|██▉       | 44/150 [01:17<02:11,  1.24s/it] 30%|███       | 45/150 [01:18<02:10,  1.24s/it]                                                 30%|███       | 45/150 [01:18<02:10,  1.24s/it]int64
{'eval_loss': 0.012277188710868359, 'eval_rouge1': 65.754, 'eval_rouge2': 55.9286, 'eval_rougeL': 65.7044, 'eval_rougeLsum': 65.6548, 'eval_runtime': 1.6772, 'eval_samples_per_second': 23.85, 'eval_steps_per_second': 1.192, 'epoch': 2.0}
{'loss': 0.028, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A                                                
                                             [A 30%|███       | 45/150 [01:20<02:10,  1.24s/it]
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [01:25<05:03,  2.92s/it] 31%|███▏      | 47/150 [01:26<04:08,  2.41s/it] 32%|███▏      | 48/150 [01:27<03:37,  2.13s/it] 33%|███▎      | 49/150 [01:29<03:08,  1.86s/it] 33%|███▎      | 50/150 [01:30<02:47,  1.68s/it] 34%|███▍      | 51/150 [01:31<02:34,  1.56s/it] 35%|███▍      | 52/150 [01:32<02:23,  1.46s/it] 35%|███▌      | 53/150 [01:34<02:15,  1.39s/it] 36%|███▌      | 54/150 [01:35<02:10,  1.35s/it] 37%|███▋      | 55/150 [01:36<02:05,  1.33s/it] 37%|███▋      | 56/150 [01:37<02:02,  1.31s/it] 38%|███▊      | 57/150 [01:39<01:59,  1.29s/it] 39%|███▊      | 58/150 [01:40<01:57,  1.28s/it] 39%|███▉      | 59/150 [01:41<01:55,  1.27s/it] 40%|████      | 60/150 [01:42<01:54,  1.27s/it]                                                 40%|████      | 60/150 [01:42<01:54,  1.27s/it]int64
{'eval_loss': 0.023795250803232193, 'eval_rouge1': 66.1607, 'eval_rouge2': 56.8571, 'eval_rougeL': 66.1607, 'eval_rougeLsum': 66.0714, 'eval_runtime': 1.7209, 'eval_samples_per_second': 23.244, 'eval_steps_per_second': 1.162, 'epoch': 3.0}
{'loss': 0.0196, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.51it/s][A                                                
                                             [A 40%|████      | 60/150 [01:44<01:54,  1.27s/it]
100%|██████████| 2/2 [00:00<00:00,  2.51it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [01:50<04:35,  3.09s/it] 41%|████▏     | 62/150 [01:51<03:43,  2.54s/it] 42%|████▏     | 63/150 [01:52<03:07,  2.15s/it] 43%|████▎     | 64/150 [01:53<02:41,  1.88s/it] 43%|████▎     | 65/150 [01:55<02:23,  1.69s/it] 44%|████▍     | 66/150 [01:56<02:10,  1.56s/it] 45%|████▍     | 67/150 [01:57<02:01,  1.46s/it] 45%|████▌     | 68/150 [01:58<01:54,  1.39s/it] 46%|████▌     | 69/150 [02:00<01:49,  1.35s/it] 47%|████▋     | 70/150 [02:01<01:48,  1.35s/it] 47%|████▋     | 71/150 [02:02<01:44,  1.32s/it] 48%|████▊     | 72/150 [02:04<01:41,  1.30s/it] 49%|████▊     | 73/150 [02:05<01:38,  1.28s/it] 49%|████▉     | 74/150 [02:06<01:36,  1.28s/it] 50%|█████     | 75/150 [02:07<01:34,  1.26s/it]                                                 50%|█████     | 75/150 [02:07<01:34,  1.26s/it]int64
{'eval_loss': 0.01399482972919941, 'eval_rouge1': 65.754, 'eval_rouge2': 55.9286, 'eval_rougeL': 65.7044, 'eval_rougeLsum': 65.6548, 'eval_runtime': 1.7193, 'eval_samples_per_second': 23.265, 'eval_steps_per_second': 1.163, 'epoch': 4.0}
{'loss': 0.0136, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A                                                
                                             [A 50%|█████     | 75/150 [02:09<01:34,  1.26s/it]
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A
                                             [A                                                 50%|█████     | 75/150 [02:15<01:34,  1.26s/it] 50%|█████     | 75/150 [02:15<02:15,  1.81s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.013266602531075478, 'eval_rouge1': 65.754, 'eval_rouge2': 55.9286, 'eval_rougeL': 65.7044, 'eval_rougeLsum': 65.6548, 'eval_runtime': 1.7109, 'eval_samples_per_second': 23.379, 'eval_steps_per_second': 1.169, 'epoch': 5.0}
{'train_runtime': 142.5565, 'train_samples_per_second': 25.253, 'train_steps_per_second': 1.052, 'train_loss': 0.46232993880907697, 'epoch': 5.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▁▃▁▁
wandb:                    eval/rouge1 ▁████
wandb:                    eval/rouge2 ▁▇█▇▇
wandb:                    eval/rougeL ▁████
wandb:                 eval/rougeLsum ▁████
wandb:                   eval/runtime █▁▃▃▃
wandb:        eval/samples_per_second ▁█▅▅▆
wandb:          eval/steps_per_second ▁█▅▅▆
wandb:                    train/epoch ▁▁▃▃▅▅▆▆███
wandb:              train/global_step ▁▁▃▃▅▅▆▆███
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01327
wandb:                    eval/rouge1 65.754
wandb:                    eval/rouge2 55.9286
wandb:                    eval/rougeL 65.7044
wandb:                 eval/rougeLsum 65.6548
wandb:                   eval/runtime 1.7109
wandb:        eval/samples_per_second 23.379
wandb:          eval/steps_per_second 1.169
wandb:                    train/epoch 5.0
wandb:              train/global_step 75
wandb:            train/learning_rate 0.00015
wandb:                     train/loss 0.0136
wandb:               train/total_flos 36394748928000.0
wandb:               train/train_loss 0.46233
wandb:            train/train_runtime 142.5565
wandb: train/train_samples_per_second 25.253
wandb:   train/train_steps_per_second 1.052
wandb: 
wandb: 🚀 View run t5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/s6rghfcm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_095222-s6rghfcm/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_095539-hrcufonz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/hrcufonz
################################################ flan-t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:15<38:27, 15.49s/it]  1%|▏         | 2/150 [00:19<21:53,  8.88s/it]  2%|▏         | 3/150 [00:23<16:28,  6.73s/it]  3%|▎         | 4/150 [00:28<13:54,  5.72s/it]  3%|▎         | 5/150 [00:32<12:29,  5.17s/it]  4%|▍         | 6/150 [00:36<11:41,  4.87s/it]  5%|▍         | 7/150 [00:40<11:08,  4.67s/it]  5%|▌         | 8/150 [00:45<10:43,  4.54s/it]  6%|▌         | 9/150 [00:49<10:25,  4.43s/it]  7%|▋         | 10/150 [00:53<10:14,  4.39s/it]  7%|▋         | 11/150 [00:57<10:02,  4.34s/it]  8%|▊         | 12/150 [01:01<09:52,  4.30s/it]  9%|▊         | 13/150 [01:06<09:43,  4.26s/it]  9%|▉         | 14/150 [01:10<09:37,  4.25s/it] 10%|█         | 15/150 [01:14<09:33,  4.25s/it]                                                 10%|█         | 15/150 [01:14<09:33,  4.25s/it]{'loss': 7.5531, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A
                                             [A                                                
100%|██████████| 2/2 [00:02<00:00,  1.26s/it][A 10%|█         | 15/150 [01:19<09:33,  4.25s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [01:35<20:47,  9.31s/it] 11%|█▏        | 17/150 [01:39<17:14,  7.78s/it] 12%|█▏        | 18/150 [01:44<14:48,  6.73s/it] 13%|█▎        | 19/150 [01:48<13:04,  5.99s/it] 13%|█▎        | 20/150 [01:52<11:48,  5.45s/it] 14%|█▍        | 21/150 [01:56<10:55,  5.08s/it] 15%|█▍        | 22/150 [02:01<10:16,  4.81s/it] 15%|█▌        | 23/150 [02:05<09:54,  4.68s/it] 16%|█▌        | 24/150 [02:09<09:35,  4.57s/it] 17%|█▋        | 25/150 [02:14<09:19,  4.48s/it] 17%|█▋        | 26/150 [02:18<09:07,  4.41s/it] 18%|█▊        | 27/150 [02:22<08:55,  4.36s/it] 19%|█▊        | 28/150 [02:26<08:47,  4.32s/it] 19%|█▉        | 29/150 [02:30<08:38,  4.29s/it] 20%|██        | 30/150 [02:35<08:33,  4.28s/it]                                                 20%|██        | 30/150 [02:35<08:33,  4.28s/it]int64
{'eval_loss': 0.9900115728378296, 'eval_rouge1': 60.8254, 'eval_rouge2': 49.6369, 'eval_rougeL': 60.5823, 'eval_rougeLsum': 60.4769, 'eval_runtime': 5.0965, 'eval_samples_per_second': 7.849, 'eval_steps_per_second': 0.392, 'epoch': 1.0}
{'loss': 0.7738, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                
                                             [A 20%|██        | 30/150 [02:40<08:33,  4.28s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [02:57<19:23,  9.78s/it] 21%|██▏       | 32/150 [03:02<15:59,  8.13s/it] 22%|██▏       | 33/150 [03:06<13:43,  7.04s/it] 23%|██▎       | 34/150 [03:10<12:00,  6.21s/it] 23%|██▎       | 35/150 [03:15<10:47,  5.63s/it] 24%|██▍       | 36/150 [03:19<09:52,  5.20s/it] 25%|██▍       | 37/150 [03:23<09:14,  4.90s/it] 25%|██▌       | 38/150 [03:27<08:46,  4.70s/it] 26%|██▌       | 39/150 [03:31<08:24,  4.55s/it] 27%|██▋       | 40/150 [03:36<08:10,  4.46s/it] 27%|██▋       | 41/150 [03:40<08:00,  4.41s/it] 28%|██▊       | 42/150 [03:44<07:50,  4.36s/it] 29%|██▊       | 43/150 [03:48<07:42,  4.33s/it] 29%|██▉       | 44/150 [03:53<07:41,  4.35s/it] 30%|███       | 45/150 [03:57<07:33,  4.32s/it]                                                 30%|███       | 45/150 [03:57<07:33,  4.32s/it]int64
{'eval_loss': 0.05416952818632126, 'eval_rouge1': 62.0933, 'eval_rouge2': 50.9524, 'eval_rougeL': 62.1429, 'eval_rougeLsum': 62.0337, 'eval_runtime': 5.1131, 'eval_samples_per_second': 7.823, 'eval_steps_per_second': 0.391, 'epoch': 2.0}
{'loss': 0.067, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                
                                             [A 30%|███       | 45/150 [04:02<07:33,  4.32s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [04:20<16:59,  9.80s/it] 31%|███▏      | 47/150 [04:24<13:57,  8.13s/it] 32%|███▏      | 48/150 [04:28<11:50,  6.97s/it] 33%|███▎      | 49/150 [04:33<10:27,  6.22s/it] 33%|███▎      | 50/150 [04:37<09:23,  5.63s/it] 34%|███▍      | 51/150 [04:41<08:37,  5.23s/it] 35%|███▍      | 52/150 [04:45<08:02,  4.93s/it] 35%|███▌      | 53/150 [04:50<07:37,  4.71s/it] 36%|███▌      | 54/150 [04:54<07:17,  4.56s/it] 37%|███▋      | 55/150 [04:58<07:03,  4.46s/it] 37%|███▋      | 56/150 [05:02<06:53,  4.40s/it] 38%|███▊      | 57/150 [05:07<06:44,  4.35s/it] 39%|███▊      | 58/150 [05:11<06:36,  4.31s/it] 39%|███▉      | 59/150 [05:15<06:31,  4.30s/it] 40%|████      | 60/150 [05:20<06:30,  4.34s/it]                                                 40%|████      | 60/150 [05:20<06:30,  4.34s/it]int64
{'eval_loss': 0.012852663174271584, 'eval_rouge1': 66.3095, 'eval_rouge2': 56.6429, 'eval_rougeL': 66.25, 'eval_rougeLsum': 66.1905, 'eval_runtime': 5.0868, 'eval_samples_per_second': 7.863, 'eval_steps_per_second': 0.393, 'epoch': 3.0}
{'loss': 0.0348, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.27s/it][A                                                
                                             [A 40%|████      | 60/150 [05:25<06:30,  4.34s/it]
100%|██████████| 2/2 [00:02<00:00,  1.27s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [05:42<14:25,  9.73s/it] 41%|████▏     | 62/150 [05:46<11:51,  8.08s/it] 42%|████▏     | 63/150 [05:50<10:03,  6.93s/it] 43%|████▎     | 64/150 [05:55<08:46,  6.12s/it] 43%|████▎     | 65/150 [05:59<07:53,  5.57s/it] 44%|████▍     | 66/150 [06:03<07:16,  5.19s/it] 45%|████▍     | 67/150 [06:07<06:47,  4.91s/it] 45%|████▌     | 68/150 [06:12<06:24,  4.69s/it] 46%|████▌     | 69/150 [06:16<06:09,  4.56s/it] 47%|████▋     | 70/150 [06:20<05:55,  4.45s/it] 47%|████▋     | 71/150 [06:24<05:47,  4.40s/it] 48%|████▊     | 72/150 [06:29<05:39,  4.35s/it] 49%|████▊     | 73/150 [06:33<05:32,  4.32s/it] 49%|████▉     | 74/150 [06:37<05:27,  4.31s/it] 50%|█████     | 75/150 [06:41<05:21,  4.28s/it]                                                 50%|█████     | 75/150 [06:41<05:21,  4.28s/it]int64
{'eval_loss': 0.007643365766853094, 'eval_rouge1': 66.3095, 'eval_rouge2': 56.6429, 'eval_rougeL': 66.25, 'eval_rougeLsum': 66.1905, 'eval_runtime': 5.1477, 'eval_samples_per_second': 7.771, 'eval_steps_per_second': 0.389, 'epoch': 4.0}
{'loss': 0.0246, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.24s/it][A                                                
                                             [A 50%|█████     | 75/150 [06:46<05:21,  4.28s/it]
100%|██████████| 2/2 [00:02<00:00,  1.24s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [07:03<11:36,  9.41s/it] 51%|█████▏    | 77/150 [07:07<09:33,  7.86s/it] 52%|█████▏    | 78/150 [07:11<08:07,  6.77s/it] 53%|█████▎    | 79/150 [07:15<07:07,  6.03s/it] 53%|█████▎    | 80/150 [07:20<06:24,  5.50s/it] 54%|█████▍    | 81/150 [07:24<05:53,  5.12s/it] 55%|█████▍    | 82/150 [07:28<05:31,  4.87s/it] 55%|█████▌    | 83/150 [07:32<05:13,  4.68s/it] 56%|█████▌    | 84/150 [07:37<05:00,  4.55s/it] 57%|█████▋    | 85/150 [07:41<04:49,  4.46s/it] 57%|█████▋    | 86/150 [07:45<04:39,  4.37s/it] 58%|█████▊    | 87/150 [07:49<04:34,  4.36s/it] 59%|█████▊    | 88/150 [07:54<04:29,  4.34s/it] 59%|█████▉    | 89/150 [07:58<04:22,  4.31s/it] 60%|██████    | 90/150 [08:02<04:17,  4.29s/it]                                                 60%|██████    | 90/150 [08:02<04:17,  4.29s/it]int64
{'eval_loss': 0.008887741714715958, 'eval_rouge1': 66.3095, 'eval_rouge2': 57.1667, 'eval_rougeL': 66.1905, 'eval_rougeLsum': 66.2798, 'eval_runtime': 5.079, 'eval_samples_per_second': 7.875, 'eval_steps_per_second': 0.394, 'epoch': 5.0}
{'loss': 0.0148, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                
                                             [A 60%|██████    | 90/150 [08:07<04:17,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [08:26<09:51, 10.02s/it] 61%|██████▏   | 92/150 [08:30<07:59,  8.27s/it] 62%|██████▏   | 93/150 [08:34<06:41,  7.05s/it] 63%|██████▎   | 94/150 [08:38<05:48,  6.22s/it] 63%|██████▎   | 95/150 [08:43<05:09,  5.63s/it] 64%|██████▍   | 96/150 [08:47<04:45,  5.29s/it] 65%|██████▍   | 97/150 [08:51<04:23,  4.97s/it] 65%|██████▌   | 98/150 [08:56<04:07,  4.76s/it] 66%|██████▌   | 99/150 [09:00<03:55,  4.61s/it] 67%|██████▋   | 100/150 [09:04<03:44,  4.49s/it] 67%|██████▋   | 101/150 [09:08<03:35,  4.40s/it] 68%|██████▊   | 102/150 [09:12<03:28,  4.34s/it] 69%|██████▊   | 103/150 [09:17<03:22,  4.30s/it] 69%|██████▉   | 104/150 [09:21<03:16,  4.27s/it] 70%|███████   | 105/150 [09:25<03:11,  4.26s/it]                                                  70%|███████   | 105/150 [09:25<03:11,  4.26s/it]int64
{'eval_loss': 0.01258180569857359, 'eval_rouge1': 65.248, 'eval_rouge2': 55.2857, 'eval_rougeL': 65.1984, 'eval_rougeLsum': 65.1389, 'eval_runtime': 5.1208, 'eval_samples_per_second': 7.811, 'eval_steps_per_second': 0.391, 'epoch': 6.0}
{'loss': 0.0122, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A                                                 
                                             [A 70%|███████   | 105/150 [09:30<03:11,  4.26s/it]
100%|██████████| 2/2 [00:02<00:00,  1.25s/it][A
                                             [A                                                  70%|███████   | 105/150 [09:48<03:11,  4.26s/it] 70%|███████   | 105/150 [09:49<04:12,  5.61s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.012606298550963402, 'eval_rouge1': 65.248, 'eval_rouge2': 55.2857, 'eval_rougeL': 65.1984, 'eval_rougeLsum': 65.1389, 'eval_runtime': 5.0846, 'eval_samples_per_second': 7.867, 'eval_steps_per_second': 0.393, 'epoch': 7.0}
{'train_runtime': 596.2563, 'train_samples_per_second': 6.038, 'train_steps_per_second': 0.252, 'train_loss': 1.211480852393877, 'epoch': 7.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▃███▇▇
wandb:                    eval/rouge2 ▁▂███▆▆
wandb:                    eval/rougeL ▁▃███▇▇
wandb:                 eval/rougeLsum ▁▃███▇▇
wandb:                   eval/runtime ▃▄▂█▁▅▂
wandb:        eval/samples_per_second ▆▄▇▁█▄▇
wandb:          eval/steps_per_second ▅▄▇▁█▄▇
wandb:                    train/epoch ▁▁▂▂▃▃▅▅▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▅▅▆▆▇▇███
wandb:            train/learning_rate █▇▆▅▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01261
wandb:                    eval/rouge1 65.248
wandb:                    eval/rouge2 55.2857
wandb:                    eval/rougeL 65.1984
wandb:                 eval/rougeLsum 65.1389
wandb:                   eval/runtime 5.0846
wandb:        eval/samples_per_second 7.867
wandb:          eval/steps_per_second 0.393
wandb:                    train/epoch 7.0
wandb:              train/global_step 105
wandb:            train/learning_rate 9e-05
wandb:                     train/loss 0.0122
wandb:               train/total_flos 192844255887360.0
wandb:               train/train_loss 1.21148
wandb:            train/train_runtime 596.2563
wandb: train/train_samples_per_second 6.038
wandb:   train/train_steps_per_second 0.252
wandb: 
wandb: 🚀 View run flan-t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/hrcufonz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_095539-hrcufonz/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_100640-5w98e7e0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/5w98e7e0
################################################ t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:14<35:49, 14.42s/it]  1%|▏         | 2/150 [00:18<20:37,  8.36s/it]  2%|▏         | 3/150 [00:22<15:35,  6.36s/it]  3%|▎         | 4/150 [00:26<13:12,  5.43s/it]  3%|▎         | 5/150 [00:30<11:50,  4.90s/it]  4%|▍         | 6/150 [00:34<11:02,  4.60s/it]  5%|▍         | 7/150 [00:38<10:29,  4.40s/it]  5%|▌         | 8/150 [00:42<10:06,  4.27s/it]  6%|▌         | 9/150 [00:46<09:49,  4.18s/it]  7%|▋         | 10/150 [00:50<09:38,  4.13s/it]  7%|▋         | 11/150 [00:54<09:27,  4.08s/it]  8%|▊         | 12/150 [00:58<09:17,  4.04s/it]  9%|▊         | 13/150 [01:02<09:11,  4.02s/it]  9%|▉         | 14/150 [01:06<09:17,  4.10s/it] 10%|█         | 15/150 [01:10<09:11,  4.08s/it]                                                 10%|█         | 15/150 [01:10<09:11,  4.08s/it]{'loss': 1.7932, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.14s/it][A
                                             [A                                                
100%|██████████| 2/2 [00:02<00:00,  1.14s/it][A 10%|█         | 15/150 [01:15<09:11,  4.08s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [01:30<19:48,  8.87s/it] 11%|█▏        | 17/150 [01:34<16:31,  7.46s/it] 12%|█▏        | 18/150 [01:38<14:07,  6.42s/it] 13%|█▎        | 19/150 [01:42<12:25,  5.69s/it] 13%|█▎        | 20/150 [01:46<11:15,  5.19s/it] 14%|█▍        | 21/150 [01:50<10:25,  4.85s/it] 15%|█▍        | 22/150 [01:54<09:49,  4.60s/it] 15%|█▌        | 23/150 [01:58<09:21,  4.42s/it] 16%|█▌        | 24/150 [02:02<09:01,  4.30s/it] 17%|█▋        | 25/150 [02:06<08:46,  4.21s/it] 17%|█▋        | 26/150 [02:11<08:34,  4.15s/it] 18%|█▊        | 27/150 [02:14<08:24,  4.10s/it] 19%|█▊        | 28/150 [02:19<08:17,  4.08s/it] 19%|█▉        | 29/150 [02:23<08:10,  4.05s/it] 20%|██        | 30/150 [02:27<08:05,  4.05s/it]                                                 20%|██        | 30/150 [02:27<08:05,  4.05s/it]int64
{'eval_loss': 0.041364651173353195, 'eval_rouge1': 62.004, 'eval_rouge2': 50.2857, 'eval_rougeL': 61.6171, 'eval_rougeLsum': 61.6468, 'eval_runtime': 4.8041, 'eval_samples_per_second': 8.326, 'eval_steps_per_second': 0.416, 'epoch': 1.0}
{'loss': 0.0567, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 20%|██        | 30/150 [02:31<08:05,  4.05s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [02:47<18:03,  9.11s/it] 21%|██▏       | 32/150 [02:51<14:53,  7.57s/it] 22%|██▏       | 33/150 [02:55<12:39,  6.49s/it] 23%|██▎       | 34/150 [02:59<11:05,  5.74s/it] 23%|██▎       | 35/150 [03:03<10:00,  5.22s/it] 24%|██▍       | 36/150 [03:07<09:13,  4.85s/it] 25%|██▍       | 37/150 [03:11<08:39,  4.60s/it] 25%|██▌       | 38/150 [03:15<08:13,  4.40s/it] 26%|██▌       | 39/150 [03:19<07:53,  4.26s/it] 27%|██▋       | 40/150 [03:23<07:40,  4.18s/it] 27%|██▋       | 41/150 [03:27<07:29,  4.12s/it] 28%|██▊       | 42/150 [03:31<07:18,  4.06s/it] 29%|██▊       | 43/150 [03:35<07:13,  4.05s/it] 29%|██▉       | 44/150 [03:39<07:09,  4.05s/it] 30%|███       | 45/150 [03:43<07:02,  4.02s/it]                                                 30%|███       | 45/150 [03:43<07:02,  4.02s/it]int64
{'eval_loss': 0.012563732452690601, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7436, 'eval_samples_per_second': 8.432, 'eval_steps_per_second': 0.422, 'epoch': 2.0}
{'loss': 0.0202, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A                                                
                                             [A 30%|███       | 45/150 [03:48<07:02,  4.02s/it]
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [04:04<15:50,  9.14s/it] 31%|███▏      | 47/150 [04:08<13:01,  7.59s/it] 32%|███▏      | 48/150 [04:12<11:04,  6.51s/it] 33%|███▎      | 49/150 [04:16<09:41,  5.75s/it] 33%|███▎      | 50/150 [04:20<08:42,  5.22s/it] 34%|███▍      | 51/150 [04:24<08:05,  4.90s/it] 35%|███▍      | 52/150 [04:28<07:34,  4.64s/it] 35%|███▌      | 53/150 [04:32<07:10,  4.44s/it] 36%|███▌      | 54/150 [04:36<06:53,  4.31s/it] 37%|███▋      | 55/150 [04:40<06:41,  4.23s/it] 37%|███▋      | 56/150 [04:44<06:32,  4.17s/it] 38%|███▊      | 57/150 [04:48<06:21,  4.10s/it] 39%|███▊      | 58/150 [04:52<06:15,  4.08s/it] 39%|███▉      | 59/150 [04:56<06:08,  4.05s/it] 40%|████      | 60/150 [05:01<06:06,  4.07s/it]                                                 40%|████      | 60/150 [05:01<06:06,  4.07s/it]int64
{'eval_loss': 0.0058716884814202785, 'eval_rouge1': 66.6667, 'eval_rouge2': 57.5714, 'eval_rougeL': 66.6071, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7195, 'eval_samples_per_second': 8.475, 'eval_steps_per_second': 0.424, 'epoch': 3.0}
{'loss': 0.0161, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 40%|████      | 60/150 [05:05<06:06,  4.07s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [05:22<13:39,  9.21s/it] 41%|████▏     | 62/150 [05:26<11:13,  7.65s/it] 42%|████▏     | 63/150 [05:30<09:30,  6.55s/it] 43%|████▎     | 64/150 [05:34<08:16,  5.78s/it] 43%|████▎     | 65/150 [05:38<07:23,  5.22s/it] 44%|████▍     | 66/150 [05:42<06:48,  4.86s/it] 45%|████▍     | 67/150 [05:46<06:23,  4.62s/it] 45%|████▌     | 68/150 [05:50<06:03,  4.43s/it] 46%|████▌     | 69/150 [05:54<05:48,  4.31s/it] 47%|████▋     | 70/150 [05:58<05:36,  4.21s/it] 47%|████▋     | 71/150 [06:02<05:28,  4.16s/it] 48%|████▊     | 72/150 [06:06<05:20,  4.10s/it] 49%|████▊     | 73/150 [06:10<05:13,  4.07s/it] 49%|████▉     | 74/150 [06:14<05:08,  4.06s/it] 50%|█████     | 75/150 [06:18<05:02,  4.03s/it]                                                 50%|█████     | 75/150 [06:18<05:02,  4.03s/it]int64
{'eval_loss': 0.005779453087598085, 'eval_rouge1': 66.25, 'eval_rouge2': 56.5714, 'eval_rougeL': 66.25, 'eval_rougeLsum': 66.25, 'eval_runtime': 4.7333, 'eval_samples_per_second': 8.451, 'eval_steps_per_second': 0.423, 'epoch': 4.0}
{'loss': 0.0141, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 50%|█████     | 75/150 [06:22<05:02,  4.03s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [06:39<11:12,  9.09s/it] 51%|█████▏    | 77/150 [06:43<09:15,  7.61s/it] 52%|█████▏    | 78/150 [06:47<07:50,  6.54s/it] 53%|█████▎    | 79/150 [06:51<06:48,  5.76s/it] 53%|█████▎    | 80/150 [06:55<06:05,  5.23s/it] 54%|█████▍    | 81/150 [06:59<05:35,  4.86s/it] 55%|█████▍    | 82/150 [07:03<05:12,  4.60s/it] 55%|█████▌    | 83/150 [07:07<04:54,  4.39s/it] 56%|█████▌    | 84/150 [07:11<04:42,  4.28s/it] 57%|█████▋    | 85/150 [07:15<04:34,  4.22s/it] 57%|█████▋    | 86/150 [07:19<04:26,  4.16s/it] 58%|█████▊    | 87/150 [07:23<04:19,  4.11s/it] 59%|█████▊    | 88/150 [07:27<04:13,  4.09s/it] 59%|█████▉    | 89/150 [07:31<04:07,  4.06s/it] 60%|██████    | 90/150 [07:35<04:02,  4.04s/it]                                                 60%|██████    | 90/150 [07:35<04:02,  4.04s/it]int64
{'eval_loss': 0.002825214760378003, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7416, 'eval_samples_per_second': 8.436, 'eval_steps_per_second': 0.422, 'epoch': 5.0}
{'loss': 0.0076, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A                                                
                                             [A 60%|██████    | 90/150 [07:40<04:02,  4.04s/it]
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [07:56<09:08,  9.30s/it] 61%|██████▏   | 92/150 [08:00<07:28,  7.73s/it] 62%|██████▏   | 93/150 [08:04<06:17,  6.62s/it] 63%|██████▎   | 94/150 [08:08<05:26,  5.84s/it] 63%|██████▎   | 95/150 [08:12<04:50,  5.29s/it] 64%|██████▍   | 96/150 [08:17<04:28,  4.97s/it] 65%|██████▍   | 97/150 [08:21<04:08,  4.69s/it] 65%|██████▌   | 98/150 [08:25<03:52,  4.47s/it] 66%|██████▌   | 99/150 [08:29<03:40,  4.33s/it] 67%|██████▋   | 100/150 [08:33<03:32,  4.25s/it] 67%|██████▋   | 101/150 [08:37<03:24,  4.17s/it] 68%|██████▊   | 102/150 [08:41<03:17,  4.11s/it] 69%|██████▊   | 103/150 [08:45<03:11,  4.07s/it] 69%|██████▉   | 104/150 [08:49<03:07,  4.07s/it] 70%|███████   | 105/150 [08:53<03:01,  4.03s/it]                                                  70%|███████   | 105/150 [08:53<03:01,  4.03s/it]int64
{'eval_loss': 0.0019732541404664516, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.8451, 'eval_samples_per_second': 8.256, 'eval_steps_per_second': 0.413, 'epoch': 6.0}
{'loss': 0.004, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A                                                 
                                             [A 70%|███████   | 105/150 [08:57<03:01,  4.03s/it]
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 106/150 [09:14<06:48,  9.29s/it] 71%|███████▏  | 107/150 [09:18<05:31,  7.70s/it] 72%|███████▏  | 108/150 [09:22<04:36,  6.59s/it] 73%|███████▎  | 109/150 [09:26<03:58,  5.81s/it] 73%|███████▎  | 110/150 [09:30<03:31,  5.28s/it] 74%|███████▍  | 111/150 [09:34<03:11,  4.90s/it] 75%|███████▍  | 112/150 [09:38<02:56,  4.64s/it] 75%|███████▌  | 113/150 [09:43<02:46,  4.51s/it] 76%|███████▌  | 114/150 [09:47<02:36,  4.36s/it] 77%|███████▋  | 115/150 [09:51<02:29,  4.26s/it] 77%|███████▋  | 116/150 [09:55<02:22,  4.18s/it] 78%|███████▊  | 117/150 [09:59<02:15,  4.11s/it] 79%|███████▊  | 118/150 [10:03<02:10,  4.09s/it] 79%|███████▉  | 119/150 [10:07<02:06,  4.07s/it] 80%|████████  | 120/150 [10:11<02:01,  4.05s/it]                                                  80%|████████  | 120/150 [10:11<02:01,  4.05s/it]int64
{'eval_loss': 0.0016330201178789139, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7695, 'eval_samples_per_second': 8.387, 'eval_steps_per_second': 0.419, 'epoch': 7.0}
{'loss': 0.0021, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A                                                 
                                             [A 80%|████████  | 120/150 [10:15<02:01,  4.05s/it]
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 121/150 [10:32<04:25,  9.16s/it] 81%|████████▏ | 122/150 [10:36<03:33,  7.63s/it] 82%|████████▏ | 123/150 [10:40<02:56,  6.54s/it] 83%|████████▎ | 124/150 [10:44<02:30,  5.77s/it] 83%|████████▎ | 125/150 [10:48<02:10,  5.24s/it] 84%|████████▍ | 126/150 [10:52<01:56,  4.87s/it] 85%|████████▍ | 127/150 [10:56<01:46,  4.61s/it] 85%|████████▌ | 128/150 [11:00<01:37,  4.42s/it] 86%|████████▌ | 129/150 [11:04<01:29,  4.28s/it] 87%|████████▋ | 130/150 [11:08<01:24,  4.22s/it] 87%|████████▋ | 131/150 [11:12<01:18,  4.14s/it] 88%|████████▊ | 132/150 [11:16<01:13,  4.09s/it] 89%|████████▊ | 133/150 [11:20<01:09,  4.06s/it] 89%|████████▉ | 134/150 [11:24<01:04,  4.04s/it] 90%|█████████ | 135/150 [11:28<01:00,  4.04s/it]                                                  90%|█████████ | 135/150 [11:28<01:00,  4.04s/it]int64
{'eval_loss': 0.0014707159716635942, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7529, 'eval_samples_per_second': 8.416, 'eval_steps_per_second': 0.421, 'epoch': 8.0}
{'loss': 0.0019, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                 
                                             [A 90%|█████████ | 135/150 [11:32<01:00,  4.04s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 136/150 [11:48<02:04,  8.87s/it] 91%|█████████▏| 137/150 [11:52<01:36,  7.41s/it] 92%|█████████▏| 138/150 [11:56<01:16,  6.39s/it] 93%|█████████▎| 139/150 [12:00<01:02,  5.68s/it] 93%|█████████▎| 140/150 [12:04<00:51,  5.18s/it] 94%|█████████▍| 141/150 [12:08<00:43,  4.84s/it] 95%|█████████▍| 142/150 [12:12<00:36,  4.59s/it] 95%|█████████▌| 143/150 [12:16<00:30,  4.42s/it] 96%|█████████▌| 144/150 [12:20<00:25,  4.30s/it] 97%|█████████▋| 145/150 [12:24<00:20,  4.19s/it] 97%|█████████▋| 146/150 [12:28<00:16,  4.14s/it] 98%|█████████▊| 147/150 [12:32<00:12,  4.09s/it] 99%|█████████▊| 148/150 [12:36<00:08,  4.08s/it] 99%|█████████▉| 149/150 [12:40<00:04,  4.06s/it]100%|██████████| 150/150 [12:44<00:00,  4.04s/it]                                                 100%|██████████| 150/150 [12:44<00:00,  4.04s/it]int64
{'eval_loss': 0.0019498886540532112, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7738, 'eval_samples_per_second': 8.379, 'eval_steps_per_second': 0.419, 'epoch': 9.0}
{'loss': 0.0012, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A                                                 
                                             [A100%|██████████| 150/150 [12:49<00:00,  4.04s/it]
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A                                                 100%|██████████| 150/150 [13:05<00:00,  4.04s/it]100%|██████████| 150/150 [13:06<00:00,  5.24s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.001779060228727758, 'eval_rouge1': 66.6071, 'eval_rouge2': 57.5833, 'eval_rougeL': 66.7262, 'eval_rougeLsum': 66.6667, 'eval_runtime': 4.7593, 'eval_samples_per_second': 8.405, 'eval_steps_per_second': 0.42, 'epoch': 10.0}
{'train_runtime': 792.7376, 'train_samples_per_second': 4.541, 'train_steps_per_second': 0.189, 'train_loss': 0.1917096728955706, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▃▂▂▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁██▇██████
wandb:                    eval/rouge2 ▁██▇██████
wandb:                    eval/rougeL ▁██▇██████
wandb:                 eval/rougeLsum ▁██▇██████
wandb:                   eval/runtime ▆▂▁▂▂█▄▃▄▃
wandb:        eval/samples_per_second ▃▇█▇▇▁▅▆▅▆
wandb:          eval/steps_per_second ▃▇█▇▇▁▅▆▅▅
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.00178
wandb:                    eval/rouge1 66.6071
wandb:                    eval/rouge2 57.5833
wandb:                    eval/rougeL 66.7262
wandb:                 eval/rougeLsum 66.6667
wandb:                   eval/runtime 4.7593
wandb:        eval/samples_per_second 8.405
wandb:          eval/steps_per_second 0.42
wandb:                    train/epoch 10.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0012
wandb:               train/total_flos 258790809600000.0
wandb:               train/train_loss 0.19171
wandb:            train/train_runtime 792.7376
wandb: train/train_samples_per_second 4.541
wandb:   train/train_steps_per_second 0.189
wandb: 
wandb: 🚀 View run t5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/5w98e7e0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_100640-5w98e7e0/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_102054-2bsvr4au
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mt5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/2bsvr4au
################################################ mt5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:14<34:48, 14.02s/it]  1%|▏         | 2/150 [00:17<18:42,  7.59s/it]  2%|▏         | 3/150 [00:20<13:33,  5.53s/it]  3%|▎         | 4/150 [00:23<11:04,  4.55s/it]  3%|▎         | 5/150 [00:26<09:41,  4.01s/it]  4%|▍         | 6/150 [00:29<08:54,  3.71s/it]  5%|▍         | 7/150 [00:32<08:20,  3.50s/it]  5%|▌         | 8/150 [00:35<07:57,  3.36s/it]  6%|▌         | 9/150 [00:38<07:41,  3.27s/it]  7%|▋         | 10/150 [00:41<07:29,  3.21s/it]  7%|▋         | 11/150 [00:44<07:21,  3.17s/it]  8%|▊         | 12/150 [00:47<07:14,  3.15s/it]  9%|▊         | 13/150 [00:50<07:07,  3.12s/it]  9%|▉         | 14/150 [00:54<07:03,  3.11s/it] 10%|█         | 15/150 [00:57<06:57,  3.09s/it]                                                 10%|█         | 15/150 [00:57<06:57,  3.09s/it]{'loss': 26.6449, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.36it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:01<00:00,  1.36it/s][A 10%|█         | 15/150 [01:00<06:57,  3.09s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 16/150 [01:14<16:19,  7.31s/it] 11%|█▏        | 17/150 [01:17<13:23,  6.04s/it] 12%|█▏        | 18/150 [01:20<11:19,  5.14s/it] 13%|█▎        | 19/150 [01:23<09:53,  4.53s/it] 13%|█▎        | 20/150 [01:26<08:52,  4.09s/it] 14%|█▍        | 21/150 [01:29<08:08,  3.79s/it] 15%|█▍        | 22/150 [01:32<07:37,  3.57s/it] 15%|█▌        | 23/150 [01:35<07:14,  3.42s/it] 16%|█▌        | 24/150 [01:38<06:59,  3.33s/it] 17%|█▋        | 25/150 [01:41<06:46,  3.25s/it] 17%|█▋        | 26/150 [01:44<06:36,  3.19s/it] 18%|█▊        | 27/150 [01:48<06:28,  3.16s/it] 19%|█▊        | 28/150 [01:51<06:23,  3.14s/it] 19%|█▉        | 29/150 [01:54<06:17,  3.12s/it] 20%|██        | 30/150 [01:57<06:11,  3.10s/it]                                                 20%|██        | 30/150 [01:57<06:11,  3.10s/it]int64
{'eval_loss': 12.475275039672852, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 3.1468, 'eval_samples_per_second': 12.711, 'eval_steps_per_second': 0.636, 'epoch': 1.0}
{'loss': 15.7084, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                
                                             [A 20%|██        | 30/150 [02:00<06:11,  3.10s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 31/150 [02:15<15:25,  7.78s/it] 21%|██▏       | 32/150 [02:19<12:31,  6.37s/it] 22%|██▏       | 33/150 [02:22<10:29,  5.38s/it] 23%|██▎       | 34/150 [02:25<09:04,  4.69s/it] 23%|██▎       | 35/150 [02:28<08:03,  4.21s/it] 24%|██▍       | 36/150 [02:31<07:21,  3.87s/it] 25%|██▍       | 37/150 [02:34<06:50,  3.63s/it] 25%|██▌       | 38/150 [02:37<06:28,  3.47s/it] 26%|██▌       | 39/150 [02:40<06:12,  3.35s/it] 27%|██▋       | 40/150 [02:43<05:58,  3.26s/it] 27%|██▋       | 41/150 [02:46<05:48,  3.20s/it] 28%|██▊       | 42/150 [02:49<05:42,  3.18s/it] 29%|██▊       | 43/150 [02:52<05:36,  3.15s/it] 29%|██▉       | 44/150 [02:55<05:30,  3.12s/it] 30%|███       | 45/150 [02:59<05:27,  3.12s/it]                                                 30%|███       | 45/150 [02:59<05:27,  3.12s/it]int64
{'eval_loss': 7.5620832443237305, 'eval_rouge1': 7.3703, 'eval_rouge2': 2.267, 'eval_rougeL': 7.48, 'eval_rougeLsum': 7.6061, 'eval_runtime': 3.6049, 'eval_samples_per_second': 11.096, 'eval_steps_per_second': 0.555, 'epoch': 2.0}
{'loss': 9.1001, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                
                                             [A 30%|███       | 45/150 [03:02<05:27,  3.12s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 46/150 [03:17<13:20,  7.69s/it] 31%|███▏      | 47/150 [03:20<10:49,  6.30s/it] 32%|███▏      | 48/150 [03:23<09:03,  5.33s/it] 33%|███▎      | 49/150 [03:26<07:49,  4.65s/it] 33%|███▎      | 50/150 [03:29<06:57,  4.18s/it] 34%|███▍      | 51/150 [03:32<06:20,  3.84s/it] 35%|███▍      | 52/150 [03:35<05:54,  3.61s/it] 35%|███▌      | 53/150 [03:38<05:34,  3.45s/it] 36%|███▌      | 54/150 [03:41<05:20,  3.33s/it] 37%|███▋      | 55/150 [03:45<05:09,  3.26s/it] 37%|███▋      | 56/150 [03:48<05:01,  3.21s/it] 38%|███▊      | 57/150 [03:51<04:54,  3.17s/it] 39%|███▊      | 58/150 [03:54<04:49,  3.14s/it] 39%|███▉      | 59/150 [03:57<04:44,  3.13s/it] 40%|████      | 60/150 [04:00<04:40,  3.11s/it]                                                 40%|████      | 60/150 [04:00<04:40,  3.11s/it]int64
{'eval_loss': 4.276005268096924, 'eval_rouge1': 26.2001, 'eval_rouge2': 8.6881, 'eval_rougeL': 25.9019, 'eval_rougeLsum': 26.0344, 'eval_runtime': 3.6131, 'eval_samples_per_second': 11.071, 'eval_steps_per_second': 0.554, 'epoch': 3.0}
{'loss': 5.2073, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.19it/s][A                                                
                                             [A 40%|████      | 60/150 [04:04<04:40,  3.11s/it]
100%|██████████| 2/2 [00:01<00:00,  1.19it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 61/150 [04:19<11:31,  7.77s/it] 41%|████▏     | 62/150 [04:22<09:20,  6.37s/it] 42%|████▏     | 63/150 [04:25<07:48,  5.38s/it] 43%|████▎     | 64/150 [04:28<06:43,  4.69s/it] 43%|████▎     | 65/150 [04:31<05:57,  4.21s/it] 44%|████▍     | 66/150 [04:34<05:25,  3.87s/it] 45%|████▍     | 67/150 [04:37<05:01,  3.63s/it] 45%|████▌     | 68/150 [04:40<04:43,  3.46s/it] 46%|████▌     | 69/150 [04:43<04:30,  3.34s/it] 47%|████▋     | 70/150 [04:46<04:21,  3.27s/it] 47%|████▋     | 71/150 [04:49<04:14,  3.22s/it] 48%|████▊     | 72/150 [04:53<04:07,  3.17s/it] 49%|████▊     | 73/150 [04:56<04:02,  3.14s/it] 49%|████▉     | 74/150 [04:59<03:58,  3.13s/it] 50%|█████     | 75/150 [05:02<03:53,  3.12s/it]                                                 50%|█████     | 75/150 [05:02<03:53,  3.12s/it]int64
{'eval_loss': 2.717198610305786, 'eval_rouge1': 18.1531, 'eval_rouge2': 6.647, 'eval_rougeL': 17.9658, 'eval_rougeLsum': 18.04, 'eval_runtime': 3.5898, 'eval_samples_per_second': 11.143, 'eval_steps_per_second': 0.557, 'epoch': 4.0}
{'loss': 2.8405, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                
                                             [A 50%|█████     | 75/150 [05:05<03:53,  3.12s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 76/150 [05:21<09:41,  7.86s/it] 51%|█████▏    | 77/150 [05:24<07:49,  6.43s/it] 52%|█████▏    | 78/150 [05:27<06:29,  5.42s/it] 53%|█████▎    | 79/150 [05:30<05:34,  4.72s/it] 53%|█████▎    | 80/150 [05:33<04:56,  4.23s/it] 54%|█████▍    | 81/150 [05:36<04:27,  3.88s/it] 55%|█████▍    | 82/150 [05:39<04:06,  3.63s/it] 55%|█████▌    | 83/150 [05:42<03:52,  3.47s/it] 56%|█████▌    | 84/150 [05:45<03:41,  3.35s/it] 57%|█████▋    | 85/150 [05:48<03:32,  3.27s/it] 57%|█████▋    | 86/150 [05:51<03:25,  3.21s/it] 58%|█████▊    | 87/150 [05:55<03:20,  3.18s/it] 59%|█████▊    | 88/150 [05:58<03:15,  3.15s/it] 59%|█████▉    | 89/150 [06:01<03:10,  3.13s/it] 60%|██████    | 90/150 [06:04<03:07,  3.12s/it]                                                 60%|██████    | 90/150 [06:04<03:07,  3.12s/it]int64
{'eval_loss': 1.3442364931106567, 'eval_rouge1': 27.7675, 'eval_rouge2': 12.2153, 'eval_rougeL': 27.9067, 'eval_rougeLsum': 27.9615, 'eval_runtime': 3.5324, 'eval_samples_per_second': 11.324, 'eval_steps_per_second': 0.566, 'epoch': 5.0}
{'loss': 1.4867, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.24it/s][A                                                
                                             [A 60%|██████    | 90/150 [06:07<03:07,  3.12s/it]
100%|██████████| 2/2 [00:01<00:00,  1.24it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 91/150 [06:22<07:33,  7.68s/it] 61%|██████▏   | 92/150 [06:25<06:04,  6.29s/it] 62%|██████▏   | 93/150 [06:28<05:03,  5.32s/it] 63%|██████▎   | 94/150 [06:31<04:20,  4.64s/it] 63%|██████▎   | 95/150 [06:34<03:49,  4.17s/it] 64%|██████▍   | 96/150 [06:37<03:26,  3.83s/it] 65%|██████▍   | 97/150 [06:41<03:11,  3.61s/it] 65%|██████▌   | 98/150 [06:44<03:00,  3.46s/it] 66%|██████▌   | 99/150 [06:47<02:50,  3.34s/it] 67%|██████▋   | 100/150 [06:50<02:42,  3.26s/it] 67%|██████▋   | 101/150 [06:53<02:36,  3.20s/it] 68%|██████▊   | 102/150 [06:56<02:32,  3.17s/it] 69%|██████▊   | 103/150 [06:59<02:27,  3.14s/it] 69%|██████▉   | 104/150 [07:02<02:23,  3.12s/it] 70%|███████   | 105/150 [07:05<02:20,  3.11s/it]                                                  70%|███████   | 105/150 [07:05<02:20,  3.11s/it]int64
{'eval_loss': 0.6906770467758179, 'eval_rouge1': 23.2229, 'eval_rouge2': 11.3831, 'eval_rougeL': 23.5283, 'eval_rougeLsum': 23.4358, 'eval_runtime': 3.3909, 'eval_samples_per_second': 11.796, 'eval_steps_per_second': 0.59, 'epoch': 6.0}
{'loss': 0.7273, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.25it/s][A                                                 
                                             [A 70%|███████   | 105/150 [07:09<02:20,  3.11s/it]
100%|██████████| 2/2 [00:01<00:00,  1.25it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 106/150 [07:24<05:41,  7.77s/it] 71%|███████▏  | 107/150 [07:27<04:33,  6.36s/it] 72%|███████▏  | 108/150 [07:30<03:45,  5.38s/it] 73%|███████▎  | 109/150 [07:33<03:12,  4.69s/it] 73%|███████▎  | 110/150 [07:36<02:48,  4.20s/it] 74%|███████▍  | 111/150 [07:39<02:30,  3.87s/it] 75%|███████▍  | 112/150 [07:42<02:18,  3.63s/it] 75%|███████▌  | 113/150 [07:45<02:07,  3.46s/it] 76%|███████▌  | 114/150 [07:48<02:00,  3.35s/it] 77%|███████▋  | 115/150 [07:52<01:54,  3.28s/it] 77%|███████▋  | 116/150 [07:55<01:49,  3.22s/it] 78%|███████▊  | 117/150 [07:58<01:44,  3.18s/it] 79%|███████▊  | 118/150 [08:01<01:40,  3.14s/it] 79%|███████▉  | 119/150 [08:04<01:36,  3.13s/it] 80%|████████  | 120/150 [08:07<01:33,  3.11s/it]                                                  80%|████████  | 120/150 [08:07<01:33,  3.11s/it]int64
{'eval_loss': 0.20734067261219025, 'eval_rouge1': 50.5556, 'eval_rouge2': 38.0605, 'eval_rougeL': 50.5196, 'eval_rougeLsum': 50.3296, 'eval_runtime': 3.3738, 'eval_samples_per_second': 11.856, 'eval_steps_per_second': 0.593, 'epoch': 7.0}
{'loss': 0.3629, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.23it/s][A                                                 
                                             [A 80%|████████  | 120/150 [08:10<01:33,  3.11s/it]
100%|██████████| 2/2 [00:01<00:00,  1.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 121/150 [08:26<03:46,  7.80s/it] 81%|████████▏ | 122/150 [08:29<02:58,  6.39s/it] 82%|████████▏ | 123/150 [08:32<02:25,  5.39s/it] 83%|████████▎ | 124/150 [08:35<02:01,  4.69s/it] 83%|████████▎ | 125/150 [08:38<01:45,  4.21s/it] 84%|████████▍ | 126/150 [08:41<01:33,  3.88s/it] 85%|████████▍ | 127/150 [08:44<01:23,  3.64s/it] 85%|████████▌ | 128/150 [08:47<01:16,  3.46s/it] 86%|████████▌ | 129/150 [08:50<01:10,  3.34s/it] 87%|████████▋ | 130/150 [08:53<01:05,  3.27s/it] 87%|████████▋ | 131/150 [08:56<01:00,  3.21s/it] 88%|████████▊ | 132/150 [08:59<00:56,  3.16s/it] 89%|████████▊ | 133/150 [09:03<00:53,  3.13s/it] 89%|████████▉ | 134/150 [09:06<00:49,  3.11s/it] 90%|█████████ | 135/150 [09:09<00:46,  3.11s/it]                                                  90%|█████████ | 135/150 [09:09<00:46,  3.11s/it]int64
{'eval_loss': 0.038038115948438644, 'eval_rouge1': 64.3651, 'eval_rouge2': 54.9524, 'eval_rougeL': 64.246, 'eval_rougeLsum': 64.3651, 'eval_runtime': 3.3986, 'eval_samples_per_second': 11.77, 'eval_steps_per_second': 0.588, 'epoch': 8.0}
{'loss': 0.2759, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.25it/s][A                                                 
                                             [A 90%|█████████ | 135/150 [09:12<00:46,  3.11s/it]
100%|██████████| 2/2 [00:01<00:00,  1.25it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 136/150 [09:27<01:48,  7.72s/it] 91%|█████████▏| 137/150 [09:30<01:22,  6.33s/it] 92%|█████████▏| 138/150 [09:33<01:04,  5.35s/it] 93%|█████████▎| 139/150 [09:37<00:51,  4.72s/it] 93%|█████████▎| 140/150 [09:40<00:42,  4.23s/it] 94%|█████████▍| 141/150 [09:43<00:34,  3.88s/it] 95%|█████████▍| 142/150 [09:46<00:29,  3.65s/it] 95%|█████████▌| 143/150 [09:49<00:24,  3.48s/it] 96%|█████████▌| 144/150 [09:52<00:20,  3.36s/it] 97%|█████████▋| 145/150 [09:55<00:16,  3.26s/it] 97%|█████████▋| 146/150 [09:58<00:12,  3.20s/it] 98%|█████████▊| 147/150 [10:01<00:09,  3.17s/it] 99%|█████████▊| 148/150 [10:04<00:06,  3.20s/it] 99%|█████████▉| 149/150 [10:08<00:03,  3.17s/it]100%|██████████| 150/150 [10:11<00:00,  3.14s/it]                                                 100%|██████████| 150/150 [10:11<00:00,  3.14s/it]int64
{'eval_loss': 0.02048189379274845, 'eval_rouge1': 65.6746, 'eval_rouge2': 56.2381, 'eval_rougeL': 65.5556, 'eval_rougeLsum': 65.6151, 'eval_runtime': 3.4054, 'eval_samples_per_second': 11.746, 'eval_steps_per_second': 0.587, 'epoch': 9.0}
{'loss': 0.2308, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.24it/s][A                                                 
                                             [A100%|██████████| 150/150 [10:14<00:00,  3.14s/it]
100%|██████████| 2/2 [00:01<00:00,  1.24it/s][A
                                             [A                                                 100%|██████████| 150/150 [10:29<00:00,  3.14s/it]100%|██████████| 150/150 [10:29<00:00,  4.20s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.018855566158890724, 'eval_rouge1': 65.6746, 'eval_rouge2': 56.2381, 'eval_rougeL': 65.5556, 'eval_rougeLsum': 65.6151, 'eval_runtime': 3.4121, 'eval_samples_per_second': 11.723, 'eval_steps_per_second': 0.586, 'epoch': 10.0}
{'train_runtime': 637.7041, 'train_samples_per_second': 5.645, 'train_steps_per_second': 0.235, 'train_loss': 6.258481740951538, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▅▃▃▂▁▁▁▁▁
wandb:                    eval/rouge1 ▁▂▄▃▄▃▆███
wandb:                    eval/rouge2 ▁▁▂▂▃▂▆███
wandb:                    eval/rougeL ▁▂▄▃▄▄▆███
wandb:                 eval/rougeLsum ▁▂▄▃▄▄▆███
wandb:                   eval/runtime ▁███▇▅▄▅▅▅
wandb:        eval/samples_per_second █▁▁▁▂▄▄▄▄▄
wandb:          eval/steps_per_second █▁▁▁▂▄▄▄▄▄
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▅▃▂▂▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01886
wandb:                    eval/rouge1 65.6746
wandb:                    eval/rouge2 56.2381
wandb:                    eval/rougeL 65.5556
wandb:                 eval/rougeLsum 65.6151
wandb:                   eval/runtime 3.4121
wandb:        eval/samples_per_second 11.723
wandb:          eval/steps_per_second 0.586
wandb:                    train/epoch 10.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2308
wandb:               train/total_flos 101169517363200.0
wandb:               train/train_loss 6.25848
wandb:            train/train_runtime 637.7041
wandb: train/train_samples_per_second 5.645
wandb:   train/train_steps_per_second 0.235
wandb: 
wandb: 🚀 View run mt5-base_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/2bsvr4au
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_102054-2bsvr4au/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/360 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/110 [00:00<?, ? examples/s]                                                   /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_103233-gy0nzzs3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mt5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/gy0nzzs3
################################################ mt5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 ################################################
  0%|          | 0/150 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/150 [00:17<43:42, 17.60s/it]Traceback (most recent call last):
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 272, in <module>
    main(
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 252, in main
    my_instance.train()
  File "/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py", line 160, in train
    trainer.train()
  File "/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/trainer.py", line 1809, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/trainer.py", line 2665, in training_step
    self.accelerator.backward(loss)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/accelerate/accelerator.py", line 1821, in backward
    loss.backward(**kwargs)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 143, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 95, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 23.70 GiB total capacity; 22.34 GiB already allocated; 7.69 MiB free; 22.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 🚀 View run mt5-large_ep-10_ga-1_b-4_lr-0.0003_wd-0.01 at: https://wandb.ai/ma-thesis/ELLIPSIS_models/runs/gy0nzzs3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_103233-gy0nzzs3/logs
