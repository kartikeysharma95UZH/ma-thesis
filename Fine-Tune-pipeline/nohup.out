/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_202634-rsu2gilz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-base_ep-10_ga-1_b-4_lr-0.0001
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/rsu2gilz
################################################ flan-t5-base_ep-10_ga-1_b-4_lr-0.0001 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:13<32:03, 13.84s/it]  1%|▏         | 2/140 [00:15<15:30,  6.74s/it]  2%|▏         | 3/140 [00:16<09:47,  4.29s/it]  3%|▎         | 4/140 [00:18<07:07,  3.14s/it]  4%|▎         | 5/140 [00:19<05:40,  2.52s/it]  4%|▍         | 6/140 [00:21<04:48,  2.15s/it]  5%|▌         | 7/140 [00:22<04:12,  1.90s/it]  6%|▌         | 8/140 [00:23<03:49,  1.74s/it]  6%|▋         | 9/140 [00:25<03:33,  1.63s/it]  7%|▋         | 10/140 [00:26<03:22,  1.56s/it]  8%|▊         | 11/140 [00:28<03:15,  1.51s/it]  9%|▊         | 12/140 [00:29<03:10,  1.49s/it]  9%|▉         | 13/140 [00:31<03:06,  1.47s/it] 10%|█         | 14/140 [00:32<03:02,  1.45s/it]                                                 10%|█         | 14/140 [00:32<03:02,  1.45s/it]{'loss': 17.4439, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.51it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:00<00:00,  2.51it/s][A 10%|█         | 14/140 [00:34<03:02,  1.45s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [00:40<07:04,  3.40s/it] 11%|█▏        | 16/140 [00:41<05:53,  2.85s/it] 12%|█▏        | 17/140 [00:43<04:56,  2.41s/it] 13%|█▎        | 18/140 [00:44<04:17,  2.11s/it] 14%|█▎        | 19/140 [00:46<03:50,  1.91s/it] 14%|█▍        | 20/140 [00:47<03:31,  1.76s/it] 15%|█▌        | 21/140 [00:49<03:17,  1.66s/it] 16%|█▌        | 22/140 [00:50<03:07,  1.59s/it] 16%|█▋        | 23/140 [00:51<03:00,  1.54s/it] 17%|█▋        | 24/140 [00:53<02:54,  1.51s/it] 18%|█▊        | 25/140 [00:54<02:50,  1.48s/it] 19%|█▊        | 26/140 [00:56<02:47,  1.47s/it] 19%|█▉        | 27/140 [00:57<02:43,  1.45s/it] 20%|██        | 28/140 [00:58<02:40,  1.43s/it]                                                 20%|██        | 28/140 [00:58<02:40,  1.43s/it]int64
{'eval_loss': 5.121242523193359, 'eval_rouge1': 41.5783, 'eval_rouge2': 34.0625, 'eval_rougeL': 41.7821, 'eval_rougeLsum': 41.9363, 'eval_runtime': 1.8827, 'eval_samples_per_second': 19.121, 'eval_steps_per_second': 1.062, 'epoch': 1.0}
{'loss': 4.5845, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.49it/s][A                                                
                                             [A 20%|██        | 28/140 [01:00<02:40,  1.43s/it]
100%|██████████| 2/2 [00:00<00:00,  2.49it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [01:31<19:59, 10.81s/it] 21%|██▏       | 30/140 [01:33<14:39,  8.00s/it] 22%|██▏       | 31/140 [01:34<10:56,  6.02s/it] 23%|██▎       | 32/140 [01:35<08:21,  4.65s/it] 24%|██▎       | 33/140 [01:37<06:34,  3.68s/it] 24%|██▍       | 34/140 [01:38<05:18,  3.01s/it] 25%|██▌       | 35/140 [01:40<04:25,  2.53s/it] 26%|██▌       | 36/140 [01:41<03:49,  2.20s/it] 26%|██▋       | 37/140 [01:43<03:28,  2.02s/it] 27%|██▋       | 38/140 [01:55<08:42,  5.12s/it] 28%|██▊       | 39/140 [01:57<06:45,  4.01s/it] 29%|██▊       | 40/140 [02:05<08:46,  5.26s/it] 29%|██▉       | 41/140 [02:15<10:56,  6.63s/it] 30%|███       | 42/140 [02:16<08:17,  5.07s/it]                                                 30%|███       | 42/140 [02:16<08:17,  5.07s/it]int64
{'eval_loss': 2.297757625579834, 'eval_rouge1': 65.975, 'eval_rouge2': 59.1554, 'eval_rougeL': 66.1037, 'eval_rougeLsum': 65.9953, 'eval_runtime': 1.7248, 'eval_samples_per_second': 20.873, 'eval_steps_per_second': 1.16, 'epoch': 2.0}
{'loss': 2.7973, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.50it/s][A                                                
                                             [A 30%|███       | 42/140 [02:18<08:17,  5.07s/it]
100%|██████████| 2/2 [00:00<00:00,  2.50it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [02:24<09:34,  5.92s/it] 31%|███▏      | 44/140 [02:25<07:18,  4.57s/it] 32%|███▏      | 45/140 [02:27<05:44,  3.62s/it] 33%|███▎      | 46/140 [02:28<04:38,  2.97s/it] 34%|███▎      | 47/140 [02:30<03:52,  2.50s/it] 34%|███▍      | 48/140 [02:31<03:20,  2.18s/it] 35%|███▌      | 49/140 [02:32<02:57,  1.95s/it] 36%|███▌      | 50/140 [02:34<02:41,  1.80s/it] 36%|███▋      | 51/140 [02:35<02:29,  1.69s/it] 37%|███▋      | 52/140 [02:37<02:21,  1.61s/it] 38%|███▊      | 53/140 [02:38<02:15,  1.55s/it] 39%|███▊      | 54/140 [02:40<02:10,  1.52s/it] 39%|███▉      | 55/140 [02:41<02:06,  1.49s/it] 40%|████      | 56/140 [02:42<02:03,  1.47s/it]                                                 40%|████      | 56/140 [02:42<02:03,  1.47s/it]int64
{'eval_loss': 1.567185640335083, 'eval_rouge1': 70.1614, 'eval_rouge2': 64.7854, 'eval_rougeL': 70.2723, 'eval_rougeLsum': 70.188, 'eval_runtime': 1.7129, 'eval_samples_per_second': 21.016, 'eval_steps_per_second': 1.168, 'epoch': 3.0}
{'loss': 1.7444, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.59it/s][A                                                
                                             [A 40%|████      | 56/140 [02:44<02:03,  1.47s/it]
100%|██████████| 2/2 [00:00<00:00,  2.59it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [03:00<08:51,  6.41s/it] 41%|████▏     | 58/140 [03:02<06:43,  4.92s/it] 42%|████▏     | 59/140 [03:03<05:13,  3.87s/it] 43%|████▎     | 60/140 [03:05<04:10,  3.14s/it] 44%|████▎     | 61/140 [03:06<03:27,  2.62s/it] 44%|████▍     | 62/140 [03:07<02:56,  2.27s/it] 45%|████▌     | 63/140 [03:09<02:34,  2.01s/it] 46%|████▌     | 64/140 [03:10<02:19,  1.84s/it] 46%|████▋     | 65/140 [03:12<02:08,  1.71s/it] 47%|████▋     | 66/140 [03:13<02:00,  1.63s/it] 48%|████▊     | 67/140 [03:15<01:54,  1.57s/it] 49%|████▊     | 68/140 [03:16<01:49,  1.52s/it] 49%|████▉     | 69/140 [03:17<01:45,  1.49s/it] 50%|█████     | 70/140 [03:19<01:42,  1.47s/it]                                                 50%|█████     | 70/140 [03:19<01:42,  1.47s/it]int64
{'eval_loss': 0.5717971324920654, 'eval_rouge1': 69.4164, 'eval_rouge2': 63.8624, 'eval_rougeL': 69.4756, 'eval_rougeLsum': 69.3573, 'eval_runtime': 1.6895, 'eval_samples_per_second': 21.308, 'eval_steps_per_second': 1.184, 'epoch': 4.0}
{'loss': 0.9998, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.54it/s][A                                                
                                             [A 50%|█████     | 70/140 [03:21<01:42,  1.47s/it]
100%|██████████| 2/2 [00:00<00:00,  2.54it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [03:27<03:59,  3.47s/it] 51%|█████▏    | 72/140 [03:28<03:14,  2.86s/it] 52%|█████▏    | 73/140 [03:30<02:42,  2.43s/it] 53%|█████▎    | 74/140 [03:31<02:20,  2.13s/it] 54%|█████▎    | 75/140 [03:33<02:04,  1.91s/it] 54%|█████▍    | 76/140 [03:34<01:52,  1.76s/it] 55%|█████▌    | 77/140 [03:35<01:44,  1.65s/it] 56%|█████▌    | 78/140 [03:42<03:12,  3.11s/it] 56%|█████▋    | 79/140 [03:43<02:38,  2.60s/it] 57%|█████▋    | 80/140 [03:45<02:14,  2.25s/it] 58%|█████▊    | 81/140 [03:46<01:57,  2.00s/it] 59%|█████▊    | 82/140 [03:48<01:45,  1.83s/it] 59%|█████▉    | 83/140 [03:49<01:36,  1.70s/it] 60%|██████    | 84/140 [03:50<01:30,  1.61s/it]                                                 60%|██████    | 84/140 [03:50<01:30,  1.61s/it]int64
{'eval_loss': 0.3032059967517853, 'eval_rouge1': 70.1945, 'eval_rouge2': 65.0132, 'eval_rougeL': 70.1712, 'eval_rougeLsum': 70.1618, 'eval_runtime': 1.7183, 'eval_samples_per_second': 20.951, 'eval_steps_per_second': 1.164, 'epoch': 5.0}
{'loss': 0.5633, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.72it/s][A                                                
                                             [A 60%|██████    | 84/140 [03:52<01:30,  1.61s/it]
100%|██████████| 2/2 [00:00<00:00,  2.72it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [03:58<03:12,  3.51s/it] 61%|██████▏   | 86/140 [04:00<02:35,  2.88s/it] 62%|██████▏   | 87/140 [04:01<02:09,  2.44s/it] 63%|██████▎   | 88/140 [04:03<01:51,  2.14s/it] 64%|██████▎   | 89/140 [04:04<01:38,  1.92s/it] 64%|██████▍   | 90/140 [04:06<01:28,  1.78s/it] 65%|██████▌   | 91/140 [04:07<01:21,  1.67s/it] 66%|██████▌   | 92/140 [04:08<01:16,  1.60s/it] 66%|██████▋   | 93/140 [04:10<01:12,  1.55s/it] 67%|██████▋   | 94/140 [04:11<01:09,  1.52s/it] 68%|██████▊   | 95/140 [04:13<01:07,  1.50s/it] 69%|██████▊   | 96/140 [04:14<01:04,  1.48s/it] 69%|██████▉   | 97/140 [04:16<01:02,  1.46s/it] 70%|███████   | 98/140 [04:17<01:00,  1.45s/it]                                                 70%|███████   | 98/140 [04:17<01:00,  1.45s/it]int64
{'eval_loss': 0.20140522718429565, 'eval_rouge1': 70.1945, 'eval_rouge2': 65.0132, 'eval_rougeL': 70.1712, 'eval_rougeLsum': 70.1618, 'eval_runtime': 1.6115, 'eval_samples_per_second': 22.339, 'eval_steps_per_second': 1.241, 'epoch': 6.0}
{'loss': 0.3292, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.57it/s][A                                                
                                             [A 70%|███████   | 98/140 [04:19<01:00,  1.45s/it]
100%|██████████| 2/2 [00:00<00:00,  2.57it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [04:25<02:19,  3.41s/it] 71%|███████▏  | 100/140 [04:26<01:52,  2.81s/it] 72%|███████▏  | 101/140 [04:28<01:33,  2.40s/it] 73%|███████▎  | 102/140 [04:29<01:20,  2.11s/it] 74%|███████▎  | 103/140 [04:31<01:10,  1.90s/it] 74%|███████▍  | 104/140 [04:32<01:03,  1.76s/it] 75%|███████▌  | 105/140 [04:34<00:58,  1.66s/it] 76%|███████▌  | 106/140 [04:35<00:54,  1.59s/it] 76%|███████▋  | 107/140 [04:36<00:50,  1.54s/it] 77%|███████▋  | 108/140 [04:38<00:48,  1.51s/it] 78%|███████▊  | 109/140 [04:39<00:46,  1.49s/it] 79%|███████▊  | 110/140 [04:41<00:43,  1.47s/it] 79%|███████▉  | 111/140 [04:42<00:42,  1.45s/it] 80%|████████  | 112/140 [04:44<00:40,  1.45s/it]                                                  80%|████████  | 112/140 [04:44<00:40,  1.45s/it]int64
{'eval_loss': 0.14316031336784363, 'eval_rouge1': 70.1945, 'eval_rouge2': 65.0132, 'eval_rougeL': 70.1712, 'eval_rougeLsum': 70.1618, 'eval_runtime': 1.6982, 'eval_samples_per_second': 21.198, 'eval_steps_per_second': 1.178, 'epoch': 7.0}
{'loss': 0.2128, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A                                                 
                                             [A 80%|████████  | 112/140 [04:45<00:40,  1.45s/it]
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [05:02<02:58,  6.61s/it] 81%|████████▏ | 114/140 [05:04<02:11,  5.06s/it] 82%|████████▏ | 115/140 [05:05<01:39,  3.97s/it] 83%|████████▎ | 116/140 [05:06<01:16,  3.20s/it] 84%|████████▎ | 117/140 [05:08<01:01,  2.67s/it] 84%|████████▍ | 118/140 [05:09<00:50,  2.30s/it] 85%|████████▌ | 119/140 [05:11<00:42,  2.04s/it] 86%|████████▌ | 120/140 [05:12<00:37,  1.86s/it] 86%|████████▋ | 121/140 [05:14<00:32,  1.73s/it] 87%|████████▋ | 122/140 [05:15<00:29,  1.64s/it] 88%|████████▊ | 123/140 [05:17<00:26,  1.58s/it] 89%|████████▊ | 124/140 [05:18<00:24,  1.54s/it] 89%|████████▉ | 125/140 [05:19<00:22,  1.50s/it] 90%|█████████ | 126/140 [05:21<00:20,  1.48s/it]                                                  90%|█████████ | 126/140 [05:21<00:20,  1.48s/it]int64
{'eval_loss': 0.096552275121212, 'eval_rouge1': 70.7684, 'eval_rouge2': 65.1013, 'eval_rougeL': 70.7765, 'eval_rougeLsum': 70.7466, 'eval_runtime': 1.6952, 'eval_samples_per_second': 21.237, 'eval_steps_per_second': 1.18, 'epoch': 8.0}
{'loss': 0.1476, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A                                                 
                                             [A 90%|█████████ | 126/140 [05:23<00:20,  1.48s/it]
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [05:37<01:17,  5.96s/it] 91%|█████████▏| 128/140 [05:39<00:55,  4.60s/it] 92%|█████████▏| 129/140 [05:40<00:40,  3.65s/it] 93%|█████████▎| 130/140 [05:41<00:29,  2.98s/it] 94%|█████████▎| 131/140 [05:43<00:22,  2.52s/it] 94%|█████████▍| 132/140 [05:44<00:17,  2.19s/it] 95%|█████████▌| 133/140 [05:46<00:13,  1.96s/it] 96%|█████████▌| 134/140 [05:47<00:10,  1.81s/it] 96%|█████████▋| 135/140 [05:49<00:08,  1.69s/it] 97%|█████████▋| 136/140 [05:50<00:06,  1.61s/it] 98%|█████████▊| 137/140 [05:51<00:04,  1.55s/it] 99%|█████████▊| 138/140 [05:53<00:03,  1.52s/it] 99%|█████████▉| 139/140 [05:54<00:01,  1.49s/it]100%|██████████| 140/140 [05:56<00:00,  1.47s/it]                                                 100%|██████████| 140/140 [05:56<00:00,  1.47s/it]int64
{'eval_loss': 0.08266867697238922, 'eval_rouge1': 70.7684, 'eval_rouge2': 65.1013, 'eval_rougeL': 70.7765, 'eval_rougeLsum': 70.7466, 'eval_runtime': 1.7037, 'eval_samples_per_second': 21.13, 'eval_steps_per_second': 1.174, 'epoch': 9.0}
{'loss': 0.123, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A                                                 
                                             [A100%|██████████| 140/140 [05:57<00:00,  1.47s/it]
100%|██████████| 2/2 [00:00<00:00,  2.58it/s][A
                                             [A                                                 100%|██████████| 140/140 [06:14<00:00,  1.47s/it]100%|██████████| 140/140 [06:14<00:00,  2.67s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.08035293221473694, 'eval_rouge1': 70.7684, 'eval_rouge2': 65.1013, 'eval_rougeL': 70.7765, 'eval_rougeLsum': 70.7466, 'eval_runtime': 1.6888, 'eval_samples_per_second': 21.317, 'eval_steps_per_second': 1.184, 'epoch': 10.0}
{'train_runtime': 382.6513, 'train_samples_per_second': 8.31, 'train_steps_per_second': 0.366, 'train_loss': 2.8945711536066874, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▄▃▂▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▇████████
wandb:                    eval/rouge2 ▁▇████████
wandb:                    eval/rougeL ▁▇████████
wandb:                 eval/rougeLsum ▁▇████████
wandb:                   eval/runtime █▄▄▃▄▁▃▃▃▃
wandb:        eval/samples_per_second ▁▅▅▆▅█▆▆▅▆
wandb:          eval/steps_per_second ▁▅▅▆▅█▆▆▅▆
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▃▂▂▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.08035
wandb:                    eval/rouge1 70.7684
wandb:                    eval/rouge2 65.1013
wandb:                    eval/rougeL 70.7765
wandb:                 eval/rougeLsum 70.7466
wandb:                   eval/runtime 1.6888
wandb:        eval/samples_per_second 21.317
wandb:          eval/steps_per_second 1.184
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.123
wandb:               train/total_flos 68047761899520.0
wandb:               train/train_loss 2.89457
wandb:            train/train_runtime 382.6513
wandb: train/train_samples_per_second 8.31
wandb:   train/train_steps_per_second 0.366
wandb: 
wandb: 🚀 View run flan-t5-base_ep-10_ga-1_b-4_lr-0.0001 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/rsu2gilz
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_202634-rsu2gilz/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_203337-rzm25413
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-base_ep-10_ga-1_b-4_lr-0.0001
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/rzm25413
################################################ t5-base_ep-10_ga-1_b-4_lr-0.0001 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:11<27:38, 11.93s/it]  1%|▏         | 2/140 [00:13<12:57,  5.63s/it]  2%|▏         | 3/140 [00:14<08:15,  3.62s/it]  3%|▎         | 4/140 [00:15<06:03,  2.68s/it]  4%|▎         | 5/140 [00:16<04:50,  2.15s/it]  4%|▍         | 6/140 [00:18<04:05,  1.83s/it]  5%|▌         | 7/140 [00:19<03:38,  1.64s/it]  6%|▌         | 8/140 [00:20<03:19,  1.51s/it]  6%|▋         | 9/140 [00:23<03:58,  1.82s/it]  7%|▋         | 10/140 [00:24<03:34,  1.65s/it]  8%|▊         | 11/140 [00:25<03:16,  1.52s/it]  9%|▊         | 12/140 [00:26<03:03,  1.43s/it]  9%|▉         | 13/140 [00:27<02:54,  1.37s/it] 10%|█         | 14/140 [00:29<02:46,  1.32s/it]                                                 10%|█         | 14/140 [00:29<02:46,  1.32s/it]{'loss': 5.1096, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.45it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:00<00:00,  2.45it/s][A 10%|█         | 14/140 [00:31<02:46,  1.32s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [00:36<06:33,  3.15s/it] 11%|█▏        | 16/140 [00:37<05:19,  2.58s/it] 12%|█▏        | 17/140 [00:39<04:27,  2.17s/it] 13%|█▎        | 18/140 [00:40<03:50,  1.89s/it] 14%|█▎        | 19/140 [00:41<03:24,  1.69s/it] 14%|█▍        | 20/140 [00:42<03:06,  1.56s/it] 15%|█▌        | 21/140 [00:43<02:53,  1.46s/it] 16%|█▌        | 22/140 [00:45<02:44,  1.39s/it] 16%|█▋        | 23/140 [00:46<02:37,  1.35s/it] 17%|█▋        | 24/140 [00:47<02:32,  1.31s/it] 18%|█▊        | 25/140 [00:48<02:28,  1.29s/it] 19%|█▊        | 26/140 [00:50<02:25,  1.28s/it] 19%|█▉        | 27/140 [00:51<02:22,  1.26s/it] 20%|██        | 28/140 [00:52<02:19,  1.25s/it]                                                 20%|██        | 28/140 [00:52<02:19,  1.25s/it]int64
{'eval_loss': 2.2639811038970947, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 1.7944, 'eval_samples_per_second': 20.062, 'eval_steps_per_second': 1.115, 'epoch': 1.0}
{'loss': 1.4894, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.44it/s][A                                                
                                             [A 20%|██        | 28/140 [00:54<02:19,  1.25s/it]
100%|██████████| 2/2 [00:00<00:00,  2.44it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [01:09<11:10,  6.04s/it] 21%|██▏       | 30/140 [01:11<08:25,  4.60s/it] 22%|██▏       | 31/140 [01:12<06:31,  3.59s/it] 23%|██▎       | 32/140 [01:13<05:11,  2.88s/it] 24%|██▎       | 33/140 [01:14<04:16,  2.40s/it] 24%|██▍       | 34/140 [01:16<03:36,  2.04s/it] 25%|██▌       | 35/140 [01:17<03:08,  1.80s/it] 26%|██▌       | 36/140 [01:18<02:49,  1.63s/it] 26%|██▋       | 37/140 [01:19<02:36,  1.52s/it] 27%|██▋       | 38/140 [01:20<02:25,  1.43s/it] 28%|██▊       | 39/140 [01:22<02:18,  1.37s/it] 29%|██▊       | 40/140 [01:23<02:13,  1.33s/it] 29%|██▉       | 41/140 [01:24<02:09,  1.31s/it] 30%|███       | 42/140 [01:25<02:04,  1.27s/it]                                                 30%|███       | 42/140 [01:25<02:04,  1.27s/it]int64
{'eval_loss': 0.7535136342048645, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 1.7905, 'eval_samples_per_second': 20.106, 'eval_steps_per_second': 1.117, 'epoch': 2.0}
{'loss': 0.6952, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.42it/s][A                                                
                                             [A 30%|███       | 42/140 [01:27<02:04,  1.27s/it]
100%|██████████| 2/2 [00:00<00:00,  2.42it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [01:48<12:32,  7.76s/it] 31%|███▏      | 44/140 [01:50<09:17,  5.81s/it] 32%|███▏      | 45/140 [01:51<07:01,  4.44s/it] 33%|███▎      | 46/140 [01:52<05:27,  3.48s/it] 34%|███▎      | 47/140 [01:53<04:20,  2.80s/it] 34%|███▍      | 48/140 [01:54<03:34,  2.33s/it] 35%|███▌      | 49/140 [01:56<03:02,  2.00s/it] 36%|███▌      | 50/140 [01:57<02:39,  1.77s/it] 36%|███▋      | 51/140 [01:58<02:23,  1.61s/it] 37%|███▋      | 52/140 [01:59<02:12,  1.50s/it] 38%|███▊      | 53/140 [02:01<02:04,  1.43s/it] 39%|███▊      | 54/140 [02:02<01:57,  1.37s/it] 39%|███▉      | 55/140 [02:03<01:53,  1.33s/it] 40%|████      | 56/140 [02:04<01:49,  1.30s/it]                                                 40%|████      | 56/140 [02:04<01:49,  1.30s/it]int64
{'eval_loss': 0.3907449543476105, 'eval_rouge1': 1.7094, 'eval_rouge2': 1.5152, 'eval_rougeL': 1.7094, 'eval_rougeLsum': 1.7094, 'eval_runtime': 1.7944, 'eval_samples_per_second': 20.063, 'eval_steps_per_second': 1.115, 'epoch': 3.0}
{'loss': 0.3345, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.47it/s][A                                                
                                             [A 40%|████      | 56/140 [02:06<01:49,  1.30s/it]
100%|██████████| 2/2 [00:00<00:00,  2.47it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [02:31<12:07,  8.76s/it] 41%|████▏     | 58/140 [02:32<08:53,  6.50s/it] 42%|████▏     | 59/140 [02:33<06:39,  4.93s/it] 43%|████▎     | 60/140 [02:34<05:05,  3.82s/it] 44%|████▎     | 61/140 [02:36<04:00,  3.04s/it] 44%|████▍     | 62/140 [02:37<03:15,  2.50s/it] 45%|████▌     | 63/140 [02:38<02:43,  2.13s/it] 46%|████▌     | 64/140 [02:39<02:20,  1.85s/it] 46%|████▋     | 65/140 [02:40<02:05,  1.67s/it] 47%|████▋     | 66/140 [02:42<01:53,  1.54s/it] 48%|████▊     | 67/140 [02:43<01:45,  1.45s/it] 49%|████▊     | 68/140 [02:44<01:39,  1.38s/it] 49%|████▉     | 69/140 [02:45<01:35,  1.34s/it] 50%|█████     | 70/140 [02:47<01:31,  1.30s/it]                                                 50%|█████     | 70/140 [02:47<01:31,  1.30s/it]int64
{'eval_loss': 0.11364451050758362, 'eval_rouge1': 61.9822, 'eval_rouge2': 56.5827, 'eval_rougeL': 62.1222, 'eval_rougeLsum': 62.2513, 'eval_runtime': 1.7811, 'eval_samples_per_second': 20.212, 'eval_steps_per_second': 1.123, 'epoch': 4.0}
{'loss': 0.1645, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A                                                
                                             [A 50%|█████     | 70/140 [02:48<01:31,  1.30s/it]
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [03:08<08:26,  7.35s/it] 51%|█████▏    | 72/140 [03:09<06:15,  5.52s/it] 52%|█████▏    | 73/140 [03:11<04:43,  4.23s/it] 53%|█████▎    | 74/140 [03:12<03:40,  3.34s/it] 54%|█████▎    | 75/140 [03:13<02:55,  2.71s/it] 54%|█████▍    | 76/140 [03:14<02:25,  2.27s/it] 55%|█████▌    | 77/140 [03:16<02:03,  1.96s/it] 56%|█████▌    | 78/140 [03:17<01:48,  1.74s/it] 56%|█████▋    | 79/140 [03:18<01:37,  1.60s/it] 57%|█████▋    | 80/140 [03:19<01:29,  1.49s/it] 58%|█████▊    | 81/140 [03:20<01:23,  1.42s/it] 59%|█████▊    | 82/140 [03:22<01:18,  1.36s/it] 59%|█████▉    | 83/140 [03:23<01:15,  1.32s/it] 60%|██████    | 84/140 [03:24<01:12,  1.29s/it]                                                 60%|██████    | 84/140 [03:24<01:12,  1.29s/it]int64
{'eval_loss': 0.05242757499217987, 'eval_rouge1': 69.8043, 'eval_rouge2': 64.0873, 'eval_rougeL': 69.821, 'eval_rougeLsum': 69.7693, 'eval_runtime': 1.5496, 'eval_samples_per_second': 23.231, 'eval_steps_per_second': 1.291, 'epoch': 5.0}
{'loss': 0.0679, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A                                                
                                             [A 60%|██████    | 84/140 [03:26<01:12,  1.29s/it]
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [03:32<02:51,  3.11s/it] 61%|██████▏   | 86/140 [03:33<02:17,  2.55s/it] 62%|██████▏   | 87/140 [03:34<01:54,  2.16s/it] 63%|██████▎   | 88/140 [03:35<01:37,  1.88s/it] 64%|██████▎   | 89/140 [03:36<01:25,  1.69s/it] 64%|██████▍   | 90/140 [03:38<01:17,  1.55s/it] 65%|██████▌   | 91/140 [03:39<01:11,  1.45s/it] 66%|██████▌   | 92/140 [03:40<01:06,  1.39s/it] 66%|██████▋   | 93/140 [03:41<01:03,  1.34s/it] 67%|██████▋   | 94/140 [03:43<01:00,  1.32s/it] 68%|██████▊   | 95/140 [03:44<00:58,  1.30s/it] 69%|██████▊   | 96/140 [03:45<00:56,  1.28s/it] 69%|██████▉   | 97/140 [03:46<00:54,  1.26s/it] 70%|███████   | 98/140 [03:48<00:52,  1.24s/it]                                                 70%|███████   | 98/140 [03:48<00:52,  1.24s/it]int64
{'eval_loss': 0.0321878045797348, 'eval_rouge1': 70.5515, 'eval_rouge2': 65.3704, 'eval_rougeL': 70.5731, 'eval_rougeLsum': 70.5587, 'eval_runtime': 1.5549, 'eval_samples_per_second': 23.152, 'eval_steps_per_second': 1.286, 'epoch': 6.0}
{'loss': 0.0399, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  3.03it/s][A                                                
                                             [A 70%|███████   | 98/140 [03:51<00:52,  1.24s/it]
100%|██████████| 2/2 [00:02<00:00,  3.03it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [03:56<02:22,  3.47s/it] 71%|███████▏  | 100/140 [03:58<01:52,  2.80s/it] 72%|███████▏  | 101/140 [03:59<01:31,  2.34s/it] 73%|███████▎  | 102/140 [04:00<01:16,  2.00s/it] 74%|███████▎  | 103/140 [04:01<01:05,  1.78s/it] 74%|███████▍  | 104/140 [04:02<00:58,  1.62s/it] 75%|███████▌  | 105/140 [04:04<00:52,  1.50s/it] 76%|███████▌  | 106/140 [04:05<00:48,  1.43s/it] 76%|███████▋  | 107/140 [04:06<00:45,  1.37s/it] 77%|███████▋  | 108/140 [04:07<00:42,  1.33s/it] 78%|███████▊  | 109/140 [04:09<00:40,  1.31s/it] 79%|███████▊  | 110/140 [04:10<00:38,  1.29s/it] 79%|███████▉  | 111/140 [04:11<00:36,  1.28s/it] 80%|████████  | 112/140 [04:13<00:36,  1.31s/it]                                                  80%|████████  | 112/140 [04:13<00:36,  1.31s/it]int64
{'eval_loss': 0.024297017604112625, 'eval_rouge1': 70.5515, 'eval_rouge2': 65.3704, 'eval_rougeL': 70.5731, 'eval_rougeLsum': 70.5587, 'eval_runtime': 3.1177, 'eval_samples_per_second': 11.547, 'eval_steps_per_second': 0.641, 'epoch': 7.0}
{'loss': 0.0303, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.85it/s][A                                                 
                                             [A 80%|████████  | 112/140 [04:14<00:36,  1.31s/it]
100%|██████████| 2/2 [00:00<00:00,  2.85it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [04:33<03:12,  7.12s/it] 81%|████████▏ | 114/140 [04:34<02:19,  5.36s/it] 82%|████████▏ | 115/140 [04:36<01:43,  4.12s/it] 83%|████████▎ | 116/140 [04:37<01:18,  3.26s/it] 84%|████████▎ | 117/140 [04:38<01:00,  2.65s/it] 84%|████████▍ | 118/140 [04:39<00:48,  2.23s/it] 85%|████████▌ | 119/140 [04:41<00:40,  1.93s/it] 86%|████████▌ | 120/140 [04:42<00:34,  1.72s/it] 86%|████████▋ | 121/140 [04:43<00:29,  1.58s/it] 87%|████████▋ | 122/140 [04:44<00:26,  1.48s/it] 88%|████████▊ | 123/140 [04:46<00:23,  1.40s/it] 89%|████████▊ | 124/140 [04:47<00:21,  1.35s/it] 89%|████████▉ | 125/140 [04:48<00:19,  1.32s/it] 90%|█████████ | 126/140 [04:49<00:18,  1.30s/it]                                                  90%|█████████ | 126/140 [04:49<00:18,  1.30s/it]int64
{'eval_loss': 0.02331959269940853, 'eval_rouge1': 70.5515, 'eval_rouge2': 65.3704, 'eval_rougeL': 70.5731, 'eval_rougeLsum': 70.5587, 'eval_runtime': 1.565, 'eval_samples_per_second': 23.003, 'eval_steps_per_second': 1.278, 'epoch': 8.0}
{'loss': 0.0271, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A                                                 
                                             [A 90%|█████████ | 126/140 [04:51<00:18,  1.30s/it]
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [04:56<00:38,  2.97s/it] 91%|█████████▏| 128/140 [04:57<00:29,  2.45s/it] 92%|█████████▏| 129/140 [04:59<00:22,  2.09s/it] 93%|█████████▎| 130/140 [05:00<00:18,  1.83s/it] 94%|█████████▎| 131/140 [05:01<00:14,  1.65s/it] 94%|█████████▍| 132/140 [05:02<00:12,  1.52s/it] 95%|█████████▌| 133/140 [05:04<00:10,  1.44s/it] 96%|█████████▌| 134/140 [05:05<00:08,  1.38s/it] 96%|█████████▋| 135/140 [05:06<00:06,  1.34s/it] 97%|█████████▋| 136/140 [05:15<00:14,  3.73s/it] 98%|█████████▊| 137/140 [05:17<00:08,  2.98s/it] 99%|█████████▊| 138/140 [05:18<00:04,  2.46s/it] 99%|█████████▉| 139/140 [05:19<00:02,  2.09s/it]100%|██████████| 140/140 [05:20<00:00,  1.84s/it]                                                 100%|██████████| 140/140 [05:20<00:00,  1.84s/it]int64
{'eval_loss': 0.02345079928636551, 'eval_rouge1': 70.5515, 'eval_rouge2': 65.3704, 'eval_rougeL': 70.5731, 'eval_rougeLsum': 70.5587, 'eval_runtime': 1.5618, 'eval_samples_per_second': 23.05, 'eval_steps_per_second': 1.281, 'epoch': 9.0}
{'loss': 0.0237, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.85it/s][A                                                 
                                             [A100%|██████████| 140/140 [05:22<00:00,  1.84s/it]
100%|██████████| 2/2 [00:00<00:00,  2.85it/s][A
                                             [A                                                 100%|██████████| 140/140 [05:28<00:00,  1.84s/it]100%|██████████| 140/140 [05:28<00:00,  2.35s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.022841045632958412, 'eval_rouge1': 70.6742, 'eval_rouge2': 65.056, 'eval_rougeL': 70.4836, 'eval_rougeLsum': 70.4577, 'eval_runtime': 1.6181, 'eval_samples_per_second': 22.248, 'eval_steps_per_second': 1.236, 'epoch': 10.0}
{'train_runtime': 337.0268, 'train_samples_per_second': 9.435, 'train_steps_per_second': 0.415, 'train_loss': 0.7982150912284851, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▃▂▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▁▁▇██████
wandb:                    eval/rouge2 ▁▁▁▇██████
wandb:                    eval/rougeL ▁▁▁▇██████
wandb:                 eval/rougeLsum ▁▁▁▇██████
wandb:                   eval/runtime ▂▂▂▂▁▁█▁▁▁
wandb:        eval/samples_per_second ▆▆▆▆██▁██▇
wandb:          eval/steps_per_second ▆▆▆▆██▁██▇
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▃▂▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.02284
wandb:                    eval/rouge1 70.6742
wandb:                    eval/rouge2 65.056
wandb:                    eval/rougeL 70.4836
wandb:                 eval/rougeLsum 70.4577
wandb:                   eval/runtime 1.6181
wandb:        eval/samples_per_second 22.248
wandb:          eval/steps_per_second 1.236
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0237
wandb:               train/total_flos 60515190374400.0
wandb:               train/train_loss 0.79822
wandb:            train/train_runtime 337.0268
wandb: train/train_samples_per_second 9.435
wandb:   train/train_steps_per_second 0.415
wandb: 
wandb: 🚀 View run t5-base_ep-10_ga-1_b-4_lr-0.0001 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/rzm25413
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_203337-rzm25413/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_204006-yi6dwwhy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-large_ep-10_ga-1_b-4_lr-0.0001
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/yi6dwwhy
################################################ flan-t5-large_ep-10_ga-1_b-4_lr-0.0001 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:14<33:38, 14.52s/it]  1%|▏         | 2/140 [00:18<19:26,  8.45s/it]  2%|▏         | 3/140 [00:22<14:51,  6.51s/it]  3%|▎         | 4/140 [00:27<12:43,  5.61s/it]  4%|▎         | 5/140 [00:31<11:29,  5.11s/it]  4%|▍         | 6/140 [00:35<10:43,  4.80s/it]  5%|▌         | 7/140 [00:43<12:45,  5.76s/it]  6%|▌         | 8/140 [00:47<11:37,  5.29s/it]  6%|▋         | 9/140 [00:51<10:49,  4.96s/it]  7%|▋         | 10/140 [00:56<10:13,  4.72s/it]  8%|▊         | 11/140 [01:00<09:50,  4.58s/it]  9%|▊         | 12/140 [01:04<09:34,  4.48s/it]  9%|▉         | 13/140 [01:08<09:20,  4.41s/it] 10%|█         | 14/140 [01:12<09:06,  4.34s/it]                                                 10%|█         | 14/140 [01:12<09:06,  4.34s/it]{'loss': 16.5941, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.21s/it][A
                                             [A                                                
100%|██████████| 2/2 [00:02<00:00,  1.21s/it][A 10%|█         | 14/140 [01:21<09:06,  4.34s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [02:13<44:05, 21.17s/it] 11%|█▏        | 16/140 [02:17<33:13, 16.07s/it] 12%|█▏        | 17/140 [02:21<25:36, 12.49s/it] 13%|█▎        | 18/140 [02:25<20:22, 10.02s/it] 14%|█▎        | 19/140 [02:29<16:39,  8.26s/it] 14%|█▍        | 20/140 [02:34<14:04,  7.04s/it] 15%|█▌        | 21/140 [02:38<12:17,  6.20s/it] 16%|█▌        | 22/140 [02:42<11:01,  5.61s/it] 16%|█▋        | 23/140 [02:46<10:08,  5.20s/it] 17%|█▋        | 24/140 [02:56<12:26,  6.43s/it] 18%|█▊        | 25/140 [03:00<11:03,  5.77s/it] 19%|█▊        | 26/140 [03:04<10:05,  5.31s/it] 19%|█▉        | 27/140 [03:08<09:22,  4.98s/it] 20%|██        | 28/140 [03:13<08:58,  4.81s/it]                                                 20%|██        | 28/140 [03:13<08:58,  4.81s/it]int64
{'eval_loss': 7.092781066894531, 'eval_rouge1': 47.1873, 'eval_rouge2': 35.1409, 'eval_rougeL': 47.2758, 'eval_rougeLsum': 46.9887, 'eval_runtime': 8.6045, 'eval_samples_per_second': 4.184, 'eval_steps_per_second': 0.232, 'epoch': 1.0}
{'loss': 4.9261, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.14s/it][A                                                
                                             [A 20%|██        | 28/140 [03:18<08:58,  4.81s/it]
100%|██████████| 2/2 [00:02<00:00,  1.14s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [04:47<58:23, 31.56s/it] 21%|██▏       | 30/140 [04:51<42:51, 23.38s/it] 22%|██▏       | 31/140 [05:06<37:45, 20.79s/it] 23%|██▎       | 32/140 [05:10<28:29, 15.83s/it] 24%|██▎       | 33/140 [05:14<22:00, 12.34s/it] 24%|██▍       | 34/140 [05:18<17:30,  9.91s/it] 25%|██▌       | 35/140 [05:37<22:00, 12.57s/it] 26%|██▌       | 36/140 [05:54<23:58, 13.83s/it] 26%|██▋       | 37/140 [05:58<18:48, 10.96s/it] 27%|██▋       | 38/140 [06:02<15:10,  8.92s/it] 28%|██▊       | 39/140 [06:07<12:38,  7.51s/it] 29%|██▊       | 40/140 [06:11<10:53,  6.54s/it] 29%|██▉       | 41/140 [06:15<09:37,  5.83s/it] 30%|███       | 42/140 [06:19<08:45,  5.36s/it]                                                 30%|███       | 42/140 [06:19<08:45,  5.36s/it]int64
{'eval_loss': 2.1930220127105713, 'eval_rouge1': 59.6434, 'eval_rouge2': 49.0002, 'eval_rougeL': 59.4411, 'eval_rougeLsum': 59.3537, 'eval_runtime': 5.0205, 'eval_samples_per_second': 7.171, 'eval_steps_per_second': 0.398, 'epoch': 2.0}
{'loss': 2.0594, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.19s/it][A                                                
                                             [A 30%|███       | 42/140 [06:33<08:45,  5.36s/it]
100%|██████████| 2/2 [00:02<00:00,  1.19s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [07:59<54:37, 33.79s/it] 31%|███▏      | 44/140 [08:04<39:51, 24.91s/it] 32%|███▏      | 45/140 [08:08<29:36, 18.70s/it] 33%|███▎      | 46/140 [08:12<22:39, 14.46s/it] 34%|███▎      | 47/140 [08:17<17:37, 11.37s/it] 34%|███▍      | 48/140 [08:21<14:09,  9.24s/it] 35%|███▌      | 49/140 [08:25<11:44,  7.74s/it] 36%|███▌      | 50/140 [08:29<10:02,  6.69s/it] 36%|███▋      | 51/140 [08:34<08:48,  5.94s/it] 37%|███▋      | 52/140 [08:38<07:57,  5.42s/it] 38%|███▊      | 53/140 [08:51<11:23,  7.85s/it] 39%|███▊      | 54/140 [08:55<09:41,  6.76s/it] 39%|███▉      | 55/140 [09:00<08:30,  6.00s/it] 40%|████      | 56/140 [09:04<07:39,  5.47s/it]                                                 40%|████      | 56/140 [09:04<07:39,  5.47s/it]int64
{'eval_loss': 0.3436025381088257, 'eval_rouge1': 64.0278, 'eval_rouge2': 56.0738, 'eval_rougeL': 64.0253, 'eval_rougeLsum': 64.0297, 'eval_runtime': 13.9468, 'eval_samples_per_second': 2.581, 'eval_steps_per_second': 0.143, 'epoch': 3.0}
{'loss': 0.4521, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 40%|████      | 56/140 [09:09<07:39,  5.47s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [10:50<49:12, 35.57s/it] 41%|████▏     | 58/140 [10:54<35:46, 26.18s/it] 42%|████▏     | 59/140 [10:58<26:27, 19.60s/it] 43%|████▎     | 60/140 [11:02<19:58, 14.98s/it] 44%|████▎     | 61/140 [11:07<15:27, 11.75s/it] 44%|████▍     | 62/140 [11:11<12:19,  9.47s/it] 45%|████▌     | 63/140 [11:15<10:08,  7.90s/it] 46%|████▌     | 64/140 [11:19<08:36,  6.80s/it] 46%|████▋     | 65/140 [11:24<07:32,  6.03s/it] 47%|████▋     | 66/140 [11:39<10:52,  8.82s/it] 48%|████▊     | 67/140 [11:43<09:02,  7.44s/it] 49%|████▊     | 68/140 [11:47<07:45,  6.46s/it] 49%|████▉     | 69/140 [11:51<06:50,  5.78s/it] 50%|█████     | 70/140 [11:56<06:10,  5.30s/it]                                                 50%|█████     | 70/140 [11:56<06:10,  5.30s/it]int64
{'eval_loss': 0.13578291237354279, 'eval_rouge1': 68.7841, 'eval_rouge2': 62.0356, 'eval_rougeL': 68.7172, 'eval_rougeLsum': 68.7743, 'eval_runtime': 4.7888, 'eval_samples_per_second': 7.518, 'eval_steps_per_second': 0.418, 'epoch': 4.0}
{'loss': 0.0896, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 50%|█████     | 70/140 [12:00<06:10,  5.30s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [13:35<38:31, 33.50s/it] 51%|█████▏    | 72/140 [13:39<28:01, 24.72s/it] 52%|█████▏    | 73/140 [13:44<20:48, 18.64s/it] 53%|█████▎    | 74/140 [13:48<15:45, 14.32s/it] 54%|█████▎    | 75/140 [13:52<12:13, 11.28s/it] 54%|█████▍    | 76/140 [13:56<09:45,  9.15s/it] 55%|█████▌    | 77/140 [14:01<08:04,  7.69s/it] 56%|█████▌    | 78/140 [14:05<06:52,  6.65s/it] 56%|█████▋    | 79/140 [14:09<06:01,  5.92s/it] 57%|█████▋    | 80/140 [14:13<05:24,  5.40s/it] 58%|█████▊    | 81/140 [14:17<04:57,  5.04s/it] 59%|█████▊    | 82/140 [14:22<04:38,  4.79s/it] 59%|█████▉    | 83/140 [14:26<04:23,  4.62s/it] 60%|██████    | 84/140 [14:30<04:17,  4.59s/it]                                                 60%|██████    | 84/140 [14:30<04:17,  4.59s/it]int64
{'eval_loss': 0.0736187994480133, 'eval_rouge1': 69.0325, 'eval_rouge2': 63.2716, 'eval_rougeL': 69.0122, 'eval_rougeLsum': 69.0815, 'eval_runtime': 4.7465, 'eval_samples_per_second': 7.584, 'eval_steps_per_second': 0.421, 'epoch': 5.0}
{'loss': 0.0286, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A                                                
                                             [A 60%|██████    | 84/140 [14:35<04:17,  4.59s/it]
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [15:14<15:04, 16.45s/it] 61%|██████▏   | 86/140 [15:19<11:30, 12.79s/it] 62%|██████▏   | 87/140 [15:23<09:01, 10.22s/it] 63%|██████▎   | 88/140 [15:27<07:17,  8.42s/it] 64%|██████▎   | 89/140 [15:31<06:04,  7.15s/it] 64%|██████▍   | 90/140 [15:36<05:13,  6.28s/it] 65%|██████▌   | 91/140 [15:40<04:37,  5.66s/it] 66%|██████▌   | 92/140 [15:44<04:10,  5.23s/it] 66%|██████▋   | 93/140 [15:48<03:51,  4.92s/it] 67%|██████▋   | 94/140 [15:52<03:36,  4.71s/it] 68%|██████▊   | 95/140 [15:58<03:43,  4.97s/it] 69%|██████▊   | 96/140 [16:02<03:29,  4.75s/it] 69%|██████▉   | 97/140 [16:06<03:18,  4.61s/it] 70%|███████   | 98/140 [16:11<03:08,  4.50s/it]                                                 70%|███████   | 98/140 [16:11<03:08,  4.50s/it]int64
{'eval_loss': 0.06748922914266586, 'eval_rouge1': 70.3186, 'eval_rouge2': 63.3488, 'eval_rougeL': 70.3498, 'eval_rougeLsum': 70.291, 'eval_runtime': 4.8347, 'eval_samples_per_second': 7.446, 'eval_steps_per_second': 0.414, 'epoch': 6.0}
{'loss': 0.0163, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 70%|███████   | 98/140 [16:16<03:08,  4.50s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [17:02<12:37, 18.48s/it] 71%|███████▏  | 100/140 [17:06<09:30, 14.27s/it] 72%|███████▏  | 101/140 [17:11<07:19, 11.27s/it] 73%|███████▎  | 102/140 [17:15<05:48,  9.16s/it] 74%|███████▎  | 103/140 [17:19<04:43,  7.68s/it] 74%|███████▍  | 104/140 [17:23<03:59,  6.64s/it] 75%|███████▌  | 105/140 [17:27<03:27,  5.92s/it] 76%|███████▌  | 106/140 [17:32<03:04,  5.42s/it] 76%|███████▋  | 107/140 [17:36<02:47,  5.07s/it] 77%|███████▋  | 108/140 [17:46<03:30,  6.58s/it] 78%|███████▊  | 109/140 [17:51<03:04,  5.96s/it] 79%|███████▊  | 110/140 [17:55<02:43,  5.44s/it] 79%|███████▉  | 111/140 [17:59<02:26,  5.06s/it] 80%|████████  | 112/140 [18:03<02:14,  4.81s/it]                                                  80%|████████  | 112/140 [18:03<02:14,  4.81s/it]int64
{'eval_loss': 0.060157306492328644, 'eval_rouge1': 71.1006, 'eval_rouge2': 64.3519, 'eval_rougeL': 71.1335, 'eval_rougeLsum': 71.1022, 'eval_runtime': 4.7931, 'eval_samples_per_second': 7.511, 'eval_steps_per_second': 0.417, 'epoch': 7.0}
{'loss': 0.0144, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                 
                                             [A 80%|████████  | 112/140 [18:08<02:14,  4.81s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [19:00<09:08, 20.32s/it] 81%|████████▏ | 114/140 [19:04<06:42, 15.48s/it] 82%|████████▏ | 115/140 [19:08<05:02, 12.09s/it] 83%|████████▎ | 116/140 [19:12<03:53,  9.74s/it] 84%|████████▎ | 117/140 [19:17<03:06,  8.09s/it] 84%|████████▍ | 118/140 [19:21<02:32,  6.92s/it] 85%|████████▌ | 119/140 [19:25<02:08,  6.11s/it] 86%|████████▌ | 120/140 [19:29<01:51,  5.56s/it] 86%|████████▋ | 121/140 [19:33<01:37,  5.15s/it] 87%|████████▋ | 122/140 [19:38<01:27,  4.87s/it] 88%|████████▊ | 123/140 [19:42<01:19,  4.68s/it] 89%|████████▊ | 124/140 [19:46<01:12,  4.55s/it] 89%|████████▉ | 125/140 [19:50<01:06,  4.47s/it] 90%|█████████ | 126/140 [19:56<01:06,  4.74s/it]                                                  90%|█████████ | 126/140 [19:56<01:06,  4.74s/it]int64
{'eval_loss': 0.0546162910759449, 'eval_rouge1': 71.0276, 'eval_rouge2': 64.2915, 'eval_rougeL': 70.8518, 'eval_rougeLsum': 70.8359, 'eval_runtime': 4.8207, 'eval_samples_per_second': 7.468, 'eval_steps_per_second': 0.415, 'epoch': 8.0}
{'loss': 0.0112, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A                                                 
                                             [A 90%|█████████ | 126/140 [20:01<01:06,  4.74s/it]
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [20:51<04:19, 19.96s/it] 91%|█████████▏| 128/140 [20:56<03:02, 15.24s/it] 92%|█████████▏| 129/140 [21:00<02:11, 11.93s/it] 93%|█████████▎| 130/140 [21:04<01:36,  9.62s/it] 94%|█████████▎| 131/140 [21:08<01:11,  7.98s/it] 94%|█████████▍| 132/140 [21:12<00:54,  6.85s/it] 95%|█████████▌| 133/140 [21:17<00:42,  6.07s/it] 96%|█████████▌| 134/140 [21:21<00:33,  5.57s/it] 96%|█████████▋| 135/140 [21:25<00:25,  5.17s/it] 97%|█████████▋| 136/140 [21:29<00:19,  4.88s/it] 98%|█████████▊| 137/140 [21:34<00:14,  4.67s/it] 99%|█████████▊| 138/140 [21:38<00:09,  4.55s/it] 99%|█████████▉| 139/140 [21:42<00:04,  4.43s/it]100%|██████████| 140/140 [21:46<00:00,  4.35s/it]                                                 100%|██████████| 140/140 [21:54<00:00,  4.35s/it]int64
{'eval_loss': 0.05691461265087128, 'eval_rouge1': 71.0276, 'eval_rouge2': 64.2915, 'eval_rougeL': 70.8518, 'eval_rougeLsum': 70.8359, 'eval_runtime': 4.8295, 'eval_samples_per_second': 7.454, 'eval_steps_per_second': 0.414, 'epoch': 9.0}
{'loss': 0.0116, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:03<00:00,  1.17s/it][A                                                 
                                             [A100%|██████████| 140/140 [22:00<00:00,  4.35s/it]
100%|██████████| 2/2 [00:03<00:00,  1.17s/it][A
                                             [A                                                 100%|██████████| 140/140 [24:51<00:00,  4.35s/it]100%|██████████| 140/140 [24:53<00:00, 10.67s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.05723118036985397, 'eval_rouge1': 71.0276, 'eval_rouge2': 64.2915, 'eval_rougeL': 70.8518, 'eval_rougeLsum': 70.8359, 'eval_runtime': 5.8711, 'eval_samples_per_second': 6.132, 'eval_steps_per_second': 0.341, 'epoch': 10.0}
{'train_runtime': 1503.2956, 'train_samples_per_second': 2.115, 'train_steps_per_second': 0.093, 'train_loss': 2.4203306151287896, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▃▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▅▆▇▇█████
wandb:                    eval/rouge2 ▁▄▆▇██████
wandb:                    eval/rougeL ▁▅▆▇▇█████
wandb:                 eval/rougeLsum ▁▅▆▇▇█████
wandb:                   eval/runtime ▄▁█▁▁▁▁▁▁▂
wandb:        eval/samples_per_second ▃▇▁██████▆
wandb:          eval/steps_per_second ▃▇▁██████▆
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▃▂▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.05723
wandb:                    eval/rouge1 71.0276
wandb:                    eval/rouge2 64.2915
wandb:                    eval/rougeL 70.8518
wandb:                 eval/rougeLsum 70.8359
wandb:                   eval/runtime 5.8711
wandb:        eval/samples_per_second 6.132
wandb:          eval/steps_per_second 0.341
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0116
wandb:               train/total_flos 229036315115520.0
wandb:               train/train_loss 2.42033
wandb:            train/train_runtime 1503.2956
wandb: train/train_samples_per_second 2.115
wandb:   train/train_steps_per_second 0.093
wandb: 
wandb: 🚀 View run flan-t5-large_ep-10_ga-1_b-4_lr-0.0001 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/yi6dwwhy
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_204006-yi6dwwhy/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_210612-waa36zo5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-large_ep-10_ga-1_b-4_lr-0.0001
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/waa36zo5
################################################ t5-large_ep-10_ga-1_b-4_lr-0.0001 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:14<33:16, 14.37s/it]  1%|▏         | 2/140 [00:24<27:17, 11.87s/it]  2%|▏         | 3/140 [00:28<18:51,  8.26s/it]  3%|▎         | 4/140 [00:32<14:51,  6.56s/it]  4%|▎         | 5/140 [00:36<12:38,  5.62s/it]  4%|▍         | 6/140 [00:40<11:19,  5.07s/it]  5%|▌         | 7/140 [00:44<10:28,  4.72s/it]  6%|▌         | 8/140 [00:48<09:51,  4.48s/it]  6%|▋         | 9/140 [00:52<09:25,  4.32s/it]  7%|▋         | 10/140 [00:56<09:08,  4.22s/it]  8%|▊         | 11/140 [01:00<08:55,  4.15s/it]  9%|▊         | 12/140 [01:04<08:42,  4.08s/it]  9%|▉         | 13/140 [01:08<08:34,  4.06s/it] 10%|█         | 14/140 [01:12<08:27,  4.03s/it]                                                 10%|█         | 14/140 [01:12<08:27,  4.03s/it]{'loss': 5.8379, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.19s/it][A                                                
                                             [A 10%|█         | 14/140 [01:17<08:27,  4.03s/it]
100%|██████████| 2/2 [00:02<00:00,  1.19s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [01:32<18:44,  9.00s/it] 11%|█▏        | 16/140 [01:36<15:28,  7.49s/it] 12%|█▏        | 17/140 [01:40<13:13,  6.45s/it] 13%|█▎        | 18/140 [01:44<11:35,  5.70s/it] 14%|█▎        | 19/140 [01:48<10:32,  5.23s/it] 14%|█▍        | 20/140 [01:54<10:50,  5.42s/it] 15%|█▌        | 21/140 [01:58<09:53,  4.98s/it] 16%|█▌        | 22/140 [02:02<09:12,  4.68s/it] 16%|█▋        | 23/140 [02:06<08:41,  4.46s/it] 17%|█▋        | 24/140 [02:10<08:20,  4.31s/it] 18%|█▊        | 25/140 [02:14<08:04,  4.21s/it] 19%|█▊        | 26/140 [02:18<07:58,  4.20s/it] 19%|█▉        | 27/140 [02:22<07:44,  4.11s/it] 20%|██        | 28/140 [02:26<07:36,  4.07s/it]                                                 20%|██        | 28/140 [02:32<07:36,  4.07s/it]int64
{'eval_loss': 1.311814546585083, 'eval_rouge1': 1.6667, 'eval_rouge2': 1.5432, 'eval_rougeL': 1.6667, 'eval_rougeLsum': 1.6667, 'eval_runtime': 4.9214, 'eval_samples_per_second': 7.315, 'eval_steps_per_second': 0.406, 'epoch': 1.0}
{'loss': 0.8417, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.20s/it][A                                                
                                             [A 20%|██        | 28/140 [02:37<07:36,  4.07s/it]
100%|██████████| 2/2 [00:02<00:00,  1.20s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [04:08<1:01:49, 33.42s/it] 21%|██▏       | 30/140 [04:12<45:03, 24.58s/it]   22%|██▏       | 31/140 [04:16<33:25, 18.40s/it] 23%|██▎       | 32/140 [04:20<25:19, 14.07s/it] 24%|██▎       | 33/140 [04:24<19:41, 11.04s/it] 24%|██▍       | 34/140 [04:28<15:46,  8.93s/it] 25%|██▌       | 35/140 [04:32<13:01,  7.45s/it] 26%|██▌       | 36/140 [04:36<11:07,  6.41s/it] 26%|██▋       | 37/140 [04:40<09:45,  5.69s/it] 27%|██▋       | 38/140 [04:44<08:48,  5.18s/it] 28%|██▊       | 39/140 [05:01<14:40,  8.72s/it] 29%|██▊       | 40/140 [05:05<12:09,  7.30s/it] 29%|██▉       | 41/140 [05:09<10:23,  6.30s/it] 30%|███       | 42/140 [05:13<09:08,  5.60s/it]                                                 30%|███       | 42/140 [05:13<09:08,  5.60s/it]int64
{'eval_loss': 0.24746571481227875, 'eval_rouge1': 25.6919, 'eval_rouge2': 23.4157, 'eval_rougeL': 25.6695, 'eval_rougeLsum': 25.8344, 'eval_runtime': 4.978, 'eval_samples_per_second': 7.232, 'eval_steps_per_second': 0.402, 'epoch': 2.0}
{'loss': 0.1571, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.08s/it][A                                                
                                             [A 30%|███       | 42/140 [05:17<09:08,  5.60s/it]
100%|██████████| 2/2 [00:02<00:00,  1.08s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [05:43<21:01, 13.00s/it] 31%|███▏      | 44/140 [05:47<16:28, 10.30s/it] 32%|███▏      | 45/140 [05:51<13:18,  8.41s/it] 33%|███▎      | 46/140 [05:55<11:04,  7.07s/it] 34%|███▎      | 47/140 [05:59<09:30,  6.14s/it] 34%|███▍      | 48/140 [06:03<08:25,  5.49s/it] 35%|███▌      | 49/140 [06:07<07:39,  5.05s/it] 36%|███▌      | 50/140 [06:11<07:05,  4.73s/it] 36%|███▋      | 51/140 [06:15<06:40,  4.50s/it] 37%|███▋      | 52/140 [06:19<06:23,  4.35s/it] 38%|███▊      | 53/140 [06:23<06:08,  4.23s/it] 39%|███▊      | 54/140 [06:28<06:18,  4.41s/it] 39%|███▉      | 55/140 [06:32<06:03,  4.28s/it] 40%|████      | 56/140 [06:35<05:50,  4.18s/it]                                                 40%|████      | 56/140 [06:35<05:50,  4.18s/it]int64
{'eval_loss': 0.06967207789421082, 'eval_rouge1': 70.5844, 'eval_rouge2': 65.4762, 'eval_rougeL': 70.5875, 'eval_rougeLsum': 70.5929, 'eval_runtime': 4.4792, 'eval_samples_per_second': 8.037, 'eval_steps_per_second': 0.447, 'epoch': 3.0}
{'loss': 0.0366, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A                                                
                                             [A 40%|████      | 56/140 [06:40<05:50,  4.18s/it]
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [08:27<50:13, 36.31s/it] 41%|████▏     | 58/140 [08:31<36:27, 26.68s/it] 42%|████▏     | 59/140 [08:35<26:48, 19.86s/it] 43%|████▎     | 60/140 [08:39<20:07, 15.09s/it] 44%|████▎     | 61/140 [08:43<15:27, 11.74s/it] 44%|████▍     | 62/140 [08:47<12:15,  9.43s/it] 45%|████▌     | 63/140 [08:51<09:59,  7.79s/it] 46%|████▌     | 64/140 [08:55<08:25,  6.66s/it] 46%|████▋     | 65/140 [08:59<07:19,  5.85s/it] 47%|████▋     | 66/140 [09:06<07:53,  6.39s/it] 48%|████▊     | 67/140 [09:10<06:52,  5.66s/it] 49%|████▊     | 68/140 [09:14<06:09,  5.14s/it] 49%|████▉     | 69/140 [09:18<05:39,  4.78s/it] 50%|█████     | 70/140 [09:22<05:16,  4.53s/it]                                                 50%|█████     | 70/140 [09:22<05:16,  4.53s/it]int64
{'eval_loss': 0.05141694098711014, 'eval_rouge1': 70.7769, 'eval_rouge2': 65.1331, 'eval_rougeL': 70.7612, 'eval_rougeLsum': 70.7765, 'eval_runtime': 4.4526, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 0.449, 'epoch': 4.0}
{'loss': 0.0198, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A                                                
                                             [A 50%|█████     | 70/140 [09:27<05:16,  4.53s/it]
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [09:53<14:17, 12.43s/it] 51%|█████▏    | 72/140 [09:57<11:12,  9.89s/it] 52%|█████▏    | 73/140 [10:01<09:03,  8.11s/it] 53%|█████▎    | 74/140 [10:05<07:32,  6.86s/it] 54%|█████▎    | 75/140 [10:09<06:29,  5.98s/it] 54%|█████▍    | 76/140 [10:13<05:44,  5.38s/it] 55%|█████▌    | 77/140 [10:17<05:11,  4.94s/it] 56%|█████▌    | 78/140 [10:21<04:47,  4.64s/it] 56%|█████▋    | 79/140 [10:34<07:13,  7.11s/it] 57%|█████▋    | 80/140 [10:38<06:09,  6.16s/it] 58%|█████▊    | 81/140 [10:41<05:24,  5.50s/it] 59%|█████▊    | 82/140 [10:45<04:51,  5.02s/it] 59%|█████▉    | 83/140 [10:49<04:28,  4.71s/it] 60%|██████    | 84/140 [10:53<04:10,  4.47s/it]                                                 60%|██████    | 84/140 [10:53<04:10,  4.47s/it]int64
{'eval_loss': 0.039373576641082764, 'eval_rouge1': 70.7769, 'eval_rouge2': 65.1331, 'eval_rougeL': 70.7612, 'eval_rougeLsum': 70.7765, 'eval_runtime': 4.4352, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 0.451, 'epoch': 5.0}
{'loss': 0.0092, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A                                                
                                             [A 60%|██████    | 84/140 [10:58<04:10,  4.47s/it]
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [12:28<28:47, 31.41s/it] 61%|██████▏   | 86/140 [12:31<20:51, 23.17s/it] 62%|██████▏   | 87/140 [12:35<15:22, 17.41s/it] 63%|██████▎   | 88/140 [12:39<11:35, 13.38s/it] 64%|██████▎   | 89/140 [12:43<08:58, 10.55s/it] 64%|██████▍   | 90/140 [12:47<07:08,  8.58s/it] 65%|██████▌   | 91/140 [12:51<05:52,  7.20s/it] 66%|██████▌   | 92/140 [12:55<04:59,  6.24s/it] 66%|██████▋   | 93/140 [12:59<04:21,  5.56s/it] 67%|██████▋   | 94/140 [13:03<03:53,  5.09s/it] 68%|██████▊   | 95/140 [13:07<03:34,  4.76s/it] 69%|██████▊   | 96/140 [13:11<03:18,  4.51s/it] 69%|██████▉   | 97/140 [13:15<03:07,  4.35s/it] 70%|███████   | 98/140 [13:19<02:57,  4.23s/it]                                                 70%|███████   | 98/140 [13:19<02:57,  4.23s/it]int64
{'eval_loss': 0.0395628958940506, 'eval_rouge1': 71.0696, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8341, 'eval_rougeLsum': 70.8625, 'eval_runtime': 4.4184, 'eval_samples_per_second': 8.148, 'eval_steps_per_second': 0.453, 'epoch': 6.0}
{'loss': 0.0058, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A                                                
                                             [A 70%|███████   | 98/140 [13:24<02:57,  4.23s/it]
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [15:42<31:24, 45.96s/it] 71%|███████▏  | 100/140 [15:46<22:14, 33.35s/it] 72%|███████▏  | 101/140 [15:50<15:56, 24.53s/it] 73%|███████▎  | 102/140 [16:22<16:53, 26.68s/it] 74%|███████▎  | 103/140 [16:26<12:14, 19.86s/it] 74%|███████▍  | 104/140 [16:43<11:24, 19.01s/it] 75%|███████▌  | 105/140 [16:47<08:27, 14.49s/it] 76%|███████▌  | 106/140 [16:51<06:25, 11.33s/it] 76%|███████▋  | 107/140 [16:55<05:01,  9.13s/it] 77%|███████▋  | 108/140 [16:59<04:02,  7.58s/it] 78%|███████▊  | 109/140 [17:03<03:21,  6.50s/it] 79%|███████▊  | 110/140 [17:07<02:51,  5.73s/it] 79%|███████▉  | 111/140 [17:11<02:31,  5.21s/it] 80%|████████  | 112/140 [17:15<02:15,  4.85s/it]                                                  80%|████████  | 112/140 [17:15<02:15,  4.85s/it]int64
{'eval_loss': 0.033689383417367935, 'eval_rouge1': 71.0696, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8341, 'eval_rougeLsum': 70.8625, 'eval_runtime': 4.398, 'eval_samples_per_second': 8.186, 'eval_steps_per_second': 0.455, 'epoch': 7.0}
{'loss': 0.0058, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:18<00:00,  1.06s/it][A                                                 
                                             [A 80%|████████  | 112/140 [17:35<02:15,  4.85s/it]
100%|██████████| 2/2 [00:18<00:00,  1.06s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [17:50<06:19, 14.07s/it] 81%|████████▏ | 114/140 [17:54<04:47, 11.04s/it] 82%|████████▏ | 115/140 [17:58<03:43,  8.93s/it] 83%|████████▎ | 116/140 [18:02<02:58,  7.42s/it] 84%|████████▎ | 117/140 [18:06<02:26,  6.38s/it] 84%|████████▍ | 118/140 [18:10<02:04,  5.66s/it] 85%|████████▌ | 119/140 [18:23<02:47,  7.95s/it] 86%|████████▌ | 120/140 [18:27<02:15,  6.75s/it] 86%|████████▋ | 121/140 [18:31<01:52,  5.91s/it] 87%|████████▋ | 122/140 [18:35<01:35,  5.33s/it] 88%|████████▊ | 123/140 [18:39<01:23,  4.93s/it] 89%|████████▊ | 124/140 [18:43<01:14,  4.63s/it] 89%|████████▉ | 125/140 [18:47<01:06,  4.42s/it] 90%|█████████ | 126/140 [18:51<01:00,  4.29s/it]                                                  90%|█████████ | 126/140 [19:04<01:00,  4.29s/it]int64
{'eval_loss': 0.03462208807468414, 'eval_rouge1': 71.0696, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8341, 'eval_rougeLsum': 70.8625, 'eval_runtime': 20.4359, 'eval_samples_per_second': 1.762, 'eval_steps_per_second': 0.098, 'epoch': 8.0}
{'loss': 0.0051, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A                                                 
                                             [A 90%|█████████ | 126/140 [19:09<01:00,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [19:27<02:57, 13.62s/it] 91%|█████████▏| 128/140 [19:31<02:08, 10.72s/it] 92%|█████████▏| 129/140 [19:35<01:35,  8.70s/it] 93%|█████████▎| 130/140 [19:39<01:12,  7.28s/it] 94%|█████████▎| 131/140 [19:42<00:56,  6.28s/it] 94%|█████████▍| 132/140 [19:59<01:15,  9.45s/it] 95%|█████████▌| 133/140 [20:03<00:54,  7.80s/it] 96%|█████████▌| 134/140 [20:07<00:39,  6.65s/it] 96%|█████████▋| 135/140 [20:11<00:29,  5.84s/it] 97%|█████████▋| 136/140 [20:15<00:21,  5.29s/it] 98%|█████████▊| 137/140 [20:19<00:14,  4.92s/it] 99%|█████████▊| 138/140 [20:23<00:09,  4.64s/it] 99%|█████████▉| 139/140 [20:27<00:04,  4.44s/it]100%|██████████| 140/140 [20:31<00:00,  4.29s/it]                                                 100%|██████████| 140/140 [20:39<00:00,  4.29s/it]int64
{'eval_loss': 0.03133707121014595, 'eval_rouge1': 71.0696, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8341, 'eval_rougeLsum': 70.8625, 'eval_runtime': 4.5083, 'eval_samples_per_second': 7.985, 'eval_steps_per_second': 0.444, 'epoch': 9.0}
{'loss': 0.0033, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A                                                 
                                             [A100%|██████████| 140/140 [20:44<00:00,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A
                                             [A                                                 100%|██████████| 140/140 [20:57<00:00,  4.29s/it]100%|██████████| 140/140 [20:57<00:00,  8.98s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.031091293320059776, 'eval_rouge1': 71.0696, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8341, 'eval_rougeLsum': 70.8625, 'eval_runtime': 4.4773, 'eval_samples_per_second': 8.04, 'eval_steps_per_second': 0.447, 'epoch': 10.0}
{'train_runtime': 1265.2453, 'train_samples_per_second': 2.513, 'train_steps_per_second': 0.111, 'train_loss': 0.6922185920444982, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▃████████
wandb:                    eval/rouge2 ▁▃████████
wandb:                    eval/rougeL ▁▃████████
wandb:                 eval/rougeLsum ▁▃████████
wandb:                   eval/runtime ▁▁▁▁▁▁▁█▁▁
wandb:        eval/samples_per_second ▇▇█████▁██
wandb:          eval/steps_per_second ▇▇█████▁██
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.03109
wandb:                    eval/rouge1 71.0696
wandb:                    eval/rouge2 65.519
wandb:                    eval/rougeL 70.8341
wandb:                 eval/rougeLsum 70.8625
wandb:                   eval/runtime 4.4773
wandb:        eval/samples_per_second 8.04
wandb:          eval/steps_per_second 0.447
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0033
wandb:               train/total_flos 215151575040000.0
wandb:               train/train_loss 0.69222
wandb:            train/train_runtime 1265.2453
wandb: train/train_samples_per_second 2.513
wandb:   train/train_steps_per_second 0.111
wandb: 
wandb: 🚀 View run t5-large_ep-10_ga-1_b-4_lr-0.0001 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/waa36zo5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_210612-waa36zo5/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_212823-6aj6dr9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mt5-base_ep-10_ga-1_b-4_lr-0.0001
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/6aj6dr9r
################################################ mt5-base_ep-10_ga-1_b-4_lr-0.0001 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:13<31:35, 13.64s/it]  1%|▏         | 2/140 [00:16<17:06,  7.44s/it]  2%|▏         | 3/140 [00:19<12:27,  5.46s/it]  3%|▎         | 4/140 [00:22<10:16,  4.54s/it]  4%|▎         | 5/140 [00:26<09:02,  4.02s/it]  4%|▍         | 6/140 [00:29<08:15,  3.70s/it]  5%|▌         | 7/140 [00:40<13:58,  6.30s/it]  6%|▌         | 8/140 [00:43<11:38,  5.29s/it]  6%|▋         | 9/140 [00:46<10:01,  4.59s/it]  7%|▋         | 10/140 [00:50<08:57,  4.13s/it]  8%|▊         | 11/140 [00:53<08:13,  3.82s/it]  9%|▊         | 12/140 [00:56<07:41,  3.60s/it]  9%|▉         | 13/140 [00:59<07:18,  3.45s/it] 10%|█         | 14/140 [01:02<06:59,  3.33s/it]                                                 10%|█         | 14/140 [01:02<06:59,  3.33s/it]{'loss': 31.2285, 'learning_rate': 9e-05, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A 10%|█         | 14/140 [01:06<06:59,  3.33s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [02:29<59:33, 28.58s/it] 11%|█▏        | 16/140 [02:32<43:18, 20.95s/it] 12%|█▏        | 17/140 [02:35<31:57, 15.59s/it] 13%|█▎        | 18/140 [02:39<24:04, 11.84s/it] 14%|█▎        | 19/140 [02:42<18:34,  9.21s/it] 14%|█▍        | 20/140 [02:45<14:45,  7.38s/it] 15%|█▌        | 21/140 [02:48<12:05,  6.09s/it] 16%|█▌        | 22/140 [02:51<10:13,  5.20s/it] 16%|█▋        | 23/140 [02:54<08:54,  4.57s/it] 17%|█▋        | 24/140 [02:57<07:59,  4.13s/it] 18%|█▊        | 25/140 [03:00<07:19,  3.82s/it] 19%|█▊        | 26/140 [03:03<06:51,  3.61s/it] 19%|█▉        | 27/140 [03:16<11:48,  6.27s/it] 20%|██        | 28/140 [03:19<09:54,  5.31s/it]                                                 20%|██        | 28/140 [03:19<09:54,  5.31s/it]int64
{'eval_loss': 22.91152000427246, 'eval_rouge1': 7.8283, 'eval_rouge2': 1.6123, 'eval_rougeL': 8.0644, 'eval_rougeLsum': 7.8766, 'eval_runtime': 3.6804, 'eval_samples_per_second': 9.782, 'eval_steps_per_second': 0.543, 'epoch': 1.0}
{'loss': 27.1264, 'learning_rate': 8e-05, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                
                                             [A 20%|██        | 28/140 [03:23<09:54,  5.31s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [05:56<1:33:48, 50.71s/it] 21%|██▏       | 30/140 [05:59<1:06:46, 36.42s/it] 22%|██▏       | 31/140 [06:02<48:00, 26.42s/it]   23%|██▎       | 32/140 [06:05<34:58, 19.43s/it] 24%|██▎       | 33/140 [06:08<25:54, 14.53s/it] 24%|██▍       | 34/140 [06:11<19:37, 11.11s/it] 25%|██▌       | 35/140 [06:14<15:14,  8.71s/it] 26%|██▌       | 36/140 [06:17<12:11,  7.04s/it] 26%|██▋       | 37/140 [06:21<10:08,  5.91s/it] 27%|██▋       | 38/140 [06:24<08:36,  5.07s/it] 28%|██▊       | 39/140 [06:27<07:31,  4.47s/it] 29%|██▊       | 40/140 [06:30<06:46,  4.06s/it] 29%|██▉       | 41/140 [06:48<13:38,  8.26s/it] 30%|███       | 42/140 [06:51<10:56,  6.70s/it]                                                 30%|███       | 42/140 [06:51<10:56,  6.70s/it]int64
{'eval_loss': 20.12794303894043, 'eval_rouge1': 11.1176, 'eval_rouge2': 4.2146, 'eval_rougeL': 11.0886, 'eval_rougeLsum': 10.9884, 'eval_runtime': 3.631, 'eval_samples_per_second': 9.915, 'eval_steps_per_second': 0.551, 'epoch': 2.0}
{'loss': 24.7685, 'learning_rate': 7e-05, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 30%|███       | 42/140 [06:55<10:56,  6.70s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [09:09<1:14:20, 45.99s/it] 31%|███▏      | 44/140 [09:12<52:59, 33.12s/it]   32%|███▏      | 45/140 [09:15<38:10, 24.11s/it] 33%|███▎      | 46/140 [09:18<27:53, 17.80s/it] 34%|███▎      | 47/140 [09:21<20:44, 13.39s/it] 34%|███▍      | 48/140 [09:24<15:46, 10.29s/it] 35%|███▌      | 49/140 [09:27<12:20,  8.13s/it] 36%|███▌      | 50/140 [09:30<09:56,  6.63s/it] 36%|███▋      | 51/140 [09:33<08:15,  5.56s/it] 37%|███▋      | 52/140 [09:36<07:04,  4.82s/it] 38%|███▊      | 53/140 [09:40<06:14,  4.30s/it] 39%|███▊      | 54/140 [09:43<05:38,  3.94s/it] 39%|███▉      | 55/140 [10:10<15:40, 11.07s/it] 40%|████      | 56/140 [10:24<16:25, 11.73s/it]                                                 40%|████      | 56/140 [10:24<16:25, 11.73s/it]int64
{'eval_loss': 17.61635971069336, 'eval_rouge1': 13.0528, 'eval_rouge2': 4.5565, 'eval_rougeL': 12.7958, 'eval_rougeLsum': 12.6913, 'eval_runtime': 3.6586, 'eval_samples_per_second': 9.84, 'eval_steps_per_second': 0.547, 'epoch': 3.0}
{'loss': 22.277, 'learning_rate': 6e-05, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 40%|████      | 56/140 [10:27<16:25, 11.73s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [15:20<2:14:24, 97.16s/it] 41%|████▏     | 58/140 [15:23<1:34:13, 68.94s/it] 42%|████▏     | 59/140 [15:26<1:06:24, 49.19s/it] 43%|████▎     | 60/140 [15:29<47:08, 35.36s/it]   44%|████▎     | 61/140 [15:32<33:48, 25.68s/it] 44%|████▍     | 62/140 [15:36<24:34, 18.90s/it] 45%|████▌     | 63/140 [15:39<18:11, 14.17s/it] 46%|████▌     | 64/140 [15:42<13:45, 10.86s/it] 46%|████▋     | 65/140 [15:45<10:39,  8.53s/it] 47%|████▋     | 66/140 [15:48<08:29,  6.89s/it] 48%|████▊     | 67/140 [15:51<06:59,  5.75s/it] 49%|████▊     | 68/140 [15:54<05:57,  4.96s/it] 49%|████▉     | 69/140 [16:08<09:00,  7.62s/it] 50%|█████     | 70/140 [16:11<07:17,  6.26s/it]                                                 50%|█████     | 70/140 [16:11<07:17,  6.26s/it]int64
{'eval_loss': 14.95408821105957, 'eval_rouge1': 16.8944, 'eval_rouge2': 5.1174, 'eval_rougeL': 16.7448, 'eval_rougeLsum': 16.7818, 'eval_runtime': 3.6475, 'eval_samples_per_second': 9.87, 'eval_steps_per_second': 0.548, 'epoch': 4.0}
{'loss': 20.0908, 'learning_rate': 5e-05, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 50%|█████     | 70/140 [16:15<07:17,  6.26s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [17:04<23:19, 20.29s/it] 51%|█████▏    | 72/140 [17:07<17:08, 15.12s/it] 52%|█████▏    | 73/140 [17:10<12:51, 11.51s/it] 53%|█████▎    | 74/140 [17:13<09:52,  8.98s/it] 54%|█████▎    | 75/140 [17:16<07:48,  7.21s/it] 54%|█████▍    | 76/140 [17:20<06:22,  5.97s/it] 55%|█████▌    | 77/140 [17:23<05:22,  5.11s/it] 56%|█████▌    | 78/140 [17:26<04:39,  4.50s/it] 56%|█████▋    | 79/140 [17:29<04:08,  4.07s/it] 57%|█████▋    | 80/140 [17:32<03:46,  3.78s/it] 58%|█████▊    | 81/140 [17:35<03:31,  3.58s/it] 59%|█████▊    | 82/140 [17:38<03:18,  3.43s/it] 59%|█████▉    | 83/140 [17:41<03:10,  3.33s/it] 60%|██████    | 84/140 [17:59<07:03,  7.57s/it]                                                 60%|██████    | 84/140 [17:59<07:03,  7.57s/it]int64
{'eval_loss': 13.09115982055664, 'eval_rouge1': 20.5807, 'eval_rouge2': 6.7308, 'eval_rougeL': 20.5317, 'eval_rougeLsum': 20.5322, 'eval_runtime': 3.6593, 'eval_samples_per_second': 9.838, 'eval_steps_per_second': 0.547, 'epoch': 5.0}
{'loss': 18.5965, 'learning_rate': 4e-05, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                
                                             [A 60%|██████    | 84/140 [18:02<07:03,  7.57s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [19:09<24:08, 26.33s/it] 61%|██████▏   | 86/140 [19:12<17:25, 19.36s/it] 62%|██████▏   | 87/140 [19:15<12:47, 14.47s/it] 63%|██████▎   | 88/140 [19:18<09:34, 11.06s/it] 64%|██████▎   | 89/140 [19:21<07:24,  8.71s/it] 64%|██████▍   | 90/140 [19:47<11:37, 13.95s/it] 65%|██████▌   | 91/140 [19:50<08:43, 10.68s/it] 66%|██████▌   | 92/140 [19:54<06:43,  8.42s/it] 66%|██████▋   | 93/140 [19:57<05:20,  6.82s/it] 67%|██████▋   | 94/140 [20:00<04:21,  5.69s/it] 68%|██████▊   | 95/140 [20:03<03:40,  4.91s/it] 69%|██████▊   | 96/140 [20:06<03:12,  4.37s/it] 69%|██████▉   | 97/140 [20:09<02:51,  3.99s/it] 70%|███████   | 98/140 [20:12<02:38,  3.76s/it]                                                 70%|███████   | 98/140 [20:12<02:38,  3.76s/it]int64
{'eval_loss': 12.516101837158203, 'eval_rouge1': 28.5101, 'eval_rouge2': 10.4452, 'eval_rougeL': 27.3996, 'eval_rougeLsum': 27.31, 'eval_runtime': 3.6426, 'eval_samples_per_second': 9.883, 'eval_steps_per_second': 0.549, 'epoch': 6.0}
{'loss': 17.212, 'learning_rate': 3e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                
                                             [A 70%|███████   | 98/140 [20:16<02:38,  3.76s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [20:45<08:28, 12.40s/it] 71%|███████▏  | 100/140 [20:48<06:24,  9.61s/it] 72%|███████▏  | 101/140 [20:51<05:00,  7.69s/it] 73%|███████▎  | 102/140 [20:54<03:59,  6.31s/it] 74%|███████▎  | 103/140 [20:57<03:17,  5.34s/it] 74%|███████▍  | 104/140 [21:00<02:47,  4.66s/it] 75%|███████▌  | 105/140 [21:03<02:26,  4.19s/it] 76%|███████▌  | 106/140 [21:07<02:11,  3.87s/it] 76%|███████▋  | 107/140 [21:10<02:00,  3.64s/it] 77%|███████▋  | 108/140 [21:23<03:31,  6.62s/it] 78%|███████▊  | 109/140 [21:26<02:52,  5.57s/it] 79%|███████▊  | 110/140 [21:30<02:26,  4.88s/it] 79%|███████▉  | 111/140 [21:33<02:05,  4.34s/it] 80%|████████  | 112/140 [21:36<01:50,  3.96s/it]                                                  80%|████████  | 112/140 [21:36<01:50,  3.96s/it]int64
{'eval_loss': 11.834796905517578, 'eval_rouge1': 27.6268, 'eval_rouge2': 11.5482, 'eval_rougeL': 26.6094, 'eval_rougeLsum': 26.4814, 'eval_runtime': 3.6534, 'eval_samples_per_second': 9.854, 'eval_steps_per_second': 0.547, 'epoch': 7.0}
{'loss': 16.6689, 'learning_rate': 2e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                 
                                             [A 80%|████████  | 112/140 [21:39<01:50,  3.96s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [22:22<07:29, 16.63s/it] 81%|████████▏ | 114/140 [22:25<05:26, 12.57s/it] 82%|████████▏ | 115/140 [22:28<04:03,  9.73s/it] 83%|████████▎ | 116/140 [22:31<03:05,  7.74s/it] 84%|████████▎ | 117/140 [22:34<02:25,  6.34s/it] 84%|████████▍ | 118/140 [22:37<01:57,  5.35s/it] 85%|████████▌ | 119/140 [22:41<01:38,  4.68s/it] 86%|████████▌ | 120/140 [22:44<01:24,  4.20s/it] 86%|████████▋ | 121/140 [22:47<01:13,  3.87s/it] 87%|████████▋ | 122/140 [22:50<01:05,  3.63s/it] 88%|████████▊ | 123/140 [22:53<00:58,  3.46s/it] 89%|████████▊ | 124/140 [22:56<00:53,  3.36s/it] 89%|████████▉ | 125/140 [23:04<01:12,  4.80s/it] 90%|█████████ | 126/140 [23:07<00:59,  4.27s/it]                                                  90%|█████████ | 126/140 [23:07<00:59,  4.27s/it]int64
{'eval_loss': 12.001450538635254, 'eval_rouge1': 28.3466, 'eval_rouge2': 10.2757, 'eval_rougeL': 27.153, 'eval_rougeLsum': 26.9204, 'eval_runtime': 3.654, 'eval_samples_per_second': 9.852, 'eval_steps_per_second': 0.547, 'epoch': 8.0}
{'loss': 15.7472, 'learning_rate': 1e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A                                                 
                                             [A 90%|█████████ | 126/140 [23:11<00:59,  4.27s/it]
100%|██████████| 2/2 [00:01<00:00,  1.16it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [25:38<10:28, 48.35s/it] 91%|█████████▏| 128/140 [25:41<06:57, 34.77s/it] 92%|█████████▏| 129/140 [25:45<04:38, 25.28s/it] 93%|█████████▎| 130/140 [25:48<03:07, 18.71s/it] 94%|█████████▎| 131/140 [25:51<02:06, 14.02s/it] 94%|█████████▍| 132/140 [25:54<01:25, 10.74s/it] 95%|█████████▌| 133/140 [25:57<00:59,  8.44s/it] 96%|█████████▌| 134/140 [26:00<00:41,  6.84s/it] 96%|█████████▋| 135/140 [26:03<00:28,  5.71s/it] 97%|█████████▋| 136/140 [26:07<00:19,  4.92s/it] 98%|█████████▊| 137/140 [26:10<00:13,  4.38s/it] 99%|█████████▊| 138/140 [26:13<00:07,  3.99s/it] 99%|█████████▉| 139/140 [26:16<00:03,  3.78s/it]100%|██████████| 140/140 [26:19<00:00,  3.57s/it]                                                 100%|██████████| 140/140 [26:19<00:00,  3.57s/it]int64
{'eval_loss': 12.45508861541748, 'eval_rouge1': 26.1914, 'eval_rouge2': 9.4796, 'eval_rougeL': 25.3386, 'eval_rougeLsum': 25.0641, 'eval_runtime': 3.6019, 'eval_samples_per_second': 9.995, 'eval_steps_per_second': 0.555, 'epoch': 9.0}
{'loss': 15.1125, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A                                                 
                                             [A100%|██████████| 140/140 [26:23<00:00,  3.57s/it]
100%|██████████| 2/2 [00:01<00:00,  1.15it/s][A
                                             [A                                                 100%|██████████| 140/140 [26:44<00:00,  3.57s/it]100%|██████████| 140/140 [26:45<00:00, 11.47s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 12.30143928527832, 'eval_rouge1': 27.1028, 'eval_rouge2': 9.3607, 'eval_rougeL': 26.2562, 'eval_rougeLsum': 26.0792, 'eval_runtime': 3.6526, 'eval_samples_per_second': 9.856, 'eval_steps_per_second': 0.548, 'epoch': 10.0}
{'train_runtime': 1621.488, 'train_samples_per_second': 1.961, 'train_steps_per_second': 0.086, 'train_loss': 20.882821764264786, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▆▅▃▂▁▁▁▁▁
wandb:                    eval/rouge1 ▁▂▃▄▅███▇█
wandb:                    eval/rouge2 ▁▃▃▃▅▇█▇▇▆
wandb:                    eval/rougeL ▁▂▃▄▆███▇█
wandb:                 eval/rougeLsum ▁▂▃▄▆███▇█
wandb:                   eval/runtime █▄▆▅▆▅▆▆▁▆
wandb:        eval/samples_per_second ▁▅▃▄▃▄▃▃█▃
wandb:          eval/steps_per_second ▁▆▃▄▃▅▃▃█▄
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▆▅▄▃▃▂▂▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 12.30144
wandb:                    eval/rouge1 27.1028
wandb:                    eval/rouge2 9.3607
wandb:                    eval/rougeL 26.2562
wandb:                 eval/rougeLsum 26.0792
wandb:                   eval/runtime 3.6526
wandb:        eval/samples_per_second 9.856
wandb:          eval/steps_per_second 0.548
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 15.1125
wandb:               train/total_flos 89366407004160.0
wandb:               train/train_loss 20.88282
wandb:            train/train_runtime 1621.488
wandb: train/train_samples_per_second 1.961
wandb:   train/train_steps_per_second 0.086
wandb: 
wandb: 🚀 View run mt5-base_ep-10_ga-1_b-4_lr-0.0001 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/6aj6dr9r
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_212823-6aj6dr9r/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_215614-m1l6195e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-base_ep-10_ga-1_b-4_lr-0.0003
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/m1l6195e
################################################ flan-t5-base_ep-10_ga-1_b-4_lr-0.0003 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:11<27:33, 11.89s/it]  1%|▏         | 2/140 [00:13<13:09,  5.72s/it]  2%|▏         | 3/140 [00:14<08:34,  3.75s/it]  3%|▎         | 4/140 [00:16<06:23,  2.82s/it]  4%|▎         | 5/140 [00:17<05:10,  2.30s/it]  4%|▍         | 6/140 [00:18<04:27,  2.00s/it]  5%|▌         | 7/140 [00:20<04:00,  1.81s/it]  6%|▌         | 8/140 [00:21<03:41,  1.68s/it]  6%|▋         | 9/140 [00:23<03:28,  1.60s/it]  7%|▋         | 10/140 [00:24<03:19,  1.54s/it]  8%|▊         | 11/140 [00:25<03:13,  1.50s/it]  9%|▊         | 12/140 [00:27<03:07,  1.46s/it]  9%|▉         | 13/140 [00:28<03:03,  1.45s/it] 10%|█         | 14/140 [00:30<03:00,  1.43s/it]                                                 10%|█         | 14/140 [00:30<03:00,  1.43s/it]{'loss': 9.9139, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:00<00:00,  2.52it/s][A 10%|█         | 14/140 [00:31<03:00,  1.43s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [00:37<06:38,  3.19s/it] 11%|█▏        | 16/140 [00:38<05:33,  2.69s/it] 12%|█▏        | 17/140 [00:40<04:43,  2.30s/it] 13%|█▎        | 18/140 [00:41<04:07,  2.03s/it] 14%|█▎        | 19/140 [00:43<03:42,  1.84s/it] 14%|█▍        | 20/140 [00:44<03:24,  1.70s/it] 15%|█▌        | 21/140 [00:45<03:11,  1.61s/it] 16%|█▌        | 22/140 [00:47<03:02,  1.54s/it] 16%|█▋        | 23/140 [00:48<02:55,  1.50s/it] 17%|█▋        | 24/140 [00:50<02:50,  1.47s/it] 18%|█▊        | 25/140 [00:51<02:45,  1.44s/it] 19%|█▊        | 26/140 [00:52<02:42,  1.42s/it] 19%|█▉        | 27/140 [00:54<02:40,  1.42s/it] 20%|██        | 28/140 [00:55<02:37,  1.41s/it]                                                 20%|██        | 28/140 [00:55<02:37,  1.41s/it]int64
{'eval_loss': 2.0064735412597656, 'eval_rouge1': 68.6171, 'eval_rouge2': 63.0722, 'eval_rougeL': 68.5751, 'eval_rougeLsum': 68.534, 'eval_runtime': 1.7293, 'eval_samples_per_second': 20.817, 'eval_steps_per_second': 1.157, 'epoch': 1.0}
{'loss': 1.695, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.55it/s][A                                                
                                             [A 20%|██        | 28/140 [00:57<02:37,  1.41s/it]
100%|██████████| 2/2 [00:00<00:00,  2.55it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [01:03<06:06,  3.30s/it] 21%|██▏       | 30/140 [01:04<05:00,  2.73s/it] 22%|██▏       | 31/140 [01:06<04:14,  2.34s/it] 23%|██▎       | 32/140 [01:07<03:41,  2.05s/it] 24%|██▎       | 33/140 [01:08<03:18,  1.86s/it] 24%|██▍       | 34/140 [01:10<03:02,  1.72s/it] 25%|██▌       | 35/140 [01:11<02:50,  1.62s/it] 26%|██▌       | 36/140 [01:13<02:42,  1.56s/it] 26%|██▋       | 37/140 [01:14<02:40,  1.56s/it] 27%|██▋       | 38/140 [01:16<02:34,  1.51s/it] 28%|██▊       | 39/140 [01:17<02:30,  1.49s/it] 29%|██▊       | 40/140 [01:18<02:26,  1.46s/it] 29%|██▉       | 41/140 [01:20<02:23,  1.44s/it] 30%|███       | 42/140 [01:21<02:19,  1.43s/it]                                                 30%|███       | 42/140 [01:21<02:19,  1.43s/it]int64
{'eval_loss': 0.3289419710636139, 'eval_rouge1': 69.6639, 'eval_rouge2': 63.2507, 'eval_rougeL': 69.6561, 'eval_rougeLsum': 69.6623, 'eval_runtime': 1.7086, 'eval_samples_per_second': 21.07, 'eval_steps_per_second': 1.171, 'epoch': 2.0}
{'loss': 0.2666, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.47it/s][A                                                
                                             [A 30%|███       | 42/140 [01:23<02:19,  1.43s/it]
100%|██████████| 2/2 [00:00<00:00,  2.47it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [01:29<05:20,  3.31s/it] 31%|███▏      | 44/140 [01:30<04:22,  2.73s/it] 32%|███▏      | 45/140 [01:32<03:42,  2.34s/it] 33%|███▎      | 46/140 [01:33<03:13,  2.05s/it] 34%|███▎      | 47/140 [01:34<02:52,  1.85s/it] 34%|███▍      | 48/140 [01:36<02:37,  1.71s/it] 35%|███▌      | 49/140 [01:37<02:27,  1.62s/it] 36%|███▌      | 50/140 [01:39<02:19,  1.55s/it] 36%|███▋      | 51/140 [01:40<02:14,  1.51s/it] 37%|███▋      | 52/140 [01:41<02:09,  1.47s/it] 38%|███▊      | 53/140 [01:43<02:06,  1.45s/it] 39%|███▊      | 54/140 [01:44<02:02,  1.43s/it] 39%|███▉      | 55/140 [01:46<02:01,  1.42s/it] 40%|████      | 56/140 [01:47<01:58,  1.42s/it]                                                 40%|████      | 56/140 [01:47<01:58,  1.42s/it]int64
{'eval_loss': 0.09848804026842117, 'eval_rouge1': 70.8964, 'eval_rouge2': 63.3857, 'eval_rougeL': 70.9143, 'eval_rougeLsum': 70.8932, 'eval_runtime': 1.7407, 'eval_samples_per_second': 20.682, 'eval_steps_per_second': 1.149, 'epoch': 3.0}
{'loss': 0.0294, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.33it/s][A                                                
                                             [A 40%|████      | 56/140 [01:49<01:58,  1.42s/it]
100%|██████████| 2/2 [00:01<00:00,  2.33it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [01:59<06:07,  4.43s/it] 41%|████▏     | 58/140 [02:00<04:48,  3.52s/it] 42%|████▏     | 59/140 [02:01<03:53,  2.88s/it] 43%|████▎     | 60/140 [02:03<03:14,  2.43s/it] 44%|████▎     | 61/140 [02:04<02:48,  2.13s/it] 44%|████▍     | 62/140 [02:06<02:29,  1.91s/it] 45%|████▌     | 63/140 [02:07<02:14,  1.75s/it] 46%|████▌     | 64/140 [02:08<02:04,  1.64s/it] 46%|████▋     | 65/140 [02:10<01:57,  1.57s/it] 47%|████▋     | 66/140 [02:11<01:51,  1.51s/it] 48%|████▊     | 67/140 [02:12<01:48,  1.48s/it] 49%|████▊     | 68/140 [02:14<01:44,  1.45s/it] 49%|████▉     | 69/140 [02:15<01:42,  1.44s/it] 50%|█████     | 70/140 [02:17<01:39,  1.42s/it]                                                 50%|█████     | 70/140 [02:17<01:39,  1.42s/it]int64
{'eval_loss': 0.059546563774347305, 'eval_rouge1': 71.904, 'eval_rouge2': 64.6806, 'eval_rougeL': 71.4212, 'eval_rougeLsum': 71.3973, 'eval_runtime': 1.8152, 'eval_samples_per_second': 19.832, 'eval_steps_per_second': 1.102, 'epoch': 4.0}
{'loss': 0.017, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.38it/s][A                                                
                                             [A 50%|█████     | 70/140 [02:18<01:39,  1.42s/it]
100%|██████████| 2/2 [00:01<00:00,  2.38it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [02:31<05:57,  5.18s/it] 51%|█████▏    | 72/140 [02:32<04:35,  4.05s/it] 52%|█████▏    | 73/140 [02:33<03:37,  3.25s/it] 53%|█████▎    | 74/140 [02:35<02:57,  2.68s/it] 54%|█████▎    | 75/140 [02:36<02:29,  2.30s/it] 54%|█████▍    | 76/140 [02:38<02:09,  2.02s/it] 55%|█████▌    | 77/140 [02:39<01:55,  1.83s/it] 56%|█████▌    | 78/140 [02:40<01:44,  1.69s/it] 56%|█████▋    | 79/140 [02:42<01:37,  1.60s/it] 57%|█████▋    | 80/140 [02:43<01:32,  1.54s/it] 58%|█████▊    | 81/140 [02:44<01:28,  1.50s/it] 59%|█████▊    | 82/140 [02:46<01:24,  1.46s/it] 59%|█████▉    | 83/140 [02:47<01:22,  1.44s/it] 60%|██████    | 84/140 [02:49<01:19,  1.42s/it]                                                 60%|██████    | 84/140 [02:49<01:19,  1.42s/it]int64
{'eval_loss': 0.04297782480716705, 'eval_rouge1': 71.7522, 'eval_rouge2': 65.0728, 'eval_rougeL': 71.511, 'eval_rougeLsum': 71.4902, 'eval_runtime': 1.8006, 'eval_samples_per_second': 19.994, 'eval_steps_per_second': 1.111, 'epoch': 5.0}
{'loss': 0.0095, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.54it/s][A                                                
                                             [A 60%|██████    | 84/140 [02:50<01:19,  1.42s/it]
100%|██████████| 2/2 [00:00<00:00,  2.54it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [03:03<04:50,  5.29s/it] 61%|██████▏   | 86/140 [03:04<03:42,  4.12s/it] 62%|██████▏   | 87/140 [03:06<02:55,  3.30s/it] 63%|██████▎   | 88/140 [03:07<02:21,  2.73s/it] 64%|██████▎   | 89/140 [03:09<01:58,  2.33s/it] 64%|██████▍   | 90/140 [03:10<01:42,  2.05s/it] 65%|██████▌   | 91/140 [03:11<01:30,  1.86s/it] 66%|██████▌   | 92/140 [03:13<01:22,  1.71s/it] 66%|██████▋   | 93/140 [03:14<01:17,  1.64s/it] 67%|██████▋   | 94/140 [03:16<01:11,  1.56s/it] 68%|██████▊   | 95/140 [03:17<01:08,  1.51s/it] 69%|██████▊   | 96/140 [03:18<01:04,  1.48s/it] 69%|██████▉   | 97/140 [03:20<01:02,  1.45s/it] 70%|███████   | 98/140 [03:21<00:59,  1.43s/it]                                                 70%|███████   | 98/140 [03:21<00:59,  1.43s/it]int64
{'eval_loss': 0.046737585216760635, 'eval_rouge1': 71.266, 'eval_rouge2': 63.7907, 'eval_rougeL': 71.2698, 'eval_rougeLsum': 71.2426, 'eval_runtime': 1.6848, 'eval_samples_per_second': 21.368, 'eval_steps_per_second': 1.187, 'epoch': 6.0}
{'loss': 0.0081, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.34it/s][A                                                
                                             [A 70%|███████   | 98/140 [03:23<00:59,  1.43s/it]
100%|██████████| 2/2 [00:01<00:00,  2.34it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [03:29<02:23,  3.51s/it] 71%|███████▏  | 100/140 [03:31<01:54,  2.87s/it] 72%|███████▏  | 101/140 [03:32<01:34,  2.43s/it] 73%|███████▎  | 102/140 [03:34<01:20,  2.12s/it] 74%|███████▎  | 103/140 [03:35<01:10,  1.91s/it] 74%|███████▍  | 104/140 [03:36<01:03,  1.76s/it] 75%|███████▌  | 105/140 [03:38<00:57,  1.65s/it] 76%|███████▌  | 106/140 [03:39<00:53,  1.58s/it] 76%|███████▋  | 107/140 [03:41<00:50,  1.53s/it] 77%|███████▋  | 108/140 [03:42<00:47,  1.49s/it] 78%|███████▊  | 109/140 [03:43<00:45,  1.47s/it] 79%|███████▊  | 110/140 [03:45<00:43,  1.44s/it] 79%|███████▉  | 111/140 [03:46<00:41,  1.43s/it] 80%|████████  | 112/140 [03:48<00:39,  1.41s/it]                                                  80%|████████  | 112/140 [03:51<00:39,  1.41s/it]int64
{'eval_loss': 0.04211704432964325, 'eval_rouge1': 71.904, 'eval_rouge2': 64.6806, 'eval_rougeL': 71.4212, 'eval_rougeLsum': 71.3973, 'eval_runtime': 1.8178, 'eval_samples_per_second': 19.804, 'eval_steps_per_second': 1.1, 'epoch': 7.0}
{'loss': 0.0054, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  2.37it/s][A                                                 
                                             [A 80%|████████  | 112/140 [03:55<00:39,  1.41s/it]
100%|██████████| 2/2 [00:02<00:00,  2.37it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [04:00<02:10,  4.83s/it] 81%|████████▏ | 114/140 [04:02<01:38,  3.80s/it] 82%|████████▏ | 115/140 [04:03<01:16,  3.08s/it] 83%|████████▎ | 116/140 [04:05<01:02,  2.62s/it] 84%|████████▎ | 117/140 [04:06<00:51,  2.25s/it] 84%|████████▍ | 118/140 [04:08<00:43,  1.98s/it] 85%|████████▌ | 119/140 [04:09<00:37,  1.81s/it] 86%|████████▌ | 120/140 [04:10<00:33,  1.68s/it] 86%|████████▋ | 121/140 [04:12<00:30,  1.59s/it] 87%|████████▋ | 122/140 [04:13<00:27,  1.53s/it] 88%|████████▊ | 123/140 [04:14<00:25,  1.49s/it] 89%|████████▊ | 124/140 [04:16<00:23,  1.47s/it] 89%|████████▉ | 125/140 [04:17<00:22,  1.48s/it] 90%|█████████ | 126/140 [04:19<00:20,  1.45s/it]                                                  90%|█████████ | 126/140 [04:19<00:20,  1.45s/it]int64
{'eval_loss': 0.04408085718750954, 'eval_rouge1': 71.5416, 'eval_rouge2': 65.4762, 'eval_rougeL': 71.0815, 'eval_rougeLsum': 71.0817, 'eval_runtime': 3.3809, 'eval_samples_per_second': 10.648, 'eval_steps_per_second': 0.592, 'epoch': 8.0}
{'loss': 0.003, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.35it/s][A                                                 
                                             [A 90%|█████████ | 126/140 [04:21<00:20,  1.45s/it]
100%|██████████| 2/2 [00:01<00:00,  2.35it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [04:33<01:10,  5.40s/it] 91%|█████████▏| 128/140 [04:35<00:50,  4.20s/it] 92%|█████████▏| 129/140 [04:36<00:36,  3.36s/it] 93%|█████████▎| 130/140 [04:38<00:27,  2.77s/it] 94%|█████████▎| 131/140 [04:39<00:21,  2.36s/it] 94%|█████████▍| 132/140 [04:40<00:16,  2.07s/it] 95%|█████████▌| 133/140 [04:42<00:13,  1.87s/it] 96%|█████████▌| 134/140 [04:43<00:10,  1.72s/it] 96%|█████████▋| 135/140 [04:45<00:08,  1.63s/it] 97%|█████████▋| 136/140 [04:46<00:06,  1.56s/it] 98%|█████████▊| 137/140 [04:47<00:04,  1.51s/it] 99%|█████████▊| 138/140 [04:49<00:02,  1.48s/it] 99%|█████████▉| 139/140 [04:50<00:01,  1.46s/it]100%|██████████| 140/140 [04:52<00:00,  1.44s/it]                                                 100%|██████████| 140/140 [04:52<00:00,  1.44s/it]int64
{'eval_loss': 0.0452958345413208, 'eval_rouge1': 71.5416, 'eval_rouge2': 65.4762, 'eval_rougeL': 71.0815, 'eval_rougeLsum': 71.0817, 'eval_runtime': 1.8037, 'eval_samples_per_second': 19.959, 'eval_steps_per_second': 1.109, 'epoch': 9.0}
{'loss': 0.003, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.34it/s][A                                                 
                                             [A100%|██████████| 140/140 [04:53<00:00,  1.44s/it]
100%|██████████| 2/2 [00:01<00:00,  2.34it/s][A
                                             [A                                                 100%|██████████| 140/140 [05:13<00:00,  1.44s/it]100%|██████████| 140/140 [05:13<00:00,  2.24s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.04573160782456398, 'eval_rouge1': 71.5416, 'eval_rouge2': 65.4762, 'eval_rougeL': 71.0815, 'eval_rougeLsum': 71.0817, 'eval_runtime': 1.8052, 'eval_samples_per_second': 19.943, 'eval_steps_per_second': 1.108, 'epoch': 10.0}
{'train_runtime': 321.733, 'train_samples_per_second': 9.884, 'train_steps_per_second': 0.435, 'train_loss': 1.1950793569375362, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▃▆██▇█▇▇▇
wandb:                    eval/rouge2 ▁▂▂▆▇▃▆███
wandb:                    eval/rougeL ▁▄▇██▇█▇▇▇
wandb:                 eval/rougeLsum ▁▄▇██▇█▇▇▇
wandb:                   eval/runtime ▁▁▁▂▁▁▂█▁▁
wandb:        eval/samples_per_second ███▇▇█▇▁▇▇
wandb:          eval/steps_per_second ███▇▇█▇▁▇▇
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.04573
wandb:                    eval/rouge1 71.5416
wandb:                    eval/rouge2 65.4762
wandb:                    eval/rougeL 71.0815
wandb:                 eval/rougeLsum 71.0817
wandb:                   eval/runtime 1.8052
wandb:        eval/samples_per_second 19.943
wandb:          eval/steps_per_second 1.108
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.003
wandb:               train/total_flos 68047761899520.0
wandb:               train/train_loss 1.19508
wandb:            train/train_runtime 321.733
wandb: train/train_samples_per_second 9.884
wandb:   train/train_steps_per_second 0.435
wandb: 
wandb: 🚀 View run flan-t5-base_ep-10_ga-1_b-4_lr-0.0003 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/m1l6195e
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_215614-m1l6195e/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_220215-2uhaqtz9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-base_ep-10_ga-1_b-4_lr-0.0003
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/2uhaqtz9
################################################ t5-base_ep-10_ga-1_b-4_lr-0.0003 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:11<26:46, 11.56s/it]  1%|▏         | 2/140 [00:12<12:38,  5.49s/it]  2%|▏         | 3/140 [00:14<08:07,  3.56s/it]  3%|▎         | 4/140 [00:15<05:58,  2.64s/it]  4%|▎         | 5/140 [00:16<04:48,  2.14s/it]  4%|▍         | 6/140 [00:17<04:05,  1.83s/it]  5%|▌         | 7/140 [00:19<03:38,  1.64s/it]  6%|▌         | 8/140 [00:20<03:18,  1.51s/it]  6%|▋         | 9/140 [00:21<03:06,  1.42s/it]  7%|▋         | 10/140 [00:22<02:57,  1.36s/it]  8%|▊         | 11/140 [00:23<02:50,  1.32s/it]  9%|▊         | 12/140 [00:25<02:46,  1.30s/it]  9%|▉         | 13/140 [00:26<02:42,  1.28s/it] 10%|█         | 14/140 [00:27<02:39,  1.27s/it]                                                 10%|█         | 14/140 [00:27<02:39,  1.27s/it]{'loss': 2.6516, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.42it/s][A                                                
                                             [A 10%|█         | 14/140 [00:29<02:39,  1.27s/it]
100%|██████████| 2/2 [00:01<00:00,  2.42it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [00:34<06:14,  2.99s/it] 11%|█▏        | 16/140 [00:35<05:05,  2.46s/it] 12%|█▏        | 17/140 [00:37<04:17,  2.09s/it] 13%|█▎        | 18/140 [00:38<03:44,  1.84s/it] 14%|█▎        | 19/140 [00:39<03:20,  1.66s/it] 14%|█▍        | 20/140 [00:40<03:04,  1.54s/it] 15%|█▌        | 21/140 [00:42<02:51,  1.44s/it] 16%|█▌        | 22/140 [00:43<02:41,  1.37s/it] 16%|█▋        | 23/140 [00:44<02:35,  1.33s/it] 17%|█▋        | 24/140 [00:45<02:30,  1.30s/it] 18%|█▊        | 25/140 [00:47<02:27,  1.28s/it] 19%|█▊        | 26/140 [00:48<02:24,  1.26s/it] 19%|█▉        | 27/140 [00:49<02:21,  1.25s/it] 20%|██        | 28/140 [00:50<02:19,  1.25s/it]                                                 20%|██        | 28/140 [00:50<02:19,  1.25s/it]int64
{'eval_loss': 0.5700773596763611, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 1.8061, 'eval_samples_per_second': 19.932, 'eval_steps_per_second': 1.107, 'epoch': 1.0}
{'loss': 0.2899, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.95it/s][A                                                
                                             [A 20%|██        | 28/140 [00:57<02:19,  1.25s/it]
100%|██████████| 2/2 [00:06<00:00,  2.95it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [01:03<08:41,  4.70s/it] 21%|██▏       | 30/140 [01:04<06:41,  3.65s/it] 22%|██▏       | 31/140 [01:05<05:19,  2.93s/it] 23%|██▎       | 32/140 [01:07<04:21,  2.42s/it] 24%|██▎       | 33/140 [01:08<03:40,  2.06s/it] 24%|██▍       | 34/140 [01:09<03:12,  1.81s/it] 25%|██▌       | 35/140 [01:10<02:51,  1.63s/it] 26%|██▌       | 36/140 [01:12<02:37,  1.52s/it] 26%|██▋       | 37/140 [01:13<02:27,  1.43s/it] 27%|██▋       | 38/140 [01:14<02:20,  1.37s/it] 28%|██▊       | 39/140 [01:15<02:13,  1.32s/it] 29%|██▊       | 40/140 [01:16<02:10,  1.30s/it] 29%|██▉       | 41/140 [01:18<02:07,  1.28s/it] 30%|███       | 42/140 [01:19<02:03,  1.26s/it]                                                 30%|███       | 42/140 [01:19<02:03,  1.26s/it]int64
{'eval_loss': 0.07220086455345154, 'eval_rouge1': 69.2126, 'eval_rouge2': 62.4324, 'eval_rougeL': 69.2521, 'eval_rougeLsum': 69.2126, 'eval_runtime': 7.2949, 'eval_samples_per_second': 4.935, 'eval_steps_per_second': 0.274, 'epoch': 2.0}
{'loss': 0.0337, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A                                                
                                             [A 30%|███       | 42/140 [01:20<02:03,  1.26s/it]
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [01:33<08:18,  5.14s/it] 31%|███▏      | 44/140 [01:34<06:20,  3.97s/it] 32%|███▏      | 45/140 [01:36<04:58,  3.14s/it] 33%|███▎      | 46/140 [01:37<04:01,  2.57s/it] 34%|███▎      | 47/140 [01:38<03:21,  2.17s/it] 34%|███▍      | 48/140 [01:39<02:53,  1.89s/it] 35%|███▌      | 49/140 [01:41<02:34,  1.70s/it] 36%|███▌      | 50/140 [01:42<02:20,  1.56s/it] 36%|███▋      | 51/140 [01:43<02:10,  1.47s/it] 37%|███▋      | 52/140 [01:44<02:02,  1.39s/it] 38%|███▊      | 53/140 [01:45<01:56,  1.34s/it] 39%|███▊      | 54/140 [01:47<01:52,  1.31s/it] 39%|███▉      | 55/140 [01:48<01:49,  1.28s/it] 40%|████      | 56/140 [01:49<01:46,  1.27s/it]                                                 40%|████      | 56/140 [01:49<01:46,  1.27s/it]int64
{'eval_loss': 0.026170650497078896, 'eval_rouge1': 71.2118, 'eval_rouge2': 65.1222, 'eval_rougeL': 70.7261, 'eval_rougeLsum': 70.7073, 'eval_runtime': 1.528, 'eval_samples_per_second': 23.56, 'eval_steps_per_second': 1.309, 'epoch': 3.0}
{'loss': 0.0162, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.83it/s][A                                                
                                             [A 40%|████      | 56/140 [01:51<01:46,  1.27s/it]
100%|██████████| 2/2 [00:00<00:00,  2.83it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [02:07<08:39,  6.25s/it] 41%|████▏     | 58/140 [02:08<06:29,  4.75s/it] 42%|████▏     | 59/140 [02:09<04:58,  3.69s/it] 43%|████▎     | 60/140 [02:11<03:55,  2.95s/it] 44%|████▎     | 61/140 [02:12<03:12,  2.44s/it] 44%|████▍     | 62/140 [02:13<02:42,  2.08s/it] 45%|████▌     | 63/140 [02:14<02:20,  1.83s/it] 46%|████▌     | 64/140 [02:16<02:05,  1.66s/it] 46%|████▋     | 65/140 [02:17<01:55,  1.54s/it] 47%|████▋     | 66/140 [02:18<01:46,  1.44s/it] 48%|████▊     | 67/140 [02:19<01:40,  1.38s/it] 49%|████▊     | 68/140 [02:21<01:36,  1.34s/it] 49%|████▉     | 69/140 [02:22<01:32,  1.31s/it] 50%|█████     | 70/140 [02:23<01:29,  1.27s/it]                                                 50%|█████     | 70/140 [02:23<01:29,  1.27s/it]int64
{'eval_loss': 0.02144768089056015, 'eval_rouge1': 71.078, 'eval_rouge2': 65.5203, 'eval_rougeL': 70.8394, 'eval_rougeLsum': 70.8576, 'eval_runtime': 1.5736, 'eval_samples_per_second': 22.878, 'eval_steps_per_second': 1.271, 'epoch': 4.0}
{'loss': 0.0141, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A                                                
                                             [A 50%|█████     | 70/140 [02:25<01:29,  1.27s/it]
100%|██████████| 2/2 [00:00<00:00,  2.84it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [02:38<06:13,  5.42s/it] 51%|█████▏    | 72/140 [02:39<04:42,  4.16s/it] 52%|█████▏    | 73/140 [02:41<03:40,  3.29s/it] 53%|█████▎    | 74/140 [02:42<02:56,  2.68s/it] 54%|█████▎    | 75/140 [02:43<02:25,  2.24s/it] 54%|█████▍    | 76/140 [02:44<02:04,  1.94s/it] 55%|█████▌    | 77/140 [02:46<01:48,  1.72s/it] 56%|█████▌    | 78/140 [02:47<01:37,  1.57s/it] 56%|█████▋    | 79/140 [02:48<01:28,  1.46s/it] 57%|█████▋    | 80/140 [02:49<01:22,  1.38s/it] 58%|█████▊    | 81/140 [02:50<01:18,  1.33s/it] 59%|█████▊    | 82/140 [02:52<01:15,  1.30s/it] 59%|█████▉    | 83/140 [02:53<01:12,  1.28s/it] 60%|██████    | 84/140 [02:54<01:10,  1.26s/it]                                                 60%|██████    | 84/140 [02:54<01:10,  1.26s/it]int64
{'eval_loss': 0.03013024479150772, 'eval_rouge1': 71.078, 'eval_rouge2': 65.5203, 'eval_rougeL': 70.8394, 'eval_rougeLsum': 70.8576, 'eval_runtime': 1.5721, 'eval_samples_per_second': 22.9, 'eval_steps_per_second': 1.272, 'epoch': 5.0}
{'loss': 0.0069, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.99it/s][A                                                
                                             [A 60%|██████    | 84/140 [02:56<01:10,  1.26s/it]
100%|██████████| 2/2 [00:00<00:00,  2.99it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [03:01<02:45,  3.01s/it] 61%|██████▏   | 86/140 [03:02<02:13,  2.47s/it] 62%|██████▏   | 87/140 [03:04<01:51,  2.11s/it] 63%|██████▎   | 88/140 [03:05<01:35,  1.85s/it] 64%|██████▎   | 89/140 [03:06<01:25,  1.67s/it] 64%|██████▍   | 90/140 [03:07<01:16,  1.54s/it] 65%|██████▌   | 91/140 [03:09<01:11,  1.45s/it] 66%|██████▌   | 92/140 [03:10<01:06,  1.38s/it] 66%|██████▋   | 93/140 [03:16<02:07,  2.71s/it] 67%|██████▋   | 94/140 [03:17<01:44,  2.27s/it] 68%|██████▊   | 95/140 [03:18<01:27,  1.95s/it] 69%|██████▊   | 96/140 [03:19<01:16,  1.74s/it] 69%|██████▉   | 97/140 [03:21<01:08,  1.59s/it] 70%|███████   | 98/140 [03:22<01:02,  1.48s/it]                                                 70%|███████   | 98/140 [03:22<01:02,  1.48s/it]int64
{'eval_loss': 0.02805200219154358, 'eval_rouge1': 71.078, 'eval_rouge2': 65.5203, 'eval_rougeL': 70.8394, 'eval_rougeLsum': 70.8576, 'eval_runtime': 1.4872, 'eval_samples_per_second': 24.207, 'eval_steps_per_second': 1.345, 'epoch': 6.0}
{'loss': 0.0042, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.91it/s][A                                                
                                             [A 70%|███████   | 98/140 [03:23<01:02,  1.48s/it]
100%|██████████| 2/2 [00:00<00:00,  2.91it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [03:41<04:42,  6.88s/it] 71%|███████▏  | 100/140 [03:43<03:27,  5.19s/it] 72%|███████▏  | 101/140 [03:44<02:36,  4.00s/it] 73%|███████▎  | 102/140 [03:45<02:00,  3.17s/it] 74%|███████▎  | 103/140 [03:46<01:35,  2.59s/it] 74%|███████▍  | 104/140 [03:47<01:18,  2.18s/it] 75%|███████▌  | 105/140 [03:49<01:06,  1.91s/it] 76%|███████▌  | 106/140 [03:50<00:57,  1.70s/it] 76%|███████▋  | 107/140 [03:51<00:51,  1.57s/it] 77%|███████▋  | 108/140 [03:52<00:47,  1.47s/it] 78%|███████▊  | 109/140 [03:54<00:43,  1.40s/it] 79%|███████▊  | 110/140 [03:55<00:40,  1.35s/it] 79%|███████▉  | 111/140 [03:56<00:38,  1.31s/it] 80%|████████  | 112/140 [03:58<00:37,  1.35s/it]                                                  80%|████████  | 112/140 [03:58<00:37,  1.35s/it]int64
{'eval_loss': 0.019400056451559067, 'eval_rouge1': 71.078, 'eval_rouge2': 65.5203, 'eval_rougeL': 70.8394, 'eval_rougeLsum': 70.8576, 'eval_runtime': 1.5223, 'eval_samples_per_second': 23.648, 'eval_steps_per_second': 1.314, 'epoch': 7.0}
{'loss': 0.005, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.73it/s][A                                                 
                                             [A 80%|████████  | 112/140 [03:59<00:37,  1.35s/it]
100%|██████████| 2/2 [00:00<00:00,  2.73it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [04:21<03:39,  8.13s/it] 81%|████████▏ | 114/140 [04:23<02:37,  6.06s/it] 82%|████████▏ | 115/140 [04:24<01:55,  4.61s/it] 83%|████████▎ | 116/140 [04:25<01:26,  3.59s/it] 84%|████████▎ | 117/140 [04:26<01:06,  2.89s/it] 84%|████████▍ | 118/140 [04:28<00:52,  2.39s/it] 85%|████████▌ | 119/140 [04:29<00:43,  2.05s/it] 86%|████████▌ | 120/140 [04:30<00:36,  1.80s/it] 86%|████████▋ | 121/140 [04:31<00:31,  1.63s/it] 87%|████████▋ | 122/140 [04:33<00:27,  1.51s/it] 88%|████████▊ | 123/140 [04:34<00:24,  1.42s/it] 89%|████████▊ | 124/140 [04:35<00:21,  1.36s/it] 89%|████████▉ | 125/140 [04:36<00:19,  1.33s/it] 90%|█████████ | 126/140 [04:37<00:18,  1.29s/it]                                                  90%|█████████ | 126/140 [04:37<00:18,  1.29s/it]int64
{'eval_loss': 0.0171205997467041, 'eval_rouge1': 71.3943, 'eval_rouge2': 65.9392, 'eval_rougeL': 71.1799, 'eval_rougeLsum': 71.1749, 'eval_runtime': 1.5972, 'eval_samples_per_second': 22.54, 'eval_steps_per_second': 1.252, 'epoch': 8.0}
{'loss': 0.0032, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.74it/s][A                                                 
                                             [A 90%|█████████ | 126/140 [04:39<00:18,  1.29s/it]
100%|██████████| 2/2 [00:00<00:00,  2.74it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [04:45<00:40,  3.11s/it] 91%|█████████▏| 128/140 [04:46<00:30,  2.55s/it] 92%|█████████▏| 129/140 [04:47<00:23,  2.15s/it] 93%|█████████▎| 130/140 [04:49<00:18,  1.88s/it] 94%|█████████▎| 131/140 [04:50<00:15,  1.69s/it] 94%|█████████▍| 132/140 [04:51<00:12,  1.55s/it] 95%|█████████▌| 133/140 [04:52<00:10,  1.46s/it] 96%|█████████▌| 134/140 [04:53<00:08,  1.39s/it] 96%|█████████▋| 135/140 [04:55<00:06,  1.35s/it] 97%|█████████▋| 136/140 [05:04<00:15,  3.78s/it] 98%|█████████▊| 137/140 [05:05<00:09,  3.01s/it] 99%|█████████▊| 138/140 [05:07<00:04,  2.48s/it] 99%|█████████▉| 139/140 [05:08<00:02,  2.11s/it]100%|██████████| 140/140 [05:09<00:00,  1.84s/it]                                                 100%|██████████| 140/140 [05:09<00:00,  1.84s/it]int64
{'eval_loss': 0.01571507751941681, 'eval_rouge1': 71.3943, 'eval_rouge2': 65.9392, 'eval_rougeL': 71.1799, 'eval_rougeLsum': 71.1749, 'eval_runtime': 1.5982, 'eval_samples_per_second': 22.526, 'eval_steps_per_second': 1.251, 'epoch': 9.0}
{'loss': 0.0027, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00,  2.74it/s][A                                                 
                                             [A100%|██████████| 140/140 [05:11<00:00,  1.84s/it]
100%|██████████| 2/2 [00:00<00:00,  2.74it/s][A
                                             [A                                                 100%|██████████| 140/140 [05:15<00:00,  1.84s/it]100%|██████████| 140/140 [05:15<00:00,  2.25s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.015732672065496445, 'eval_rouge1': 71.3943, 'eval_rouge2': 65.9392, 'eval_rougeL': 71.1799, 'eval_rougeLsum': 71.1749, 'eval_runtime': 1.5948, 'eval_samples_per_second': 22.574, 'eval_steps_per_second': 1.254, 'epoch': 10.0}
{'train_runtime': 322.9707, 'train_samples_per_second': 9.846, 'train_steps_per_second': 0.433, 'train_loss': 0.30275783339249235, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁█████████
wandb:                    eval/rouge2 ▁█████████
wandb:                    eval/rougeL ▁█████████
wandb:                 eval/rougeLsum ▁█████████
wandb:                   eval/runtime ▁█▁▁▁▁▁▁▁▁
wandb:        eval/samples_per_second ▆▁█████▇▇▇
wandb:          eval/steps_per_second ▆▁█████▇▇▇
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01573
wandb:                    eval/rouge1 71.3943
wandb:                    eval/rouge2 65.9392
wandb:                    eval/rougeL 71.1799
wandb:                 eval/rougeLsum 71.1749
wandb:                   eval/runtime 1.5948
wandb:        eval/samples_per_second 22.574
wandb:          eval/steps_per_second 1.254
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0027
wandb:               train/total_flos 60515190374400.0
wandb:               train/train_loss 0.30276
wandb:            train/train_runtime 322.9707
wandb: train/train_samples_per_second 9.846
wandb:   train/train_steps_per_second 0.433
wandb: 
wandb: 🚀 View run t5-base_ep-10_ga-1_b-4_lr-0.0003 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/2uhaqtz9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_220215-2uhaqtz9/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_220826-shhduu7i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flan-t5-large_ep-10_ga-1_b-4_lr-0.0003
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/shhduu7i
################################################ flan-t5-large_ep-10_ga-1_b-4_lr-0.0003 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:14<34:36, 14.94s/it]  1%|▏         | 2/140 [00:19<19:56,  8.67s/it]  2%|▏         | 3/140 [00:23<15:15,  6.68s/it]  3%|▎         | 4/140 [00:27<12:58,  5.72s/it]  4%|▎         | 5/140 [00:32<11:42,  5.20s/it]  4%|▍         | 6/140 [00:36<10:56,  4.90s/it]  5%|▌         | 7/140 [00:40<10:23,  4.69s/it]  6%|▌         | 8/140 [00:44<10:01,  4.56s/it]  6%|▋         | 9/140 [00:49<09:44,  4.46s/it]  7%|▋         | 10/140 [00:53<09:32,  4.41s/it]  8%|▊         | 11/140 [00:57<09:23,  4.37s/it]  9%|▊         | 12/140 [01:01<09:13,  4.32s/it]  9%|▉         | 13/140 [01:06<09:06,  4.31s/it] 10%|█         | 14/140 [01:10<09:00,  4.29s/it]                                                 10%|█         | 14/140 [01:10<09:00,  4.29s/it]{'loss': 9.8793, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A
                                             [A                                                
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A 10%|█         | 14/140 [01:15<09:00,  4.29s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [02:19<49:32, 23.78s/it] 11%|█▏        | 16/140 [02:23<37:01, 17.91s/it] 12%|█▏        | 17/140 [02:27<28:17, 13.80s/it] 13%|█▎        | 18/140 [02:32<22:13, 10.93s/it] 14%|█▎        | 19/140 [02:36<18:01,  8.94s/it] 14%|█▍        | 20/140 [02:40<15:04,  7.53s/it] 15%|█▌        | 21/140 [02:45<12:59,  6.55s/it] 16%|█▌        | 22/140 [02:49<11:33,  5.88s/it] 16%|█▋        | 23/140 [02:53<10:32,  5.40s/it] 17%|█▋        | 24/140 [02:57<09:47,  5.06s/it] 18%|█▊        | 25/140 [03:02<09:15,  4.83s/it] 19%|█▊        | 26/140 [03:06<08:51,  4.67s/it] 19%|█▉        | 27/140 [03:10<08:33,  4.55s/it] 20%|██        | 28/140 [03:15<08:28,  4.54s/it]                                                 20%|██        | 28/140 [03:15<08:28,  4.54s/it]int64
{'eval_loss': 1.9403173923492432, 'eval_rouge1': 59.8837, 'eval_rouge2': 49.8508, 'eval_rougeL': 59.6469, 'eval_rougeLsum': 59.7003, 'eval_runtime': 4.7574, 'eval_samples_per_second': 7.567, 'eval_steps_per_second': 0.42, 'epoch': 1.0}
{'loss': 1.5472, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.14s/it][A                                                
                                             [A 20%|██        | 28/140 [03:20<08:28,  4.54s/it]
100%|██████████| 2/2 [00:02<00:00,  1.14s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [07:16<2:19:57, 75.65s/it] 21%|██▏       | 30/140 [07:21<1:39:25, 54.23s/it] 22%|██▏       | 31/140 [07:25<1:11:17, 39.24s/it] 23%|██▎       | 32/140 [07:29<51:45, 28.76s/it]   24%|██▎       | 33/140 [07:33<38:11, 21.42s/it] 24%|██▍       | 34/140 [07:38<28:43, 16.26s/it] 25%|██▌       | 35/140 [07:42<22:08, 12.65s/it] 26%|██▌       | 36/140 [07:57<23:23, 13.50s/it] 26%|██▋       | 37/140 [08:02<18:23, 10.71s/it] 27%|██▋       | 38/140 [08:06<14:55,  8.78s/it] 28%|██▊       | 39/140 [08:28<21:23, 12.71s/it] 29%|██▊       | 40/140 [08:32<16:56, 10.17s/it] 29%|██▉       | 41/140 [08:36<13:51,  8.40s/it] 30%|███       | 42/140 [08:41<11:45,  7.20s/it]                                                 30%|███       | 42/140 [08:41<11:45,  7.20s/it]int64
{'eval_loss': 0.16921333968639374, 'eval_rouge1': 69.9941, 'eval_rouge2': 63.2099, 'eval_rougeL': 69.9226, 'eval_rougeLsum': 69.9315, 'eval_runtime': 4.7752, 'eval_samples_per_second': 7.539, 'eval_steps_per_second': 0.419, 'epoch': 2.0}
{'loss': 0.1152, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A                                                
                                             [A 30%|███       | 42/140 [08:46<11:45,  7.20s/it]
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [09:48<40:59, 25.36s/it] 31%|███▏      | 44/140 [09:53<30:27, 19.03s/it] 32%|███▏      | 45/140 [09:57<23:05, 14.59s/it] 33%|███▎      | 46/140 [10:01<17:59, 11.49s/it] 34%|███▎      | 47/140 [10:05<14:26,  9.32s/it] 34%|███▍      | 48/140 [10:10<11:58,  7.81s/it] 35%|███▌      | 49/140 [10:14<10:13,  6.74s/it] 36%|███▌      | 50/140 [10:18<09:01,  6.01s/it] 36%|███▋      | 51/140 [10:22<08:07,  5.48s/it] 37%|███▋      | 52/140 [10:27<07:30,  5.12s/it] 38%|███▊      | 53/140 [10:33<07:52,  5.43s/it] 39%|███▊      | 54/140 [10:37<07:18,  5.10s/it] 39%|███▉      | 55/140 [10:42<06:53,  4.86s/it] 40%|████      | 56/140 [10:46<06:33,  4.68s/it]                                                 40%|████      | 56/140 [10:46<06:33,  4.68s/it]int64
{'eval_loss': 0.0463426299393177, 'eval_rouge1': 70.7878, 'eval_rouge2': 63.8459, 'eval_rougeL': 70.7365, 'eval_rougeLsum': 70.7267, 'eval_runtime': 4.8637, 'eval_samples_per_second': 7.402, 'eval_steps_per_second': 0.411, 'epoch': 3.0}
{'loss': 0.0203, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A                                                
                                             [A 40%|████      | 56/140 [10:51<06:33,  4.68s/it]
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [12:49<55:45, 40.31s/it] 41%|████▏     | 58/140 [12:54<40:19, 29.50s/it] 42%|████▏     | 59/140 [12:58<29:36, 21.93s/it] 43%|████▎     | 60/140 [13:02<22:10, 16.63s/it] 44%|████▎     | 61/140 [13:06<17:00, 12.91s/it] 44%|████▍     | 62/140 [13:11<13:24, 10.31s/it] 45%|████▌     | 63/140 [13:15<10:56,  8.52s/it] 46%|████▌     | 64/140 [13:19<09:12,  7.26s/it] 46%|████▋     | 65/140 [13:23<07:56,  6.36s/it] 47%|████▋     | 66/140 [13:29<07:41,  6.23s/it] 48%|████▊     | 67/140 [13:34<06:53,  5.66s/it] 49%|████▊     | 68/140 [13:38<06:17,  5.24s/it] 49%|████▉     | 69/140 [13:42<05:51,  4.95s/it] 50%|█████     | 70/140 [13:47<05:32,  4.74s/it]                                                 50%|█████     | 70/140 [13:47<05:32,  4.74s/it]int64
{'eval_loss': 0.054261188954114914, 'eval_rouge1': 69.3685, 'eval_rouge2': 61.1111, 'eval_rougeL': 69.1163, 'eval_rougeLsum': 69.0235, 'eval_runtime': 4.8178, 'eval_samples_per_second': 7.472, 'eval_steps_per_second': 0.415, 'epoch': 4.0}
{'loss': 0.0095, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A                                                
                                             [A 50%|█████     | 70/140 [13:51<05:32,  4.74s/it]
100%|██████████| 2/2 [00:02<00:00,  1.16s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [15:57<48:48, 42.44s/it] 51%|█████▏    | 72/140 [16:01<35:07, 30.99s/it] 52%|█████▏    | 73/140 [16:05<25:38, 22.97s/it] 53%|█████▎    | 74/140 [16:10<19:05, 17.35s/it] 54%|█████▎    | 75/140 [16:14<14:32, 13.43s/it] 54%|█████▍    | 76/140 [16:18<11:22, 10.67s/it] 55%|█████▌    | 77/140 [16:22<09:10,  8.74s/it] 56%|█████▌    | 78/140 [16:27<07:38,  7.39s/it] 56%|█████▋    | 79/140 [16:31<06:33,  6.45s/it] 57%|█████▋    | 80/140 [16:46<09:07,  9.12s/it] 58%|█████▊    | 81/140 [16:51<07:32,  7.66s/it] 59%|█████▊    | 82/140 [16:55<06:25,  6.65s/it] 59%|█████▉    | 83/140 [16:59<05:38,  5.93s/it] 60%|██████    | 84/140 [17:03<05:04,  5.43s/it]                                                 60%|██████    | 84/140 [17:03<05:04,  5.43s/it]int64
{'eval_loss': 0.04683495685458183, 'eval_rouge1': 70.1315, 'eval_rouge2': 62.092, 'eval_rougeL': 69.8982, 'eval_rougeLsum': 69.8108, 'eval_runtime': 4.8329, 'eval_samples_per_second': 7.449, 'eval_steps_per_second': 0.414, 'epoch': 5.0}
{'loss': 0.0066, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A                                                
                                             [A 60%|██████    | 84/140 [17:08<05:04,  5.43s/it]
100%|██████████| 2/2 [00:02<00:00,  1.18s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [18:04<20:09, 22.00s/it] 61%|██████▏   | 86/140 [18:09<15:05, 16.77s/it] 62%|██████▏   | 87/140 [18:13<11:37, 13.16s/it] 63%|██████▎   | 88/140 [18:18<09:14, 10.66s/it] 64%|██████▎   | 89/140 [18:23<07:35,  8.93s/it] 64%|██████▍   | 90/140 [18:28<06:25,  7.70s/it] 65%|██████▌   | 91/140 [18:33<05:35,  6.86s/it] 66%|██████▌   | 92/140 [18:38<04:58,  6.23s/it] 66%|██████▋   | 93/140 [18:49<06:07,  7.83s/it] 67%|██████▋   | 94/140 [18:53<05:10,  6.75s/it] 68%|██████▊   | 95/140 [18:58<04:30,  6.01s/it] 69%|██████▊   | 96/140 [19:02<04:01,  5.48s/it] 69%|██████▉   | 97/140 [19:06<03:39,  5.11s/it] 70%|███████   | 98/140 [19:10<03:23,  4.84s/it]                                                 70%|███████   | 98/140 [19:10<03:23,  4.84s/it]int64
{'eval_loss': 0.044190675020217896, 'eval_rouge1': 69.8117, 'eval_rouge2': 63.2256, 'eval_rougeL': 69.5434, 'eval_rougeLsum': 69.4987, 'eval_runtime': 4.8656, 'eval_samples_per_second': 7.399, 'eval_steps_per_second': 0.411, 'epoch': 6.0}
{'loss': 0.0016, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.22s/it][A                                                
                                             [A 70%|███████   | 98/140 [19:15<03:23,  4.84s/it]
100%|██████████| 2/2 [00:02<00:00,  1.22s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [20:09<14:22, 21.03s/it] 71%|███████▏  | 100/140 [20:13<10:39, 15.98s/it] 72%|███████▏  | 101/140 [20:17<08:05, 12.44s/it] 73%|███████▎  | 102/140 [20:22<06:19,  9.99s/it] 74%|███████▎  | 103/140 [20:27<05:12,  8.45s/it] 74%|███████▍  | 104/140 [20:32<04:29,  7.49s/it] 75%|███████▌  | 105/140 [20:37<03:59,  6.85s/it] 76%|███████▌  | 106/140 [20:42<03:36,  6.38s/it] 76%|███████▋  | 107/140 [20:51<03:51,  7.03s/it] 77%|███████▋  | 108/140 [20:57<03:32,  6.64s/it] 78%|███████▊  | 109/140 [21:02<03:17,  6.36s/it] 79%|███████▊  | 110/140 [21:08<03:07,  6.24s/it] 79%|███████▉  | 111/140 [21:14<02:56,  6.07s/it] 80%|████████  | 112/140 [21:20<02:45,  5.93s/it]                                                  80%|████████  | 112/140 [21:20<02:45,  5.93s/it]int64
{'eval_loss': 0.04988348111510277, 'eval_rouge1': 70.8415, 'eval_rouge2': 62.963, 'eval_rougeL': 70.3886, 'eval_rougeLsum': 70.3116, 'eval_runtime': 4.9499, 'eval_samples_per_second': 7.273, 'eval_steps_per_second': 0.404, 'epoch': 7.0}
{'loss': 0.003, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:03<00:00,  1.71s/it][A                                                 
                                             [A 80%|████████  | 112/140 [21:27<02:45,  5.93s/it]
100%|██████████| 2/2 [00:03<00:00,  1.71s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [23:22<18:23, 40.85s/it] 81%|████████▏ | 114/140 [23:26<12:56, 29.88s/it] 82%|████████▏ | 115/140 [23:31<09:14, 22.19s/it] 83%|████████▎ | 116/140 [23:35<06:43, 16.80s/it] 84%|████████▎ | 117/140 [23:39<04:59, 13.03s/it] 84%|████████▍ | 118/140 [23:43<03:48, 10.38s/it] 85%|████████▌ | 119/140 [23:48<02:59,  8.55s/it] 86%|████████▌ | 120/140 [23:52<02:25,  7.26s/it] 86%|████████▋ | 121/140 [23:56<02:00,  6.35s/it] 87%|████████▋ | 122/140 [24:00<01:44,  5.79s/it] 88%|████████▊ | 123/140 [24:05<01:30,  5.33s/it] 89%|████████▊ | 124/140 [24:13<01:39,  6.25s/it] 89%|████████▉ | 125/140 [24:17<01:24,  5.65s/it] 90%|█████████ | 126/140 [24:22<01:13,  5.24s/it]                                                  90%|█████████ | 126/140 [24:22<01:13,  5.24s/it]int64
{'eval_loss': 0.0495540052652359, 'eval_rouge1': 69.5459, 'eval_rouge2': 62.5934, 'eval_rougeL': 68.9291, 'eval_rougeLsum': 69.0133, 'eval_runtime': 6.9354, 'eval_samples_per_second': 5.191, 'eval_steps_per_second': 0.288, 'epoch': 8.0}
{'loss': 0.0026, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A                                                 
                                             [A 90%|█████████ | 126/140 [24:26<01:13,  5.24s/it]
100%|██████████| 2/2 [00:02<00:00,  1.15s/it][A
                                             [A                                                  90%|█████████ | 126/140 [25:42<01:13,  5.24s/it] 90%|█████████ | 126/140 [25:43<02:51, 12.25s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.05327357351779938, 'eval_rouge1': 70.9068, 'eval_rouge2': 63.4259, 'eval_rougeL': 70.6436, 'eval_rougeLsum': 70.6314, 'eval_runtime': 4.7901, 'eval_samples_per_second': 7.516, 'eval_steps_per_second': 0.418, 'epoch': 9.0}
{'train_runtime': 1549.9806, 'train_samples_per_second': 2.052, 'train_steps_per_second': 0.09, 'train_loss': 1.2872700360381888, 'epoch': 9.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁▇█▇█▇█▇█
wandb:                    eval/rouge2 ▁██▇▇██▇█
wandb:                    eval/rougeL ▁▇█▇▇▇█▇█
wandb:                 eval/rougeLsum ▁▇█▇▇▇█▇█
wandb:                   eval/runtime ▁▁▁▁▁▁▂█▁
wandb:        eval/samples_per_second ██████▇▁█
wandb:          eval/steps_per_second ██████▇▁█
wandb:                    train/epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███
wandb:            train/learning_rate █▇▆▅▅▄▃▂▁
wandb:                     train/loss █▂▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.05327
wandb:                    eval/rouge1 70.9068
wandb:                    eval/rouge2 63.4259
wandb:                    eval/rougeL 70.6436
wandb:                 eval/rougeLsum 70.6314
wandb:                   eval/runtime 4.7901
wandb:        eval/samples_per_second 7.516
wandb:          eval/steps_per_second 0.418
wandb:                    train/epoch 9.0
wandb:              train/global_step 126
wandb:            train/learning_rate 3e-05
wandb:                     train/loss 0.0026
wandb:               train/total_flos 206132683603968.0
wandb:               train/train_loss 1.28727
wandb:            train/train_runtime 1549.9806
wandb: train/train_samples_per_second 2.052
wandb:   train/train_steps_per_second 0.09
wandb: 
wandb: 🚀 View run flan-t5-large_ep-10_ga-1_b-4_lr-0.0003 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/shhduu7i
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_220826-shhduu7i/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_223528-wcwm6gh8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t5-large_ep-10_ga-1_b-4_lr-0.0003
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/wcwm6gh8
################################################ t5-large_ep-10_ga-1_b-4_lr-0.0003 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:14<34:41, 14.98s/it]  1%|▏         | 2/140 [00:18<19:32,  8.50s/it]  2%|▏         | 3/140 [00:22<14:40,  6.43s/it]  3%|▎         | 4/140 [00:26<12:21,  5.45s/it]  4%|▎         | 5/140 [00:30<11:02,  4.91s/it]  4%|▍         | 6/140 [00:34<10:12,  4.57s/it]  5%|▌         | 7/140 [00:38<09:40,  4.37s/it]  6%|▌         | 8/140 [00:42<09:19,  4.24s/it]  6%|▋         | 9/140 [00:46<09:01,  4.14s/it]  7%|▋         | 10/140 [00:50<08:49,  4.07s/it]  8%|▊         | 11/140 [00:54<08:41,  4.04s/it]  9%|▊         | 12/140 [00:58<08:37,  4.04s/it]  9%|▉         | 13/140 [01:02<08:30,  4.02s/it] 10%|█         | 14/140 [01:06<08:23,  4.00s/it]                                                 10%|█         | 14/140 [01:06<08:23,  4.00s/it]{'loss': 4.2132, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A
                                             [A                                                
100%|██████████| 2/2 [00:02<00:00,  1.17s/it][A 10%|█         | 14/140 [01:11<08:23,  4.00s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [02:21<53:03, 25.47s/it] 11%|█▏        | 16/140 [02:25<39:15, 18.99s/it] 12%|█▏        | 17/140 [02:37<34:19, 16.74s/it] 13%|█▎        | 18/140 [02:41<26:13, 12.90s/it] 14%|█▎        | 19/140 [02:45<20:43, 10.28s/it] 14%|█▍        | 20/140 [02:49<16:45,  8.38s/it] 15%|█▌        | 21/140 [02:53<13:59,  7.05s/it] 16%|█▌        | 22/140 [02:57<12:03,  6.13s/it] 16%|█▋        | 23/140 [03:01<10:42,  5.49s/it] 17%|█▋        | 24/140 [03:05<09:42,  5.02s/it] 18%|█▊        | 25/140 [03:08<09:00,  4.70s/it] 19%|█▊        | 26/140 [03:13<08:37,  4.54s/it] 19%|█▉        | 27/140 [03:25<12:52,  6.84s/it] 20%|██        | 28/140 [03:29<11:07,  5.96s/it]                                                 20%|██        | 28/140 [03:29<11:07,  5.96s/it]int64
{'eval_loss': 0.425707072019577, 'eval_rouge1': 9.6459, 'eval_rouge2': 8.49, 'eval_rougeL': 9.9325, 'eval_rougeLsum': 9.6459, 'eval_runtime': 4.832, 'eval_samples_per_second': 7.45, 'eval_steps_per_second': 0.414, 'epoch': 1.0}
{'loss': 0.1381, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A                                                
                                             [A 20%|██        | 28/140 [03:33<11:07,  5.96s/it]
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [05:16<1:07:19, 36.39s/it] 21%|██▏       | 30/140 [05:20<48:52, 26.66s/it]   22%|██▏       | 31/140 [05:24<36:03, 19.85s/it] 23%|██▎       | 32/140 [05:28<27:08, 15.08s/it] 24%|██▎       | 33/140 [05:32<20:55, 11.73s/it] 24%|██▍       | 34/140 [05:36<16:35,  9.39s/it] 25%|██▌       | 35/140 [05:40<13:35,  7.77s/it] 26%|██▌       | 36/140 [05:44<11:29,  6.63s/it] 26%|██▋       | 37/140 [05:48<10:00,  5.83s/it] 27%|██▋       | 38/140 [06:00<13:03,  7.68s/it] 28%|██▊       | 39/140 [06:04<11:04,  6.58s/it] 29%|██▊       | 40/140 [06:08<09:40,  5.80s/it] 29%|██▉       | 41/140 [06:12<08:39,  5.25s/it] 30%|███       | 42/140 [06:16<07:55,  4.85s/it]                                                 30%|███       | 42/140 [06:16<07:55,  4.85s/it]int64
{'eval_loss': 0.029777441173791885, 'eval_rouge1': 71.1946, 'eval_rouge2': 65.9379, 'eval_rougeL': 71.206, 'eval_rougeLsum': 71.1835, 'eval_runtime': 4.4497, 'eval_samples_per_second': 8.091, 'eval_steps_per_second': 0.449, 'epoch': 2.0}
{'loss': 0.0275, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A                                                
                                             [A 30%|███       | 42/140 [06:20<07:55,  4.85s/it]
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [07:00<26:58, 16.69s/it] 31%|███▏      | 44/140 [07:04<20:35, 12.86s/it] 32%|███▏      | 45/140 [07:08<16:09, 10.21s/it] 33%|███▎      | 46/140 [07:12<13:03,  8.34s/it] 34%|███▎      | 47/140 [07:16<10:52,  7.02s/it] 34%|███▍      | 48/140 [07:20<09:20,  6.09s/it] 35%|███▌      | 49/140 [07:24<08:14,  5.44s/it] 36%|███▌      | 50/140 [07:28<07:29,  4.99s/it] 36%|███▋      | 51/140 [07:32<06:56,  4.68s/it] 37%|███▋      | 52/140 [07:39<07:58,  5.44s/it] 38%|███▊      | 53/140 [07:43<07:14,  4.99s/it] 39%|███▊      | 54/140 [07:47<06:43,  4.69s/it] 39%|███▉      | 55/140 [07:51<06:19,  4.47s/it] 40%|████      | 56/140 [07:55<06:00,  4.29s/it]                                                 40%|████      | 56/140 [07:55<06:00,  4.29s/it]int64
{'eval_loss': 0.02418437972664833, 'eval_rouge1': 71.0757, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8559, 'eval_rougeLsum': 70.857, 'eval_runtime': 4.436, 'eval_samples_per_second': 8.115, 'eval_steps_per_second': 0.451, 'epoch': 3.0}
{'loss': 0.0107, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A                                                
                                             [A 40%|████      | 56/140 [07:59<06:00,  4.29s/it]
100%|██████████| 2/2 [00:02<00:00,  1.07s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [09:17<38:25, 27.78s/it] 41%|████▏     | 58/140 [09:21<28:16, 20.68s/it] 42%|████▏     | 59/140 [09:25<21:08, 15.66s/it] 43%|████▎     | 60/140 [09:29<16:12, 12.16s/it] 44%|████▎     | 61/140 [09:33<12:46,  9.70s/it] 44%|████▍     | 62/140 [09:37<10:21,  7.97s/it] 45%|████▌     | 63/140 [09:41<08:40,  6.77s/it] 46%|████▌     | 64/140 [09:45<07:30,  5.93s/it] 46%|████▋     | 65/140 [09:49<06:39,  5.32s/it] 47%|████▋     | 66/140 [09:53<06:01,  4.89s/it] 48%|████▊     | 67/140 [09:57<05:36,  4.61s/it] 49%|████▊     | 68/140 [10:06<07:00,  5.84s/it] 49%|████▉     | 69/140 [10:10<06:15,  5.29s/it] 50%|█████     | 70/140 [10:13<05:40,  4.87s/it]                                                 50%|█████     | 70/140 [10:13<05:40,  4.87s/it]int64
{'eval_loss': 0.023530304431915283, 'eval_rouge1': 71.0757, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8559, 'eval_rougeLsum': 70.857, 'eval_runtime': 4.4242, 'eval_samples_per_second': 8.137, 'eval_steps_per_second': 0.452, 'epoch': 4.0}
{'loss': 0.0147, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A                                                
                                             [A 50%|█████     | 70/140 [10:18<05:40,  4.87s/it]
100%|██████████| 2/2 [00:02<00:00,  1.06s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [10:45<14:39, 12.75s/it] 51%|█████▏    | 72/140 [10:48<11:26, 10.10s/it] 52%|█████▏    | 73/140 [10:52<09:12,  8.24s/it] 53%|█████▎    | 74/140 [10:56<07:39,  6.96s/it] 54%|█████▎    | 75/140 [11:00<06:33,  6.06s/it] 54%|█████▍    | 76/140 [11:04<05:45,  5.41s/it] 55%|█████▌    | 77/140 [11:08<05:12,  4.96s/it] 56%|█████▌    | 78/140 [11:12<04:48,  4.66s/it] 56%|█████▋    | 79/140 [11:16<04:32,  4.46s/it] 57%|█████▋    | 80/140 [11:29<07:09,  7.15s/it] 58%|█████▊    | 81/140 [11:33<06:04,  6.18s/it] 59%|█████▊    | 82/140 [11:37<05:20,  5.53s/it] 59%|█████▉    | 83/140 [11:41<04:48,  5.06s/it] 60%|██████    | 84/140 [11:45<04:23,  4.71s/it]                                                 60%|██████    | 84/140 [11:45<04:23,  4.71s/it]int64
{'eval_loss': 0.023426376283168793, 'eval_rouge1': 71.0757, 'eval_rouge2': 65.519, 'eval_rougeL': 70.8559, 'eval_rougeLsum': 70.857, 'eval_runtime': 4.3991, 'eval_samples_per_second': 8.184, 'eval_steps_per_second': 0.455, 'epoch': 5.0}
{'loss': 0.0027, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.11s/it][A                                                
                                             [A 60%|██████    | 84/140 [11:50<04:23,  4.71s/it]
100%|██████████| 2/2 [00:02<00:00,  1.11s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [13:23<30:02, 32.77s/it] 61%|██████▏   | 86/140 [13:27<21:42, 24.12s/it] 62%|██████▏   | 87/140 [13:31<15:58, 18.08s/it] 63%|██████▎   | 88/140 [13:35<12:00, 13.86s/it] 64%|██████▎   | 89/140 [13:39<09:14, 10.88s/it] 64%|██████▍   | 90/140 [13:43<07:20,  8.80s/it] 65%|██████▌   | 91/140 [13:47<05:59,  7.35s/it] 66%|██████▌   | 92/140 [13:51<05:04,  6.34s/it] 66%|██████▋   | 93/140 [13:55<04:24,  5.63s/it] 67%|██████▋   | 94/140 [14:05<05:11,  6.77s/it] 68%|██████▊   | 95/140 [14:09<04:27,  5.94s/it] 69%|██████▊   | 96/140 [14:13<03:55,  5.35s/it] 69%|██████▉   | 97/140 [14:17<03:32,  4.95s/it] 70%|███████   | 98/140 [14:21<03:14,  4.64s/it]                                                 70%|███████   | 98/140 [14:21<03:14,  4.64s/it]int64
{'eval_loss': 0.01249845139682293, 'eval_rouge1': 71.3842, 'eval_rouge2': 65.9088, 'eval_rougeL': 71.176, 'eval_rougeLsum': 71.1847, 'eval_runtime': 4.5822, 'eval_samples_per_second': 7.857, 'eval_steps_per_second': 0.436, 'epoch': 6.0}
{'loss': 0.0035, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.12s/it][A                                                
                                             [A 70%|███████   | 98/140 [14:25<03:14,  4.64s/it]
100%|██████████| 2/2 [00:02<00:00,  1.12s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [15:42<18:51, 27.59s/it] 71%|███████▏  | 100/140 [15:46<13:40, 20.50s/it] 72%|███████▏  | 101/140 [15:50<10:06, 15.55s/it] 73%|███████▎  | 102/140 [15:54<07:39, 12.09s/it] 74%|███████▎  | 103/140 [15:58<05:57,  9.65s/it] 74%|███████▍  | 104/140 [16:02<04:45,  7.94s/it] 75%|███████▌  | 105/140 [16:06<03:56,  6.74s/it] 76%|███████▌  | 106/140 [16:10<03:21,  5.92s/it] 76%|███████▋  | 107/140 [16:13<02:55,  5.32s/it] 77%|███████▋  | 108/140 [16:17<02:37,  4.91s/it] 78%|███████▊  | 109/140 [16:21<02:23,  4.63s/it] 79%|███████▊  | 110/140 [16:38<04:02,  8.08s/it] 79%|███████▉  | 111/140 [16:42<03:19,  6.87s/it] 80%|████████  | 112/140 [16:46<02:47,  6.00s/it]                                                  80%|████████  | 112/140 [16:46<02:47,  6.00s/it]int64
{'eval_loss': 0.012140374630689621, 'eval_rouge1': 71.3842, 'eval_rouge2': 65.9088, 'eval_rougeL': 71.176, 'eval_rougeLsum': 71.1847, 'eval_runtime': 4.5627, 'eval_samples_per_second': 7.89, 'eval_steps_per_second': 0.438, 'epoch': 7.0}
{'loss': 0.0016, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.11s/it][A                                                 
                                             [A 80%|████████  | 112/140 [16:50<02:47,  6.00s/it]
100%|██████████| 2/2 [00:02<00:00,  1.11s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [18:21<14:49, 32.93s/it] 81%|████████▏ | 114/140 [18:25<10:29, 24.23s/it] 82%|████████▏ | 115/140 [18:29<07:33, 18.14s/it] 83%|████████▎ | 116/140 [18:33<05:33, 13.90s/it] 84%|████████▎ | 117/140 [18:37<04:11, 10.93s/it] 84%|████████▍ | 118/140 [18:41<03:14,  8.83s/it] 85%|████████▌ | 119/140 [18:45<02:34,  7.37s/it] 86%|████████▌ | 120/140 [18:49<02:07,  6.37s/it] 86%|████████▋ | 121/140 [18:53<01:47,  5.65s/it] 87%|████████▋ | 122/140 [18:57<01:32,  5.12s/it] 88%|████████▊ | 123/140 [19:03<01:32,  5.41s/it] 89%|████████▊ | 124/140 [19:07<01:19,  5.00s/it] 89%|████████▉ | 125/140 [19:11<01:10,  4.68s/it] 90%|█████████ | 126/140 [19:15<01:02,  4.45s/it]                                                  90%|█████████ | 126/140 [19:15<01:02,  4.45s/it]int64
{'eval_loss': 0.012922730296850204, 'eval_rouge1': 71.3842, 'eval_rouge2': 65.9088, 'eval_rougeL': 71.176, 'eval_rougeLsum': 71.1847, 'eval_runtime': 4.5351, 'eval_samples_per_second': 7.938, 'eval_steps_per_second': 0.441, 'epoch': 8.0}
{'loss': 0.0011, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.12s/it][A                                                 
                                             [A 90%|█████████ | 126/140 [19:20<01:02,  4.45s/it]
100%|██████████| 2/2 [00:02<00:00,  1.12s/it][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [21:03<07:40, 35.40s/it] 91%|█████████▏| 128/140 [21:06<05:11, 25.97s/it] 92%|█████████▏| 129/140 [21:10<03:32, 19.36s/it] 93%|█████████▎| 130/140 [21:14<02:27, 14.76s/it] 94%|█████████▎| 131/140 [21:19<01:43, 11.55s/it] 94%|█████████▍| 132/140 [21:23<01:14,  9.29s/it] 95%|█████████▌| 133/140 [21:27<00:53,  7.69s/it] 96%|█████████▌| 134/140 [21:31<00:39,  6.59s/it] 96%|█████████▋| 135/140 [21:35<00:29,  5.82s/it] 97%|█████████▋| 136/140 [21:39<00:21,  5.26s/it] 98%|█████████▊| 137/140 [21:50<00:21,  7.25s/it] 99%|█████████▊| 138/140 [21:54<00:12,  6.27s/it] 99%|█████████▉| 139/140 [21:58<00:05,  5.59s/it]100%|██████████| 140/140 [22:02<00:00,  5.09s/it]                                                 100%|██████████| 140/140 [22:02<00:00,  5.09s/it]int64
{'eval_loss': 0.013066587038338184, 'eval_rouge1': 71.3842, 'eval_rouge2': 65.9088, 'eval_rougeL': 71.176, 'eval_rougeLsum': 71.1847, 'eval_runtime': 4.6023, 'eval_samples_per_second': 7.822, 'eval_steps_per_second': 0.435, 'epoch': 9.0}
{'loss': 0.001, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:02<00:00,  1.13s/it][A                                                 
                                             [A100%|██████████| 140/140 [22:07<00:00,  5.09s/it]
100%|██████████| 2/2 [00:02<00:00,  1.13s/it][A
                                             [A                                                 100%|██████████| 140/140 [23:01<00:00,  5.09s/it]100%|██████████| 140/140 [23:02<00:00,  9.88s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 0.013331962749361992, 'eval_rouge1': 71.3842, 'eval_rouge2': 65.9088, 'eval_rougeL': 71.176, 'eval_rougeLsum': 71.1847, 'eval_runtime': 4.6426, 'eval_samples_per_second': 7.754, 'eval_steps_per_second': 0.431, 'epoch': 10.0}
{'train_runtime': 1389.6085, 'train_samples_per_second': 2.288, 'train_steps_per_second': 0.101, 'train_loss': 0.44141166687144767, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▁▁▁▁▁▁▁▁▁
wandb:                    eval/rouge1 ▁█████████
wandb:                    eval/rouge2 ▁█████████
wandb:                    eval/rougeL ▁█████████
wandb:                 eval/rougeLsum ▁█████████
wandb:                   eval/runtime █▂▂▁▁▄▄▃▄▅
wandb:        eval/samples_per_second ▁▇▇██▅▅▆▅▄
wandb:          eval/steps_per_second ▁▇▇▇█▅▅▆▅▄
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.01333
wandb:                    eval/rouge1 71.3842
wandb:                    eval/rouge2 65.9088
wandb:                    eval/rougeL 71.176
wandb:                 eval/rougeLsum 71.1847
wandb:                   eval/runtime 4.6426
wandb:        eval/samples_per_second 7.754
wandb:          eval/steps_per_second 0.431
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.001
wandb:               train/total_flos 215151575040000.0
wandb:               train/train_loss 0.44141
wandb:            train/train_runtime 1389.6085
wandb: train/train_samples_per_second 2.288
wandb:   train/train_steps_per_second 0.101
wandb: 
wandb: 🚀 View run t5-large_ep-10_ga-1_b-4_lr-0.0003 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/wcwm6gh8
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_223528-wcwm6gh8/logs
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/user/ksharma/miniconda3/envs/ks_tars_thesis did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}
  warn(msg)
/home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!
  warn(msg)
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/big_test_noun_ellipsis.py:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  self.metric = load_metric(metric)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/user/ksharma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Map:   0%|          | 0/318 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/36 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/50 [00:00<?, ? examples/s]                                                  /home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: kartikey-sharma (ma-thesis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/user/ksharma/ks_thesis/ma-thesis/pipeline_test/wandb/run-20230725_225940-jtd0uwm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mt5-base_ep-10_ga-1_b-4_lr-0.0003
wandb: ⭐️ View project at https://wandb.ai/ma-thesis/ELLIPSIS_models_english
wandb: 🚀 View run at https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/jtd0uwm0
################################################ mt5-base_ep-10_ga-1_b-4_lr-0.0003 ################################################
  0%|          | 0/140 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/140 [00:13<31:49, 13.74s/it]  1%|▏         | 2/140 [00:16<17:15,  7.50s/it]  2%|▏         | 3/140 [00:20<12:34,  5.51s/it]  3%|▎         | 4/140 [00:23<10:18,  4.55s/it]  4%|▎         | 5/140 [00:26<09:03,  4.02s/it]  4%|▍         | 6/140 [00:35<13:04,  5.86s/it]  5%|▌         | 7/140 [00:38<11:00,  4.96s/it]  6%|▌         | 8/140 [00:41<09:36,  4.37s/it]  6%|▋         | 9/140 [00:55<16:00,  7.33s/it]  7%|▋         | 10/140 [00:58<13:03,  6.03s/it]  8%|▊         | 11/140 [01:01<11:04,  5.15s/it]  9%|▊         | 12/140 [01:05<09:39,  4.53s/it]  9%|▉         | 13/140 [01:08<08:40,  4.10s/it] 10%|█         | 14/140 [01:11<07:58,  3.79s/it]                                                 10%|█         | 14/140 [01:11<07:58,  3.79s/it]{'loss': 28.1632, 'learning_rate': 0.00027, 'epoch': 1.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A                                                
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A 10%|█         | 14/140 [01:14<07:58,  3.79s/it]
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|█         | 15/140 [02:48<1:06:52, 32.10s/it] 11%|█▏        | 16/140 [02:52<48:22, 23.41s/it]   12%|█▏        | 17/140 [02:55<35:28, 17.30s/it] 13%|█▎        | 18/140 [02:58<26:31, 13.04s/it] 14%|█▎        | 19/140 [03:01<20:16, 10.05s/it] 14%|█▍        | 20/140 [03:04<15:58,  7.98s/it] 15%|█▌        | 21/140 [03:07<12:57,  6.54s/it] 16%|█▌        | 22/140 [03:10<10:50,  5.51s/it] 16%|█▋        | 23/140 [03:14<09:21,  4.80s/it] 17%|█▋        | 24/140 [03:17<08:18,  4.30s/it] 18%|█▊        | 25/140 [03:20<07:34,  3.95s/it] 19%|█▊        | 26/140 [03:23<07:03,  3.72s/it] 19%|█▉        | 27/140 [03:34<10:59,  5.84s/it] 20%|██        | 28/140 [03:37<09:20,  5.01s/it]                                                 20%|██        | 28/140 [03:37<09:20,  5.01s/it]int64
{'eval_loss': 16.776763916015625, 'eval_rouge1': 7.7373, 'eval_rouge2': 0.3704, 'eval_rougeL': 7.6435, 'eval_rougeLsum': 7.66, 'eval_runtime': 3.5638, 'eval_samples_per_second': 10.102, 'eval_steps_per_second': 0.561, 'epoch': 1.0}
{'loss': 22.0905, 'learning_rate': 0.00023999999999999998, 'epoch': 2.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                
                                             [A 20%|██        | 28/140 [03:40<09:20,  5.01s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|██        | 29/140 [05:06<55:57, 30.25s/it] 21%|██▏       | 30/140 [05:09<40:32, 22.11s/it] 22%|██▏       | 31/140 [05:12<29:50, 16.42s/it] 23%|██▎       | 32/140 [05:15<22:23, 12.44s/it] 24%|██▎       | 33/140 [05:19<17:11,  9.64s/it] 24%|██▍       | 34/140 [05:22<13:34,  7.68s/it] 25%|██▌       | 35/140 [05:25<11:03,  6.32s/it] 26%|██▌       | 36/140 [05:28<09:17,  5.36s/it] 26%|██▋       | 37/140 [05:31<08:10,  4.76s/it] 27%|██▋       | 38/140 [05:34<07:15,  4.27s/it] 28%|██▊       | 39/140 [05:38<06:36,  3.93s/it] 29%|██▊       | 40/140 [05:41<06:09,  3.69s/it] 29%|██▉       | 41/140 [05:44<05:49,  3.53s/it] 30%|███       | 42/140 [05:47<05:32,  3.39s/it]                                                 30%|███       | 42/140 [05:58<05:32,  3.39s/it]int64
{'eval_loss': 14.504185676574707, 'eval_rouge1': 13.1311, 'eval_rouge2': 2.9957, 'eval_rougeL': 12.3069, 'eval_rougeLsum': 12.3077, 'eval_runtime': 3.5551, 'eval_samples_per_second': 10.126, 'eval_steps_per_second': 0.563, 'epoch': 2.0}
{'loss': 18.1501, 'learning_rate': 0.00020999999999999998, 'epoch': 3.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                
                                             [A 30%|███       | 42/140 [06:02<05:32,  3.39s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|███       | 43/140 [07:08<43:17, 26.77s/it] 31%|███▏      | 44/140 [07:11<31:29, 19.68s/it] 32%|███▏      | 45/140 [07:15<23:18, 14.72s/it] 33%|███▎      | 46/140 [07:18<17:36, 11.24s/it] 34%|███▎      | 47/140 [07:21<13:39,  8.81s/it] 34%|███▍      | 48/140 [07:24<10:54,  7.11s/it] 35%|███▌      | 49/140 [07:40<14:48,  9.76s/it] 36%|███▌      | 50/140 [07:43<11:39,  7.77s/it] 36%|███▋      | 51/140 [07:47<09:46,  6.59s/it] 37%|███▋      | 52/140 [07:50<08:08,  5.55s/it] 38%|███▊      | 53/140 [07:53<06:59,  4.83s/it] 39%|███▊      | 54/140 [08:20<16:13, 11.32s/it] 39%|███▉      | 55/140 [08:23<12:33,  8.86s/it] 40%|████      | 56/140 [08:26<09:59,  7.14s/it]                                                 40%|████      | 56/140 [08:26<09:59,  7.14s/it]int64
{'eval_loss': 12.096590042114258, 'eval_rouge1': 20.0763, 'eval_rouge2': 9.6006, 'eval_rougeL': 18.8944, 'eval_rougeLsum': 18.7718, 'eval_runtime': 3.5936, 'eval_samples_per_second': 10.018, 'eval_steps_per_second': 0.557, 'epoch': 3.0}
{'loss': 13.6931, 'learning_rate': 0.00017999999999999998, 'epoch': 4.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.23it/s][A                                                
                                             [A 40%|████      | 56/140 [08:29<09:59,  7.14s/it]
100%|██████████| 2/2 [00:01<00:00,  1.23it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|████      | 57/140 [08:45<14:40, 10.61s/it] 41%|████▏     | 58/140 [08:48<11:25,  8.36s/it] 42%|████▏     | 59/140 [08:51<09:09,  6.79s/it] 43%|████▎     | 60/140 [08:54<07:35,  5.70s/it] 44%|████▎     | 61/140 [08:57<06:29,  4.93s/it] 44%|████▍     | 62/140 [09:12<10:08,  7.80s/it] 45%|████▌     | 63/140 [09:15<08:12,  6.40s/it] 46%|████▌     | 64/140 [09:18<06:51,  5.41s/it] 46%|████▋     | 65/140 [09:21<05:54,  4.73s/it] 47%|████▋     | 66/140 [09:24<05:14,  4.25s/it] 48%|████▊     | 67/140 [09:27<04:45,  3.91s/it] 49%|████▊     | 68/140 [09:30<04:24,  3.68s/it] 49%|████▉     | 69/140 [09:33<04:09,  3.51s/it] 50%|█████     | 70/140 [09:37<03:57,  3.39s/it]                                                 50%|█████     | 70/140 [09:37<03:57,  3.39s/it]int64
{'eval_loss': 9.75368881225586, 'eval_rouge1': 17.0958, 'eval_rouge2': 0.6283, 'eval_rougeL': 16.6763, 'eval_rougeLsum': 16.6651, 'eval_runtime': 3.446, 'eval_samples_per_second': 10.447, 'eval_steps_per_second': 0.58, 'epoch': 4.0}
{'loss': 9.7145, 'learning_rate': 0.00015, 'epoch': 5.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                
                                             [A 50%|█████     | 70/140 [09:40<03:57,  3.39s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████     | 71/140 [10:03<11:42, 10.18s/it] 51%|█████▏    | 72/140 [10:06<09:08,  8.06s/it] 52%|█████▏    | 73/140 [10:09<07:20,  6.57s/it] 53%|█████▎    | 74/140 [10:12<06:05,  5.53s/it] 54%|█████▎    | 75/140 [10:15<05:12,  4.81s/it] 54%|█████▍    | 76/140 [10:18<04:35,  4.31s/it] 55%|█████▌    | 77/140 [10:21<04:08,  3.95s/it] 56%|█████▌    | 78/140 [10:24<03:49,  3.71s/it] 56%|█████▋    | 79/140 [10:32<05:01,  4.94s/it] 57%|█████▋    | 80/140 [10:35<04:23,  4.39s/it] 58%|█████▊    | 81/140 [10:38<03:56,  4.02s/it] 59%|█████▊    | 82/140 [10:42<03:37,  3.74s/it] 59%|█████▉    | 83/140 [10:45<03:23,  3.56s/it] 60%|██████    | 84/140 [10:48<03:12,  3.43s/it]                                                 60%|██████    | 84/140 [10:48<03:12,  3.43s/it]int64
{'eval_loss': 8.266946792602539, 'eval_rouge1': 19.5344, 'eval_rouge2': 1.3177, 'eval_rougeL': 19.2886, 'eval_rougeLsum': 19.3157, 'eval_runtime': 3.5768, 'eval_samples_per_second': 10.065, 'eval_steps_per_second': 0.559, 'epoch': 5.0}
{'loss': 7.5059, 'learning_rate': 0.00011999999999999999, 'epoch': 6.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                
                                             [A 60%|██████    | 84/140 [10:51<03:12,  3.43s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 61%|██████    | 85/140 [11:07<07:21,  8.02s/it] 61%|██████▏   | 86/140 [11:10<05:53,  6.56s/it] 62%|██████▏   | 87/140 [11:13<04:52,  5.52s/it] 63%|██████▎   | 88/140 [11:16<04:09,  4.81s/it] 64%|██████▎   | 89/140 [11:19<03:41,  4.35s/it] 64%|██████▍   | 90/140 [11:22<03:19,  3.99s/it] 65%|██████▌   | 91/140 [11:28<03:41,  4.51s/it] 66%|██████▌   | 92/140 [11:31<03:16,  4.09s/it] 66%|██████▋   | 93/140 [11:34<02:58,  3.81s/it] 67%|██████▋   | 94/140 [11:37<02:45,  3.60s/it] 68%|██████▊   | 95/140 [11:41<02:35,  3.46s/it] 69%|██████▊   | 96/140 [11:44<02:27,  3.36s/it] 69%|██████▉   | 97/140 [11:47<02:21,  3.29s/it] 70%|███████   | 98/140 [11:50<02:17,  3.28s/it]                                                 70%|███████   | 98/140 [11:50<02:17,  3.28s/it]int64
{'eval_loss': 7.598823547363281, 'eval_rouge1': 16.877, 'eval_rouge2': 1.2316, 'eval_rougeL': 16.4713, 'eval_rougeLsum': 16.4357, 'eval_runtime': 3.5506, 'eval_samples_per_second': 10.139, 'eval_steps_per_second': 0.563, 'epoch': 6.0}
{'loss': 6.1736, 'learning_rate': 8.999999999999999e-05, 'epoch': 7.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                
                                             [A 70%|███████   | 98/140 [11:54<02:17,  3.28s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|███████   | 99/140 [13:03<16:34, 24.26s/it] 71%|███████▏  | 100/140 [13:06<11:56, 17.92s/it] 72%|███████▏  | 101/140 [13:10<08:48, 13.55s/it] 73%|███████▎  | 102/140 [13:13<06:36, 10.43s/it] 74%|███████▎  | 103/140 [13:16<05:04,  8.24s/it] 74%|███████▍  | 104/140 [13:19<04:01,  6.71s/it] 75%|███████▌  | 105/140 [13:22<03:17,  5.65s/it] 76%|███████▌  | 106/140 [13:26<02:46,  4.90s/it] 76%|███████▋  | 107/140 [13:29<02:24,  4.38s/it] 77%|███████▋  | 108/140 [13:32<02:07,  4.00s/it] 78%|███████▊  | 109/140 [13:35<01:55,  3.73s/it] 79%|███████▊  | 110/140 [13:38<01:49,  3.66s/it] 79%|███████▉  | 111/140 [13:42<01:41,  3.52s/it] 80%|████████  | 112/140 [13:45<01:34,  3.39s/it]                                                  80%|████████  | 112/140 [13:45<01:34,  3.39s/it]int64
{'eval_loss': 6.521877765655518, 'eval_rouge1': 17.4076, 'eval_rouge2': 0.9425, 'eval_rougeL': 17.4847, 'eval_rougeLsum': 17.4625, 'eval_runtime': 3.5397, 'eval_samples_per_second': 10.17, 'eval_steps_per_second': 0.565, 'epoch': 7.0}
{'loss': 5.2929, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                 
                                             [A 80%|████████  | 112/140 [13:48<01:34,  3.39s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|████████  | 113/140 [15:21<14:04, 31.28s/it] 81%|████████▏ | 114/140 [15:24<09:53, 22.84s/it] 82%|████████▏ | 115/140 [15:27<07:03, 16.92s/it] 83%|████████▎ | 116/140 [15:30<05:06, 12.78s/it] 84%|████████▎ | 117/140 [15:34<03:47,  9.91s/it] 84%|████████▍ | 118/140 [15:37<02:53,  7.86s/it] 85%|████████▌ | 119/140 [15:40<02:15,  6.44s/it] 86%|████████▌ | 120/140 [15:43<01:48,  5.45s/it] 86%|████████▋ | 121/140 [15:46<01:30,  4.75s/it] 87%|████████▋ | 122/140 [15:49<01:17,  4.28s/it] 88%|████████▊ | 123/140 [15:52<01:06,  3.94s/it] 89%|████████▊ | 124/140 [15:56<00:59,  3.70s/it] 89%|████████▉ | 125/140 [15:59<00:53,  3.54s/it] 90%|█████████ | 126/140 [16:02<00:47,  3.41s/it]                                                  90%|█████████ | 126/140 [16:02<00:47,  3.41s/it]int64
{'eval_loss': 6.287270545959473, 'eval_rouge1': 16.6034, 'eval_rouge2': 0.463, 'eval_rougeL': 16.653, 'eval_rougeLsum': 16.6309, 'eval_runtime': 3.5742, 'eval_samples_per_second': 10.072, 'eval_steps_per_second': 0.56, 'epoch': 8.0}
{'loss': 4.8269, 'learning_rate': 2.9999999999999997e-05, 'epoch': 9.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A                                                 
                                             [A 90%|█████████ | 126/140 [16:05<00:47,  3.41s/it]
100%|██████████| 2/2 [00:01<00:00,  1.18it/s][A
                                             [A/home/user/ksharma/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|█████████ | 127/140 [19:11<12:49, 59.17s/it] 91%|█████████▏| 128/140 [19:14<08:28, 42.35s/it] 92%|█████████▏| 129/140 [19:17<05:36, 30.59s/it] 93%|█████████▎| 130/140 [19:21<03:44, 22.41s/it] 94%|█████████▎| 131/140 [19:24<02:29, 16.63s/it] 94%|█████████▍| 132/140 [19:27<01:40, 12.59s/it] 95%|█████████▌| 133/140 [19:30<01:08,  9.75s/it] 96%|█████████▌| 134/140 [19:33<00:46,  7.77s/it] 96%|█████████▋| 135/140 [19:36<00:31,  6.38s/it] 97%|█████████▋| 136/140 [19:56<00:41, 10.29s/it] 98%|█████████▊| 137/140 [19:59<00:24,  8.15s/it] 99%|█████████▊| 138/140 [20:02<00:13,  6.63s/it] 99%|█████████▉| 139/140 [20:05<00:05,  5.64s/it]100%|██████████| 140/140 [20:09<00:00,  4.88s/it]                                                 100%|██████████| 140/140 [20:09<00:00,  4.88s/it]int64
{'eval_loss': 5.93906307220459, 'eval_rouge1': 16.3359, 'eval_rouge2': 0.5456, 'eval_rougeL': 16.2714, 'eval_rougeLsum': 16.2516, 'eval_runtime': 3.5718, 'eval_samples_per_second': 10.079, 'eval_steps_per_second': 0.56, 'epoch': 9.0}
{'loss': 4.8025, 'learning_rate': 0.0, 'epoch': 10.0}

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:01<00:00,  1.20it/s][A                                                 
                                             [A100%|██████████| 140/140 [20:12<00:00,  4.88s/it]
100%|██████████| 2/2 [00:01<00:00,  1.20it/s][A
                                             [A                                                 100%|██████████| 140/140 [21:24<00:00,  4.88s/it]100%|██████████| 140/140 [21:24<00:00,  9.17s/it]
/home/user/ksharma/miniconda3/envs/ks_tars_thesis/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
int64
{'eval_loss': 5.866437911987305, 'eval_rouge1': 17.2051, 'eval_rouge2': 0.7746, 'eval_rougeL': 17.1946, 'eval_rougeLsum': 17.2604, 'eval_runtime': 3.5273, 'eval_samples_per_second': 10.206, 'eval_steps_per_second': 0.567, 'epoch': 10.0}
{'train_runtime': 1292.0864, 'train_samples_per_second': 2.461, 'train_steps_per_second': 0.108, 'train_loss': 12.041328757149833, 'epoch': 10.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▇▅▃▃▂▁▁▁▁
wandb:                    eval/rouge1 ▁▄█▆█▆▆▆▆▆
wandb:                    eval/rouge2 ▁▃█▁▂▂▁▁▁▁
wandb:                    eval/rougeL ▁▄█▆█▆▇▆▆▇
wandb:                 eval/rougeLsum ▁▄█▆█▆▇▆▆▇
wandb:                   eval/runtime ▇▆█▁▇▆▅▇▇▅
wandb:        eval/samples_per_second ▂▃▁█▂▃▃▂▂▄
wandb:          eval/steps_per_second ▂▃▁█▂▃▃▂▂▄
wandb:                    train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:              train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███
wandb:            train/learning_rate █▇▆▆▅▄▃▃▂▁
wandb:                     train/loss █▆▅▄▂▂▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 5.86644
wandb:                    eval/rouge1 17.2051
wandb:                    eval/rouge2 0.7746
wandb:                    eval/rougeL 17.1946
wandb:                 eval/rougeLsum 17.2604
wandb:                   eval/runtime 3.5273
wandb:        eval/samples_per_second 10.206
wandb:          eval/steps_per_second 0.567
wandb:                    train/epoch 10.0
wandb:              train/global_step 140
wandb:            train/learning_rate 0.0
wandb:                     train/loss 4.8025
wandb:               train/total_flos 89366407004160.0
wandb:               train/train_loss 12.04133
wandb:            train/train_runtime 1292.0864
wandb: train/train_samples_per_second 2.461
wandb:   train/train_steps_per_second 0.108
wandb: 
wandb: 🚀 View run mt5-base_ep-10_ga-1_b-4_lr-0.0003 at: https://wandb.ai/ma-thesis/ELLIPSIS_models_english/runs/jtd0uwm0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230725_225940-jtd0uwm0/logs
